---
title: "8A: Geographically Weighted Predictive Models"
subtitle: "In this exercise, we will learn how to build predictive models using the geographical random forest method to predict outcomes based on geospatial factors and historical geospatial locations."
# draft: true
date: "Sep 28, 2024"
date-modified: "last-modified"
author: Teng Kok Wai (Walter)
execute:
  echo: true
  eval: true
  freeze: true
  message: false
  warning: false
format:
  html:
    code-link: true
    toc: true
number-sections: true
number-offset: 1
editor: visual
---

## Exercise 8A Reference

[R for Geospatial Data Science and Analytics - 14  Geographically Weighted Predictive Models](https://r4gdsa.netlify.app/chap14.html)

## Overview

In this exercise, we will learn how to build predictive models using the geographical random forest method to predict outcomes based on geospatial factors and historical geospatial locations.

Predictive modeling uses statistical and machine learning techniques to forecast future outcomes. To build these models, we start with a dataset where the outcomes are already known, along with various input variables (predictors) that might influence those outcomes. The model learns from these known examples to make accurate predictions about future events.

Geospatial predictive modelling is conceptually rooted in the principle that the occurrences of events being modeled are limited in distribution.

> When geographically referenced data are used, occurrences of events are neither uniform nor random in distribution over space. There are geospatial factors (infrastructure, sociocultural, topographic, etc.) that constrain and influence where the locations of events occur.

Geospatial predictive modeling identifies and analyzes these factors by examining the relationship between past event locations and the environmental conditions that might have influenced those locations. This helps to better understand and predict where similar events are likely to happen in the future.

::: callout-tip
Several sections in this exercise will take a while to compute... , we will save several intermediate rds file as checkpoints for future uses.
:::

## Learning Outcome

-   Prepare training and test datasets using appropriate data sampling methods.
-   Calibrate predictive models using both geospatial statistical learning and machine learning methods.
-   Compare and select the best model for predicting future outcomes.
-   Predict future outcomes using the best-calibrated model.

## The Data

The following datasets will be used in this study:

| **Data Type** | **Description** | **Format** |
|---------------|------------------------------------------|---------------|
| **Aspatial dataset** | HDB Resale data: a list of HDB resale transacted prices in Singapore from Jan 2017 onwards. | CSV |
| **Geospatial dataset** | *MP14_SUBZONE_WEB_PL*: A polygon feature dataset providing information on URA 2014 Master Plan Planning Subzone boundary data. | ESRI Shapefile |
| **Locational factors with geographic coordinates** | Various data sets including Eldercare, Hawker Centre, Parks, Supermarket, CHAS clinics, Childcare services, Kindergartens, MRT, and Bus stops. | GeoJSON/Shapefile |
| **Locational factors without geographic coordinates** | Primary school data, CBD coordinates, Shopping malls, Good primary schools (ranking by popularity). | CSV/Other Sources |

## Installing and Launching the R Packages

The following R packages will be used in this exercise:

| **Package** | **Purpose** | **Use Case in Exercise** |
|-----------------|--------------------------|-----------------------------|
| **sf** | Handles vector-based geospatial data. | Importing and manipulating polygon and point feature data. |
| **spdep** | Provides functions for spatial dependence analysis, including spatial weights and spatial autocorrelation. | Performing spatially constrained cluster analysis using geographically weighted regression (GWR). |
| **GWmodel** | Provides geographically weighted modeling methods. | Calibrating models to predict HDB resale prices using geographically weighted regression. |
| **SpatialML** | Supports geographical random forest models and spatial machine learning methods. | Calibrating models using geographically weighted random forest (GW RF). |
| **tmap** | Creates static and interactive thematic maps. | Visualizing geospatial data, model predictions, and other geographic patterns. |
| **rsample** | Provides tools for data resampling. | Splitting datasets into training and testing subsets. |
| **Metrics** | Provides evaluation metrics for statistical and machine learning models. | Calculating RMSE (Root Mean Square Error) to evaluate model accuracy. |
| **tidyverse** | A collection of packages for data science tasks such as data manipulation, visualization, and modeling. | Data wrangling, visualization, and performing statistical operations on datasets. |

To install and load these packages, use the following code:

```{r pacman}
pacman::p_load(sf, spdep, GWmodel, SpatialML, tmap, rsample, Metrics, tidyverse)
```

## Import Data and Preparation

### Reading Data File

We begin by loading the input data, which is stored as a simple feature data frame.

```{r}
mdata <- read_rds("data/aspatial/mdata.rds")
```

### Data Sampling

The data is split into 65% training and 35% test sets using the `initial_split()` function from the **rsample** package (part of **tidymodels**).

```{r}
#| eval: false
set.seed(1234)

resale_split <- initial_split(mdata, 
                              prop = 6.5/10,)

train_data <- training(resale_split)
test_data <- testing(resale_split)
```

Save the split datasets:

```{r}
#| eval: false
write_rds(train_data, "data/model/train_data.rds")
write_rds(test_data, "data/model/test_data.rds")
```

### Computing Correlation Matrix

It is important to check for multicollinearity using a correlation matrix before building the predictive model.

```{r corr_matrix}
mdata_nogeo <- mdata %>% st_drop_geometry()
corrplot::corrplot(cor(mdata_nogeo[, 2:17]), 
                   diag = FALSE, 
                   order = "AOE", 
                   tl.pos = "td", 
                   tl.cex = 0.5, 
                   method = "number", 
                   type = "upper")
```

::: callout-note
**Observations:** All correlation values are below 0.8, indicating no sign of multicollinearity.
:::

### Retrieving Stored Data

Finally, load the previously saved training and test data:

```{r}
train_data <- read_rds("data/model/train_data.rds")
test_data <- read_rds("data/model/test_data.rds")
```

## Building a Non-Spatial Multiple Linear Regression

Next, we will build a multiple linear regression (MLR) model to predict *resale_price* using various predictors:

```{r}
price_mlr <- lm(resale_price ~ 
                  floor_area_sqm +
                  storey_order + 
                  remaining_lease_mths +
                  PROX_CBD + 
                  PROX_ELDERLYCARE + 
                  PROX_HAWKER +
                  PROX_MRT + 
                  PROX_PARK + 
                  #PROX_GOOD_PRISCH + 
                  PROX_MALL +
                  #PROX_CHAS + 
                  PROX_SUPERMARKET + 
                  WITHIN_350M_KINDERGARTEN +
                  WITHIN_350M_CHILDCARE + 
                  WITHIN_350M_BUS +
                  WITHIN_1KM_PRISCH,
                data=train_data)

summary(price_mlr)
```

Save the model for future use:

```{r}
#| eval: false 
write_rds(price_mlr, "data/model/price_mlr.rds") 
```

## GWR Predictive Method

In this section, we use **Geographically Weighted Regression (GWR)** to predict HDB resale prices.

### Converting sf Data Frame to SpatialPointsDataFrame

Convert the training data into a **SpatialPointsDataFrame** for use in GWR:

```{r}
train_data_sp <- as_Spatial(train_data)
train_data_sp
```

### Computing Adaptive Bandwidth

Next, we use `bw.gwr()` to compute the optimal adaptive bandwidth using cross-validation:

```{r}
#| eval: false
bw_adaptive <- bw.gwr(resale_price ~ 
                        floor_area_sqm +
                        storey_order + 
                        remaining_lease_mths +
                        PROX_CBD + 
                        PROX_ELDERLYCARE + 
                        PROX_HAWKER +
                        PROX_MRT + 
                        PROX_PARK + 
                        #PROX_GOOD_PRISCH +
                        PROX_MALL +
                        #PROX_CHAS +
                        PROX_SUPERMARKET + 
                        WITHIN_350M_KINDERGARTEN +
                        WITHIN_350M_CHILDCARE + 
                        WITHIN_350M_BUS +
                        WITHIN_1KM_PRISCH,
                      data=train_data_sp,
                      approach="CV",
                      kernel="gaussian",
                      adaptive=TRUE,
                      longlat=FALSE)
```

::: callout-note
**Observations:**

Based on the output from `bw.gwr()` function, the optimal bandwidth is determined to be 40 neighbor points. This means the model will consider the 40 nearest neighbors when estimating parameters for a specific location.
:::

Save the result:

```{r}
#| eval: false
write_rds(bw_adaptive, "data/model/bw_adaptive.rds")
```

### Constructing the Adaptive Bandwidth GWR Model

Load the saved bandwidth and calibrate the GWR model:

```{r}
bw_adaptive <- read_rds("data/model/bw_adaptive.rds")
bw_adaptive
```

Now, we can go ahead to calibrate the gwr-based hedonic pricing model by using adaptive bandwidth and Gaussian kernel as shown in the code below.

```{r}
#| eval: false
gwr_adaptive <- gwr.basic(formula = resale_price ~
                            floor_area_sqm + 
                            storey_order +
                            remaining_lease_mths + 
                            PROX_CBD + 
                            PROX_ELDERLYCARE + 
                            PROX_HAWKER +
                            PROX_MRT + 
                            PROX_PARK +
                            #PROX_GOOD_PRISCH +
                            PROX_MALL + 
                            #PROX_CHAS +
                            PROX_SUPERMARKET + 
                            WITHIN_350M_KINDERGARTEN +
                            WITHIN_350M_CHILDCARE + 
                            WITHIN_350M_BUS +
                            WITHIN_1KM_PRISCH,
                          data = train_data_sp,
                          bw = bw_adaptive, 
                          kernel = 'gaussian', 
                          adaptive = TRUE,
                          longlat = FALSE)
```

Save the GWR model:

```{r}
#| eval: false
write_rds(gwr_adaptive, "data/model/gwr_adaptive.rds")
```

### Retrieving the GWR Model

To retrieve and display the saved GWR model:

```{r}
gwr_adaptive <- read_rds("data/model/gwr_adaptive.rds")
gwr_adaptive
```

### Converting Test Data to SpatialPointsDataFrame

Convert the test data:

```{r}
test_data_sp <- test_data %>%
  as_Spatial()
test_data_sp
```
### Computing Adaptive Bandwidth for the Test Data

Similarly, we use the `bw.gwr()` function from the GWmodel package to determine the optimal bandwidth for our GWR model on the test data.

```{r}
#| eval: false
gwr_bw_test_adaptive <- bw.gwr(resale_price ~ 
                                 floor_area_sqm +
                                 storey_order + 
                                 remaining_lease_mths +
                                 PROX_CBD + 
                                 PROX_ELDERLYCARE + 
                                 PROX_HAWKER +
                                 PROX_MRT + 
                                 PROX_PARK + 
                                 #PROX_GOOD_PRISCH +
                                 PROX_MALL + 
                                 #PROX_CHAS +
                                 PROX_SUPERMARKET + 
                                 WITHIN_350M_KINDERGARTEN +
                                 WITHIN_350M_CHILDCARE + 
                                 WITHIN_350M_BUS +
                                 WITHIN_1KM_PRISCH,
                               data = test_data_sp,
                               approach = "CV",
                               kernel = "gaussian",
                               adaptive = TRUE,
                               longlat = FALSE)
```
```{r}
#| eval: false
write_rds(gwr_bw_test_adaptive, "data/model/gwr_bw_test_adaptive.rds")
```

```{r}
#| eval: false
gwr_bw_test_adaptive <- read_rds("data/model/gwr_bw_test_adaptive.rds")
```

### Computing Predicted Values of the Test Data

Finally, we use the `gwr.predict()` function from the GWmodel package to compute the predicted values of the test data based on our GWR model. We specify our formula, training data, test data, bandwidth, kernel type, and set `adaptive=TRUE` and `longlat=FALSE`.

```{r}
#| eval: false
gwr_pred <- gwr.predict(formula = resale_price ~
                          floor_area_sqm + storey_order +
                          remaining_lease_mths + PROX_CBD + 
                          PROX_ELDERLYCARE + PROX_HAWKER + 
                          PROX_MRT + PROX_PARK + PROX_MALL + 
                          PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +
                          WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + 
                          WITHIN_1KM_PRISCH, 
                        data=train_data_sp, 
                        predictdata = test_data_sp, 
                        bw=40, 
                        kernel = 'gaussian', 
                        adaptive=TRUE, 
                        longlat = FALSE)
```

## Preparing Coordinates Data

### Extracting Coordinates

The code below extracts the x, y coordinates for the full, training, and test datasets:

```{r}
#| eval: false
coords <- st_coordinates(mdata)
coords_train <- st_coordinates(train_data)
coords_test <- st_coordinates(test_data)
```

Save the extracted coordinates for future use:

```{r}
#| eval: false
coords_train <- write_rds(coords_train, "data/model/coords_train.rds")
coords_test <- write_rds(coords_test, "data/model/coords_test.rds")
```

```{r}
coords_train <- read_rds("data/model/coords_train.rds")
coords_test <- read_rds("data/model/coords_test.rds")
```

### Dropping Geometry Field

We remove the geometry column from the training data using `st_drop_geometry()`:

```{r}
#| eval: false
train_data <- train_data %>% 
  st_drop_geometry()
```

## Calibrating Random Forest Model

We will now calibrate a random forest model using the **ranger** package to predict HDB resale prices:

```{r}
#| eval: false
set.seed(1234)

rf <- ranger(resale_price ~ floor_area_sqm + storey_order + 
               remaining_lease_mths + PROX_CBD + PROX_ELDERLYCARE + 
               PROX_HAWKER + PROX_MRT + PROX_PARK + PROX_MALL + 
               PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +
               WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + 
               WITHIN_1KM_PRISCH,
             data=train_data)
```

Save the model:

```{r}
#| eval: false
write_rds(rf, "data/model/rf.rds")
```

```{r}
rf <- read_rds("data/model/rf.rds")
rf
```

## Calibrating Geographical Random Forest Model

We now calibrate a geographic random forest model using `grf()` from the **SpatialML** package.

### Calibrating with Training Data

Calibrate the model with an adaptive bandwidth:

```{r}
#| eval: false
set.seed(1234)
gwRF_adaptive <- grf(formula = resale_price ~ floor_area_sqm + storey_order +
                       remaining_lease_mths + PROX_CBD + PROX_ELDERLYCARE +
                       PROX_HAWKER + PROX_MRT + PROX_PARK + PROX_MALL +
                       PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +
                       WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +
                       WITHIN_1KM_PRISCH,
                     dframe=train_data, 
                     bw=55,
                     kernel="adaptive",
                     coords=coords_train)
```

Save the model:

```{r}
#| eval: false
write_rds(gwRF_adaptive, "data/model/gwRF_adaptive.rds")
```

```{r}
#| eval: false
gwRF_adaptive <- read_rds("data/model/gwRF_adaptive.rds")
```

### Predicting with Test Data

#### Preparing the Test Data

Combine the test data with its coordinates:

```{r}
#| eval: false
test_data <- cbind(test_data, coords_test) %>% 
  st_drop_geometry()
```

#### Predicting Resale Prices

Use the trained geographical random forest model to predict prices:

```{r}
#| eval: false
gwRF_pred <- predict.grf(gwRF_adaptive, 
                           test_data, 
                           x.var.name = "X",
                           y.var.name = "Y", 
                           local.w = 1,
                           global.w = 0)
```

Save the predicted values:

```{r}
#| eval: false
GRF_pred <- write_rds(gwRF_pred, "data/model/GRF_pred.rds")
```

#### Converting Predicted Output into a Data Frame

The output of the `predict.grf()` function is a vector of predicted values. For further visualization and analysis, it’s useful to convert it into a data frame. To convert the prediction output to a data frame for analysis:

```{r}
#| eval: false
GRF_pred <- read_rds("data/model/GRF_pred.rds")
GRF_pred_df <- as.data.frame(GRF_pred)
```

```{r}
#| eval: false
# append pred values into the test data
test_data_p <- cbind(test_data, GRF_pred_df)
```

```{r}
#| eval: false
write_rds(test_data_p, "data/model/test_data_p.rds")
```

### Calculating Root Mean Square Error (RMSE)

Compute RMSE to evaluate the model’s predictive accuracy:

```{r}
test_data_p <- read_rds("data/model/test_data_p.rds")

rmse(test_data_p$resale_price, 
     test_data_p$GRF_pred)
```

The lower the RMSE value, the better the predictive model is.

### Visualizing the Predicted Values

Create a scatterplot to compare actual vs. predicted resale prices:

```{r scatterplot_actual_pred_sales}
ggplot(data = test_data_p, aes(x = GRF_pred, y = resale_price)) +
  geom_point()+
    geom_abline(slope = 1, intercept = 0, color = "red", linetype = "solid")
```

::: callout-note
A good predictive model should have the scatter point close to the diagonal line. The scatter plot can be also used to detect if any outliers in the model.
:::



[
  {
    "objectID": "In-class_Ex/In-class_Ex01/data/MPSZ-2019.html",
    "href": "In-class_Ex/In-class_Ex01/data/MPSZ-2019.html",
    "title": "Walter Teng's Coursework for SMU ISSS626: Geospatial Analytics and Applications",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "In-class_Ex/index.html",
    "href": "In-class_Ex/index.html",
    "title": "In-Class Exercise",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nIn-Class Exercise 4\n\n\n#todo\n\n\n17 min\n\n\n\nSep 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nIn-Class Exercise 3\n\n\nThis session reviews past Hands-on exercises and address questions from classmates on Piazza.\n\n\n5 min\n\n\n\nSep 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nIn-Class Exercise 2\n\n\nIn this exercise, we will learn to analyze spatial point patterns using spatstat methods, including installing necessary packages, creating spatial objects, performing kernel density estimation, and applying edge correction methods.\n\n\n11 min\n\n\n\nSep 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nIn-Class Exercise 1\n\n\nIn the exercise, we will learn to handle geospatial data in R, create various maps, and perform statistical analysis using sf, tmap, and ggstatsplot.\n\n\n23 min\n\n\n\nAug 26, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html",
    "title": "In-Class Exercise 3",
    "section": "",
    "text": "This session reviews past Hands-on exercises and address questions from classmates on Piazza. We will use a combination of R packages and datasets introduced up to Hands-on Exercise 03. Prior knowledge of content covered up to this exercise is required."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#overview",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#overview",
    "title": "In-Class Exercise 3",
    "section": "",
    "text": "This session reviews past Hands-on exercises and address questions from classmates on Piazza. We will use a combination of R packages and datasets introduced up to Hands-on Exercise 03. Prior knowledge of content covered up to this exercise is required."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#import-the-r-packages",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#import-the-r-packages",
    "title": "In-Class Exercise 3",
    "section": "2 Import the R Packages",
    "text": "2 Import the R Packages\n\npacman::p_load(sf, spNetwork, tmap, tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#tip-1-observe-data-dimension-carefully",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#tip-1-observe-data-dimension-carefully",
    "title": "In-Class Exercise 3",
    "section": "3 Tip 1: Observe data dimension carefully",
    "text": "3 Tip 1: Observe data dimension carefully\nRelevant Links: ISSS 626 | Piazza QA, Details about NKDE • spNetwork\nWhen computing NKDE, we may encounter error if the event contains 3D coordinates (XYZ). Typically, publicly accessible geospatial data from Singapore data portals are 2D. During data conversion from kml or other formats, XY coordinates may be unknowingly converted into a XYZ coordinates.\n\nnetwork &lt;- st_read(dsn=\"data/geospatial\", \n                   layer=\"Punggol_St\")\n\nReading layer `Punggol_St' from data source \n  `/Users/walter/code/isss626/isss626-gaa/In-class_Ex/In-class_Ex03/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 2642 features and 2 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 34038.56 ymin: 40941.11 xmax: 38882.85 ymax: 44801.27\nProjected CRS: SVY21 / Singapore TM\n\nchildcare &lt;- st_read(dsn=\"data/geospatial\",\n                     layer=\"Punggol_CC\")\n\nReading layer `Punggol_CC' from data source \n  `/Users/walter/code/isss626/isss626-gaa/In-class_Ex/In-class_Ex03/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 61 features and 1 field\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 34423.98 ymin: 41503.6 xmax: 37619.47 ymax: 44685.77\nz_range:       zmin: 0 zmax: 0\nProjected CRS: SVY21 / Singapore TM\n\n\n\n\n\n\n\n\nNote\n\n\n\nObserve the output carefully. This dataset contains XYZ coordinates in where the z coordinate value is always 0. It is redundant.\n\n\n\n3.1 Solution: Use st_zm() to drop the Z dimension\n\nchildcare &lt;- st_zm(childcare, drop=TRUE, what = \"ZM\")\nchildcare\n\nSimple feature collection with 61 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 34423.98 ymin: 41503.6 xmax: 37619.47 ymax: 44685.77\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n      Name                  geometry\n1   kml_10 POINT (36173.81 42550.33)\n2   kml_99 POINT (36479.56 42405.21)\n3  kml_100 POINT (36618.72 41989.13)\n4  kml_101 POINT (36285.37 42261.42)\n5  kml_122  POINT (35414.54 42625.1)\n6  kml_161 POINT (36545.16 42580.09)\n7  kml_172 POINT (35289.44 44083.57)\n8  kml_188 POINT (36520.56 42844.74)\n9  kml_205  POINT (36924.01 41503.6)\n10 kml_222 POINT (37141.76 42326.36)\n\n\nObserve that Z dimension is dropped for all points."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#tip-2-usage-of-st_geometry",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#tip-2-usage-of-st_geometry",
    "title": "In-Class Exercise 3",
    "section": "4 Tip 2: Usage of st_geometry()",
    "text": "4 Tip 2: Usage of st_geometry()\n\nNotice that network has 2 non-spatial columns and a geometry column\n\n\nnames(network)\n\n[1] \"LINK_ID\"  \"ST_NAME\"  \"geometry\"\n\n\n\nplot(network): Plots the entire network object, which might include non-spatial data. Each non-spatial column will create a separate plot. (This is typically not what we want to viz.)\n\n\nplot(network)\nplot(childcare,\n     add=T, # overlay: true\n     col='red',pch=19)\n\n\n\n\n\n\n\n\n\nplot(st_geometry(network)): Specifically plots only the geometric component of the network object if it is an sf object, which can be useful if you want to focus solely on spatial features. (This is usually what we want.)\n\n\nplot(st_geometry(network))\nplot(childcare,add=T,col='red',pch=19)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#tip-3-use-tmap-to-create-more-advanced-complex-maps",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#tip-3-use-tmap-to-create-more-advanced-complex-maps",
    "title": "In-Class Exercise 3",
    "section": "5 Tip 3: Use tmap to create more advanced, complex maps",
    "text": "5 Tip 3: Use tmap to create more advanced, complex maps\nIn Tip 2, we discussed map plotting using plot() from base R. It is fast for quick viz, but lacks customization as compared to libraries such as tmap\n\nUsing plotUsing tmap\n\n\n\nplot(st_geometry(network))\nplot(childcare,add=T,col='red',pch = 18)\n\n\n\n\n\n\n\n\n\n\nWe have more mapping options / control and ability to create interactive maps etc.\n\ntmap_mode('view')\ntm_shape(childcare) + \n  tm_dots(col = 'red') + \n  tm_shape(network) +\n  tm_lines()\n\n\n\n\ntmap_mode('plot')"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#tip-4-preparing-lixel-objects",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#tip-4-preparing-lixel-objects",
    "title": "In-Class Exercise 3",
    "section": "6 Tip 4: Preparing Lixel Objects",
    "text": "6 Tip 4: Preparing Lixel Objects\n\nlixels &lt;- lixelize_lines(network,700,mindist=350)\n\nThe choice of 700m is based on NTU research on people’s willingness to walk. The values of lixel length and mindist varies, depending on your research area, eg. walking commuters, cars, etc."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#tip-5-difference-between-k-function-and-g-function",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#tip-5-difference-between-k-function-and-g-function",
    "title": "In-Class Exercise 3",
    "section": "7 Tip 5: Difference between K-Function and G-Function",
    "text": "7 Tip 5: Difference between K-Function and G-Function\nReference Link: Network k Functions\n\nK-Function (Disc-like): The K-function calculates the proportion of points within a distance r from a typical point. It tells us the “average number of neighbors” around each point within that radius. This measure is cumulative, meaning it looks at all points within growing circles (disks) around a point.\nG-Function (Pair Correlation Function, Ring-like): The G-function is a variation of the K-function that focuses on specific distances rather than cumulative distances. Instead of looking at all points within a growing circle, it examines points within a narrow ring at a specific distance. This allows it to analyze point concentrations at different geographic scales more precisely.\n\n\n\n7.1 When to Use K-Function vs. G-Function\n\nUse the K-Function:\n\nWhen you want to understand the overall pattern of clustering or dispersion of points across various distances.\nUseful for identifying general trends in the spatial arrangement, such as whether points tend to cluster together or spread out over a broad area.\nExample: Analyzing the general distribution of trees in a forest to see if they are randomly spaced, clustered, or regularly spaced.\n\nUse the G-Function:\n\nWhen you are interested in the point concentration at specific distances or scales.\nUseful for detecting specific scales of clustering or dispersion that might be hidden in the cumulative analysis of the K-function.\nExample: Studying the distribution of retail stores to understand if there is clustering at a specific distance (e.g., stores tend to cluster within 500 meters of each other but not at larger scales).\n\n\n\nkfun_childcare &lt;- kfunctions(network, \n                             childcare,\n                             start = 0, \n                             end = 1000, \n                             step = 50, \n                             width = 50, \n                             nsim = 50, \n                             resolution = 50,\n                             verbose = FALSE, \n                             conf_int = 0.05)\n\n\nkfun_childcare$plotk\n\n\n\n\n\n\n\n\n\nkfun_childcare$plotg"
  },
  {
    "objectID": "Take-home_Ex/index.html",
    "href": "Take-home_Ex/index.html",
    "title": "In-Class Exercise",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nTake Home Exercise 1\n\n\nIn this exercise, we will apply spatial and spatio-temporal point pattern analysis methods to identify factors affecting road traffic accidents in the Bangkok Metropolitan Region (BMR), including visualizing spatio-temporal dynamics, conducting spatial analysis using Network Spatial Point Patterns, and analyzing spatio-temporal patterns using Temporal Network Spatial Point Patterns.\n\n\n62 min\n\n\n\nSep 5, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Exploration/CSDS/01-spatial-data-handling/index.html",
    "href": "Exploration/CSDS/01-spatial-data-handling/index.html",
    "title": "01 Spatial Data Handline",
    "section": "",
    "text": "Spatial Data Handling by Luc Anselin and Grant Morrison"
  },
  {
    "objectID": "Exploration/CSDS/01-spatial-data-handling/index.html#exercise-reference",
    "href": "Exploration/CSDS/01-spatial-data-handling/index.html#exercise-reference",
    "title": "01 Spatial Data Handline",
    "section": "",
    "text": "Spatial Data Handling by Luc Anselin and Grant Morrison"
  },
  {
    "objectID": "Exploration/CSDS/01-spatial-data-handling/index.html#introduction",
    "href": "Exploration/CSDS/01-spatial-data-handling/index.html#introduction",
    "title": "01 Spatial Data Handline",
    "section": "2 Introduction",
    "text": "2 Introduction\nIn this lab, we will use the City of Chicago open data portal to download data on abandoned vehicles. Our end goal is to create a choropleth map with abandoned vehicles per capita for Chicago community areas. Before we can create the maps, we will need to download the information, select observations, aggregate data, join different files and carry out variable transformations in order to obtain a so-called “spatially intensive” variable for mapping (i.e., not just a count of abandoned vehicles, but a per capita ratio)."
  },
  {
    "objectID": "Exploration/CSDS/01-spatial-data-handling/index.html#learning-outcome",
    "href": "Exploration/CSDS/01-spatial-data-handling/index.html#learning-outcome",
    "title": "01 Spatial Data Handline",
    "section": "3 Learning Outcome",
    "text": "3 Learning Outcome\nLearn how to carry out the following tasks:\n\nDownload data from any Socrata-driven open data portal, such as the City of Chicago open data portal\nFiltering a data frame for specific entries\nSelecting and renaming columns\nCreating a simple features spatial object\nChecking and adding/adjusting projection information\nDealing with missing data\nSpatial join\nSpatial aggregation\nParsing a pdf file\nMerging data sets\nCreating new variables\nBasic choropleth mapping"
  },
  {
    "objectID": "Exploration/CSDS/01-spatial-data-handling/index.html#r-packages-used",
    "href": "Exploration/CSDS/01-spatial-data-handling/index.html#r-packages-used",
    "title": "01 Spatial Data Handline",
    "section": "4 R Packages Used",
    "text": "4 R Packages Used\n\n\n\n\n\n\n\nPackage\nDescription\n\n\n\n\nRSocrata\nTo read data directly from a Socrata powered open data portal, such as the Chicago open data portal.\n\n\ntidyverse (includes dplyr)\nTo manipulate data frames, such as filtering data, selecting columns, and creating new variables.\n\n\nlubridate\nTo select information out of the date format when filtering the data.\n\n\nsf\nTo create and manipulate simple features spatial objects, to read in the boundary file, and perform point in polygon on the data set to fill in missing community area information.\n\n\npdftools\nTo read and parse a PDF for Chicago community area population information.\n\n\ntmap\nTo make nice-looking choropleth maps.\n\n\n\n\n4.1 R Commands Used\nBelow follows a list of the commands used in this notebook. For further details and a comprehensive list of options, please consult the R documentation. Here is the information in a markdown table format:\n\n\n\n\n\n\n\nPackage\nFunctions\n\n\n\n\nbase R\nsetwd, install.packages, library, head, dim, class, as.Date, names, !is.na, is.numeric, as.integer, is.integer, length, strsplit, unlist, for, vector, substr, gsub, as.numeric, data.frame\n\n\nRSocrata\nread.socrata\n\n\ntidyverse\nfilter, %&gt;% (pipe), select (with renaming), count, rename, mutate\n\n\nlubridate\nyear, month\n\n\nsf\nst_as_sf, plot, st_crs, read_sf, st_transform, st_join, st_geometry, st_write\n\n\npdftools\npdf_text\n\n\ntmap\ntm_shape, tm_polygons\n\n\n\nImport the required R packages mentioned above\n\npacman::p_load(tidyverse, lubridate, sf, tmap, pdftools, RSocrata)"
  },
  {
    "objectID": "Exploration/CSDS/01-spatial-data-handling/index.html#obtaining-data-from-the-chicago-open-data-portal",
    "href": "Exploration/CSDS/01-spatial-data-handling/index.html#obtaining-data-from-the-chicago-open-data-portal",
    "title": "01 Spatial Data Handline",
    "section": "5 Obtaining data from the Chicago Open Data portal",
    "text": "5 Obtaining data from the Chicago Open Data portal\nWe will use the specialized RSocrata package to download the file with 311 calls about abandoned vehicles from the City of Chicago open data portal. A list of different types of 311 nuisance calls is given by selecting the button for Service Requests. The abandoned vehicles data are contained in the entry for 311 Service Requests - Abandoned Vehicles.\nTo download the file, select the API button and copy the API Endpoint from the interface. This endpoint will be the target file URL. Instead of directly using the read.socrata function from the RSocrata package, we will first check if the file already exists in our local directory (../data). If it does not exist, we will download it from the City of Chicago open data portal and save it locally. If the file already exists, we will simply read it from the local directory, avoiding redundant downloads. ### Read using RSocrata\n\nsocrata.file &lt;- \"https://data.cityofchicago.org/resource/suj7-cg3j.csv\"\nlocal.file &lt;- \"../data/suj7-cg3j.csv\"\n\nif (!file.exists(local.file)) {\n  vehicle.data &lt;- read.socrata(socrata.file)\n  write_csv(vehicle.data, local.file)\n  print(\"Data downloaded and saved locally\")\n} else {\n  vehicle.data &lt;- read_csv(local.file)\n  print(\"Data loaded from local\")\n}\n\n[1] \"Data loaded from local\"\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you try other ways to obtain this file, you may obtain a different variant of this data.\nFor example, if you try to download the csv file direct via the URL path, you may only obtain 1000 rows as there is a rate limit on the API.\nIf you download the csv file directly from the web portal, the CSV is truncated and may contain different/missing columns.\n\n\n\ndim(vehicle.data)\n\n[1] 261486     26\n\n\nThe table has 261,486 observations on 26 variables.\nIn RStudio, the type of the variable in each column is listed under its name. For example, under creation_date, we see S3: POSIXct. You can also find out the same information by applying a class command to the variable vehicle.data$creation_date, as in\n\nclass(vehicle.data$creation_date)\n\n[1] \"POSIXct\" \"POSIXt\" \n\n\n\n\n\n\n\n\nTip\n\n\n\nAlternatively, to load the data with the correct class from a CSV, use tidyverse’s read_csv instead of base R’s read.csv. Without the correct data class, you may have to perform manual conversion before you can join data frame in downstream tasks.\n\n\n\n5.1 Extracting observations for the desired time period\nTo extract the observations for the selected year (2016) and month (9), we will use the year and month functions from the lubridate package. We will embed these expressions in a filter command (from tidyverse) to select the rows/observations that match the specified criterion. We will also use the pipe command %&gt;% to move the original data frame through the different filter stages and assign the end result to vehicle.sept16.\nWe again check the contents with a head command.\n\nvehicle.sept16 &lt;- vehicle.data %&gt;% filter(year(creation_date) == 2016) %&gt;%\n                  filter(month(creation_date) == 9)\nhead(vehicle.sept16)\n\n# A tibble: 6 × 26\n  creation_date       status          completion_date     service_request_number\n  &lt;dttm&gt;              &lt;chr&gt;           &lt;dttm&gt;              &lt;chr&gt;                 \n1 2016-09-01 16:00:00 Completed - Dup 2016-09-01 16:00:00 16-06219980           \n2 2016-09-01 16:00:00 Completed - Dup 2016-09-01 16:00:00 16-06220033           \n3 2016-09-01 16:00:00 Completed - Dup 2016-09-01 16:00:00 16-06220056           \n4 2016-09-01 16:00:00 Completed - Dup 2016-09-01 16:00:00 16-06220096           \n5 2016-09-01 16:00:00 Completed - Dup 2016-09-01 16:00:00 16-06221253           \n6 2016-09-01 16:00:00 Completed - Dup 2016-09-01 16:00:00 16-06225666           \n# ℹ 22 more variables: type_of_service_request &lt;chr&gt;, license_plate &lt;chr&gt;,\n#   vehicle_make_model &lt;chr&gt;, vehicle_color &lt;chr&gt;, current_activity &lt;chr&gt;,\n#   most_recent_action &lt;chr&gt;,\n#   how_many_days_has_the_vehicle_been_reported_as_parked_ &lt;dbl&gt;,\n#   street_address &lt;chr&gt;, zip_code &lt;dbl&gt;, x_coordinate &lt;dbl&gt;,\n#   y_coordinate &lt;dbl&gt;, ward &lt;dbl&gt;, police_district &lt;dbl&gt;,\n#   community_area &lt;dbl&gt;, ssa &lt;dbl&gt;, latitude &lt;dbl&gt;, longitude &lt;dbl&gt;, …\n\n\nand the dimension:\n\ndim(vehicle.sept16)\n\n[1] 2555   26\n\n\nThe filtered table now only has 2,637 observations.\n\n\n5.2 Selecting the variables for the final table\nThe current data frame contains 26 variables. Several of these are not really of interest to us, since we basically want the locations of the events. We will use the select command from tidyverse to pick out the columns that we want to keep. In addition, we will use the rename option in select to give new variable names. While this is not absolutely necessary at this stage (RSocrata has turned any weird variable names into proper R names), we may later want to save the data as a point shape file. The data associated with a shape file are store in a separate dBase file, and dBase only allows 10 characters for variable names.\nSo, in order to save ourselves some work later on, we will rename the selected variables to strings that do not exceed 10 characters.\nFirst, we check the variable names using the names command.\n\nnames(vehicle.sept16)\n\n [1] \"creation_date\"                                         \n [2] \"status\"                                                \n [3] \"completion_date\"                                       \n [4] \"service_request_number\"                                \n [5] \"type_of_service_request\"                               \n [6] \"license_plate\"                                         \n [7] \"vehicle_make_model\"                                    \n [8] \"vehicle_color\"                                         \n [9] \"current_activity\"                                      \n[10] \"most_recent_action\"                                    \n[11] \"how_many_days_has_the_vehicle_been_reported_as_parked_\"\n[12] \"street_address\"                                        \n[13] \"zip_code\"                                              \n[14] \"x_coordinate\"                                          \n[15] \"y_coordinate\"                                          \n[16] \"ward\"                                                  \n[17] \"police_district\"                                       \n[18] \"community_area\"                                        \n[19] \"ssa\"                                                   \n[20] \"latitude\"                                              \n[21] \"longitude\"                                             \n[22] \"location\"                                              \n[23] \"location_address\"                                      \n[24] \"location_city\"                                         \n[25] \"location_state\"                                        \n[26] \"location_zip\"                                          \n\n\nTo keep things simple, we will only keep community_area, latitude and longitude, and turn them into comm, lat and lon. The new data set is vehicles.final. Note that to rename a variable, the new name is listed first, on the left hand side of the equal sign, and the old name is on the right hand side. We check the result with the head command.\n\nvehicles.final &lt;- vehicle.sept16 %&gt;% select(comm = community_area, \n                          lat = latitude, lon = longitude)\nhead(vehicles.final)\n\n# A tibble: 6 × 3\n   comm   lat   lon\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    17  41.9 -87.8\n2    20  41.9 -87.7\n3    20  41.9 -87.7\n4     1  42.0 -87.7\n5    15  42.0 -87.7\n6    70  41.7 -87.7"
  },
  {
    "objectID": "Exploration/CSDS/01-spatial-data-handling/index.html#creating-a-point-layer",
    "href": "Exploration/CSDS/01-spatial-data-handling/index.html#creating-a-point-layer",
    "title": "01 Spatial Data Handline",
    "section": "6 Creating a Point Layer",
    "text": "6 Creating a Point Layer\nSo far, we have only dealt with a regular data frame, without taking advantage of any spatial features. However, the data frame contains fields with coordinates and R can turn these into an explicit spatial points layer that can be saved in a range of GIS formats. To accomplish this, we will use the (new) simple features or sf package functionality, which improves upon the older sp.\nWe will first use the lat and lon columns in the data frame to create a spatial points object. Note that lon is the x-coordinate and lat is the y-coordinate.\n\n6.1 Creating a point layer from coordinates in a table - principle\nIn sf, a simple features object is constructed by combining a geometry with the actual data (in a data frame). However, this is simplified for point objects when the data frame contains the coordinates as variables. This is the case in our example, where we have latitude and longitude. We also have x and y, but since we are not sure what projection these coordinates correspond with, they are not useful at this stage.\nThe advantage of lat-lon is that they are decimal degrees, and thus unprojected. However, we can provide the information on the datum, typically WGS84 (the standard used in most applications for decimal degrees) by passing the coordinate reference system argument (crs) set to the EPSG code 4326. After that, we can use the built-in projection transformation functionality in sf to turn the points into any projection we want.1\n\n6.1.1 Missing coordinates\nIn order to create a points layer, we need coordinates for every observation. However, as we can see from the head command above, there are (at least) two observations that do not have lat-lon information. Before we can proceed, we need to remove these from the data frame.\nWe again use a filter command, but now combine it with the !is.na expression, i.e., is not missing (na). We take a little short cut by assuming that if one of lat or lon is missing, the other one will be missing as well (although to keep it completely general, we would need to check each variable separately). We assign the result to the vehicle.coord data frame.\n\nvehicle.coord &lt;- vehicles.final %&gt;% filter(!(is.na(lat)))\ndim(vehicle.coord)\n\n[1] 2553    3\n\n\nThere are 2 records with missing coordinates, so we will omit them. The data records reduce from 2637 to 2635.\n\n\n\n6.2 Creating a spatial points object\nThe sf package turns a non-spatial object like a data frame into a simple features spatial object by means of the st_as_sf function. This function can take a large number of arguments, but for now we will only use a few:\n\nthe name of the data frame, i.e., vehicle.coord\ncoords: the variable names for x and y (given in parentheses)\ncrs: the coordinate reference system, here using the EPSG code of 4326\nagr: the so-called attibute-geometry-relationship which specifies how the attribute information (the data) relate to the geometry (the points); in our example, we will use “constant”\n\nIn our example, we create vehicle.points and check its class.\n\nvehicle.points = st_as_sf(vehicle.coord, coords = c(\"lon\", \"lat\"), crs = 4326, agr = \"constant\")\nclass(vehicle.points)\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nEven though it is not that informative at this stage, we can also make a quick plot. Later, we will see how we can refine these plots using the tmap package.\n\nplot(vehicle.points)\n\n\n\n\n\n\n\n\nWe can also do a quick check of the projection information using the st_crs command.\n\nst_crs(vehicle.points)\n\nCoordinate Reference System:\n  User input: EPSG:4326 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n        MEMBER[\"World Geodetic System 1984 (Transit)\"],\n        MEMBER[\"World Geodetic System 1984 (G730)\"],\n        MEMBER[\"World Geodetic System 1984 (G873)\"],\n        MEMBER[\"World Geodetic System 1984 (G1150)\"],\n        MEMBER[\"World Geodetic System 1984 (G1674)\"],\n        MEMBER[\"World Geodetic System 1984 (G1762)\"],\n        MEMBER[\"World Geodetic System 1984 (G2139)\"],\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ENSEMBLEACCURACY[2.0]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]"
  },
  {
    "objectID": "Exploration/CSDS/01-spatial-data-handling/index.html#abandoned-vehicles-by-community-area",
    "href": "Exploration/CSDS/01-spatial-data-handling/index.html#abandoned-vehicles-by-community-area",
    "title": "01 Spatial Data Handline",
    "section": "7 Abandoned Vehicles by Community Area",
    "text": "7 Abandoned Vehicles by Community Area\nAt this point, we will go about things in a slightly different way from how they are illustrated in the GeoDa workbook example. As it turns out, some of the points have missing community area information, which is a critical element to compute the number of abandoned cars at that scale. In GeoDa, we used a visual approach to obtain the missing information. Here, we will exploit some of the GIS functionality in sf to carry out a spatial join. This boils down to identifying which points belong to each community area (a so-called point in polygon query) and assigning the corresponding community area identifier to each point.\nWe proceed in three steps. First, we create a simple features spatial polygon object with the boundaries of the community areas, which we download from the Chicago Open Data portal. Next, we carry out a spatial join between our points object and the polygon object to assign a community area code to each point. Finally, we compute the point count by community area.\n\nCommunity Area boundary file\nWe resort to the City of Chicago open data portal for the boundary file of the community areas. From the opening screen, select the button for Facilities & Geo Boundaries. This yields a list of different boundary files for a range of geographic areal units. The one for the community areas is Boundaries - Community Areas (current). This brings up an overview map of the geography of the community areas of Chicago. Of course, we could simply select one of the export buttons to download the files, but we want to do this programmatically. As it turns out, sf can read a geojson formatted file directly from the web, and we will exploit that functionality.\nFirst, we need the name for the file. We can check the Socrata API file name, but that contains a json file, and we want a specific geojson file. As it turns out, the latter is simply the same file name, but with the geojson file extension. We set our variable comm.file to this URL and then use sf_read to load the boundary information into chicago.comm. As before, we can do a quick check of the class using the class command.\n\ncomm.file &lt;- \"https://data.cityofchicago.org/resource/igwz-8jzy.geojson\"\ncomm.local &lt;- \"../data/igwz-8jzy.geojson\"\n\nif (!file.exists(comm.local)) {\n  download.file(comm.file, comm.local, method = \"curl\")\n  print(\"File downloaded and saved locally\")\n} else {\n  print(\"File exists locally. Reading locally\")\n}\n\n[1] \"File exists locally. Reading locally\"\n\nchicago.comm &lt;- read_sf(comm.local)\nclass(chicago.comm)\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\n\nclass(chicago.comm$area_num_1)\n\n[1] \"character\"\n\n\nIn addition, we check the projection information using st_crs.\n\nst_crs(chicago.comm)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n\nAgain, the layer is unprojected in decimal degrees. Also, a quick plot. Note that, by default, sf draws a choropleth map for each variable included in the data frame. Since we won’t be using sf for mapping, we ignore that aspect for now.\n\nplot(chicago.comm)\n\n\n\n\n\n\n\n\nWe also use head to check on the types of the variables.\n\nhead(chicago.comm)\n\nSimple feature collection with 6 features and 9 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -87.7069 ymin: 41.79448 xmax: -87.58001 ymax: 41.99076\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 10\n  community  area  shape_area perimeter area_num_1 area_numbe comarea_id comarea\n  &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;  \n1 DOUGLAS    0     46004621.… 0         35         35         0          0      \n2 OAKLAND    0     16913961.… 0         36         36         0          0      \n3 FULLER PA… 0     19916704.… 0         37         37         0          0      \n4 GRAND BOU… 0     48492503.… 0         38         38         0          0      \n5 KENWOOD    0     29071741.… 0         39         39         0          0      \n6 LINCOLN S… 0     71352328.… 0         4          4          0          0      \n# ℹ 2 more variables: shape_len &lt;chr&gt;, geometry &lt;MULTIPOLYGON [°]&gt;\n\n\n\n7.0.1 Changing projections\nBefore moving on to the spatial join operation, we will convert both the community area boundaries and the vehicle points to the same projection, using the st_transform command. We assign the UTM (Universal Tranverse Mercator) zone 16N, which the the proper one for Chicago, with an EPSG code of 32616. After the projection transformation, we check the result using st_crs.\n\nchicago.comm &lt;- st_transform(chicago.comm,32616)\nst_crs(chicago.comm)\n\nCoordinate Reference System:\n  User input: EPSG:32616 \n  wkt:\nPROJCRS[\"WGS 84 / UTM zone 16N\",\n    BASEGEOGCRS[\"WGS 84\",\n        ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n            MEMBER[\"World Geodetic System 1984 (Transit)\"],\n            MEMBER[\"World Geodetic System 1984 (G730)\"],\n            MEMBER[\"World Geodetic System 1984 (G873)\"],\n            MEMBER[\"World Geodetic System 1984 (G1150)\"],\n            MEMBER[\"World Geodetic System 1984 (G1674)\"],\n            MEMBER[\"World Geodetic System 1984 (G1762)\"],\n            MEMBER[\"World Geodetic System 1984 (G2139)\"],\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ENSEMBLEACCURACY[2.0]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"UTM zone 16N\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",-87,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"Between 90°W and 84°W, northern hemisphere between equator and 84°N, onshore and offshore. Belize. Canada - Manitoba; Nunavut; Ontario. Costa Rica. Cuba. Ecuador - Galapagos. El Salvador. Guatemala. Honduras. Mexico. Nicaragua. United States (USA).\"],\n        BBOX[0,-90,84,-84]],\n    ID[\"EPSG\",32616]]\n\n\n\nvehicle.points &lt;- st_transform(vehicle.points,32616)\nst_crs(vehicle.points)\n\nCoordinate Reference System:\n  User input: EPSG:32616 \n  wkt:\nPROJCRS[\"WGS 84 / UTM zone 16N\",\n    BASEGEOGCRS[\"WGS 84\",\n        ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n            MEMBER[\"World Geodetic System 1984 (Transit)\"],\n            MEMBER[\"World Geodetic System 1984 (G730)\"],\n            MEMBER[\"World Geodetic System 1984 (G873)\"],\n            MEMBER[\"World Geodetic System 1984 (G1150)\"],\n            MEMBER[\"World Geodetic System 1984 (G1674)\"],\n            MEMBER[\"World Geodetic System 1984 (G1762)\"],\n            MEMBER[\"World Geodetic System 1984 (G2139)\"],\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ENSEMBLEACCURACY[2.0]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"UTM zone 16N\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",-87,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"Between 90°W and 84°W, northern hemisphere between equator and 84°N, onshore and offshore. Belize. Canada - Manitoba; Nunavut; Ontario. Costa Rica. Cuba. Ecuador - Galapagos. El Salvador. Guatemala. Honduras. Mexico. Nicaragua. United States (USA).\"],\n        BBOX[0,-90,84,-84]],\n    ID[\"EPSG\",32616]]\n\n\n\n\n\n7.1 Spatial join\nIn essence, the spatial join operation finds the polygon to which each point belongs. Several points belong to the same polygon, so this is a many-to-one join. Instead of joining all the features of the polygon layer, we specify just area_num_1, which is the community area indicator. The command is st_join to which we pass the point layer as the first sf object, and the polygon layer as the second sf object (with only one column designated). We assign the result to the new spatial object comm.pts. We check the contents of the new object using a head command.\n\ncomm.pts &lt;- st_join(vehicle.points,chicago.comm[\"area_num_1\"])\nhead(comm.pts)\n\nSimple feature collection with 6 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 432913.8 ymin: 4621705 xmax: 443548.5 ymax: 4651818\nProjected CRS: WGS 84 / UTM zone 16N\n# A tibble: 6 × 3\n   comm           geometry area_num_1\n  &lt;dbl&gt;        &lt;POINT [m]&gt; &lt;chr&gt;     \n1    17 (432913.8 4642827) 17        \n2    20 (438997.5 4641020) 20        \n3    20 (438997.5 4641020) 20        \n4     1 (443548.5 4651818) 1         \n5    15 (438145.2 4645227) 15        \n6    70 (440657.5 4621705) 70        \n\n\nAs we can see, the community area in comm matches the entry in area_num_1. However, there is one more issue to deal with. Upon closer examination, we find that the area_num_1 variable is not numeric using the is.numeric check.\n\nis.numeric(comm.pts$area_num_1)\n\n[1] FALSE\n\n\nSo, we proceed to turn this variable into a numeric format using as.integer and then do a quick check by means of is.integer.\n\ncomm.pts$area_num_1 &lt;- as.integer(comm.pts$area_num_1)\nis.integer(comm.pts$area_num_1)\n\n[1] TRUE\n\n\nThe same problem occurs in the chicago.comm data set, which can cause trouble later on when we will join it with other data. Therefore, we turn it into an integer as well.\n\nchicago.comm$area_num_1 &lt;- as.integer(chicago.comm$area_num_1)\n\n\n\n7.2 Counts by community area\nWe now need to count the number of points in each polygon. We proceed in two steps. First, we illustrate how we can move back from the simple features spatial points object to a simple data frame by stripping the geometry column. This is accomplished by setting st_geometry to NULL. We check the class of the new object to make sure it is no longer a simple feature.\n\nst_geometry(comm.pts) &lt;- NULL\nclass(comm.pts)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nWe next take advantage of the tidyverse count function to create a new data frame with the identifier of the community area and the number of points contained in each community area.\n\nveh.cnts &lt;- comm.pts %&gt;% count(area_num_1)\nhead(veh.cnts)\n\n# A tibble: 6 × 2\n  area_num_1     n\n       &lt;int&gt; &lt;int&gt;\n1          1    63\n2          2    91\n3          3    22\n4          4    31\n5          5    18\n6          6    20\n\n\nThe new data frame has two fields: the original identifier area_num_1 and the count as n. We can change the variable names for the count to something more meaningful by means of the tidyverse rename command and turn it from n to AGG.COUNT (to use the same variable as in the GeoDa workbook). Similarly, we also shorten area_num_1 to comm. Again, the new name is on the LHS of the equal sign and the old name on the RHS.\n\nveh.cnts &lt;- veh.cnts %&gt;% rename(comm = area_num_1, AGG.COUNT = n)\nhead(veh.cnts)\n\n# A tibble: 6 × 2\n   comm AGG.COUNT\n  &lt;int&gt;     &lt;int&gt;\n1     1        63\n2     2        91\n3     3        22\n4     4        31\n5     5        18\n6     6        20\n\n\n\n\n7.3 Mapping the vehicle counts\nAt this point, we have a polygon layer with the community area boundaries and some identifiers (chicago.comm) and a data frame with the community identifier and the aggregate vehicle count (veh.cnts). In order to map the vehicle counts by community area, we need to join the two tables. We use the left_join command and use area_num_1 as the key for the first table (the community area boundaries), and comm as the key for the second table (the vehicle counts). Since we assured that both variables are now integers, the join will work (if one were a character and the other integer, there would be an error message). Note how in the command below, the two keys can have different variable names (but they must have the same values), which is made explicit in the by statement.\n\nchicago.comm &lt;- left_join(chicago.comm,veh.cnts, by = c(\"area_num_1\" = \"comm\"))\n\nWe can double check that the vehicle counts were added using the head command.\n\nhead(chicago.comm)\n\nSimple feature collection with 6 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 441440.4 ymin: 4627153 xmax: 451817.1 ymax: 4648971\nProjected CRS: WGS 84 / UTM zone 16N\n# A tibble: 6 × 11\n  community  area  shape_area perimeter area_num_1 area_numbe comarea_id comarea\n  &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;          &lt;int&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;  \n1 DOUGLAS    0     46004621.… 0                 35 35         0          0      \n2 OAKLAND    0     16913961.… 0                 36 36         0          0      \n3 FULLER PA… 0     19916704.… 0                 37 37         0          0      \n4 GRAND BOU… 0     48492503.… 0                 38 38         0          0      \n5 KENWOOD    0     29071741.… 0                 39 39         0          0      \n6 LINCOLN S… 0     71352328.… 0                  4 4          0          0      \n# ℹ 3 more variables: shape_len &lt;chr&gt;, geometry &lt;MULTIPOLYGON [m]&gt;,\n#   AGG.COUNT &lt;int&gt;\n\n\n\n7.3.1 Basic choropleth map\nAs we saw earlier, we can construct rudimentary maps using the plot command in sf, but for further control, we will use the tmap package. This uses a logic similar to Wilkinson’s grammar of graphics, which is also the basis for the structure of the plot commands in the ggplot package.\nWe leave a detailed treatment of tmap for a future lab and just use the basic defaults in this example. The commands are layered and always start by specifying a layer using the tm_shape command. In our example, this is chicago.comm. Next (after the plus sign) follow one of more drawing commands that cover a wide range of geographic shapes. Here, we will just use tm_polygons and specify AGG.COUNT as the variable to determine the classification. We leave everything to the default and obtain a map that illustrates the spatial distribution of the abandoned vehicle counts by community area.\n\ntm_shape(chicago.comm) +\n  tm_polygons(\"AGG.COUNT\")"
  },
  {
    "objectID": "Exploration/CSDS/01-spatial-data-handling/index.html#community-area-population-data",
    "href": "Exploration/CSDS/01-spatial-data-handling/index.html#community-area-population-data",
    "title": "01 Spatial Data Handline",
    "section": "8 Community Area Population Data",
    "text": "8 Community Area Population Data\nThe Chicago Community Area 2010 population is contained in a pdf file, available from the City of Chicago web site.\nThis link is to a pdf file that contains a table with the neighborhood ID, the neighborhood name, the populations for 2010 and 2000, the difference between the two years and the percentage difference. The full path to the pdf file is https://www.cityofchicago.org/content/dam/city/depts/zlup/Zoning_Main_Page/Publications/Census_2010_Community_Area_Profiles/Census_2010_and_2000_CA_Populations.pdf\n\n8.1 Extracting a pdf file\nA pdf file is difficult to handle as a source of data, since it doesn’t contain tags like an html file. We will use the pdftools package that allows us to turn the contents of a pdf file into a list of long character strings.\nThe resulting data structure is somewhat complex and not necessarily easy to parse. However, in our case, the table has such a simple structure that we can extract the population values by doing some sleuthing on which columns contain those values. This will illustrate the power of the various parsing and text extraction functions available in R.\nWe use the pdf_text function from pdftools to turn the pdf file into a list of character strings, one for each page. We specify the URL of the file as the input source.\n\npdf.file &lt;- \"https://www.cityofchicago.org/content/dam/city/depts/zlup/Zoning_Main_Page/Publications/Census_2010_Community_Area_Profiles/Census_2010_and_2000_CA_Populations.pdf\"\nlocal.file &lt;- \"../data/Census_2010_and_2000_CA_Populations.pdf\"\n\nif (!file.exists(local.file)) {\n  download.file(pdf.file, local.file, method = \"curl\")\n  print(\"PDF file downloaded and saved locally\")\n} else {\n  print(\"PDF file exists locally. Reading from local\")\n}\n\n[1] \"PDF file exists locally. Reading from local\"\n\npop.dat &lt;- pdf_text(local.file)\nclass(pop.dat)\n\n[1] \"character\"\n\n\nWe check the length of the data object using the length command and find that indeed it has only two elements (one for each page).\n\nlength(pop.dat)\n\n[1] 2\n\n\n\n\nParsing the pdf file\nThe pop.dat object has two entries, one for each page. Each entry is a single string. So, when you check the length of each item, it may be surprising that its length is only 1. That is because the underlying structure is unknown, it is simply a collection of characters contained in the string. For example, the first element, pop.dat[[1]]:\n\nlength(pop.dat[[1]])\n\n[1] 1\n\n\nWe will parse this file by first turning each element into a separate list and then extracting the parts we are interested in.\nFirst, to illustrate in detail what is going on, we will go through each step one by one, but then, in order to reach some level of efficiency, we turn it into a loop over the two elements, for (i in 1:2).\nWe start by initializing a vector (nnlist) with an empty character, and confirm that it is indeed initialized.\n\nnnlist &lt;- \"\"\nnnlist\n\n[1] \"\"\n\n\nNext, we create a list of strings, one for each line in the table, by using the strsplit operation. This splits the long string into a list of one string for each line, by using the return character \\n as the separator (the value for the split argument).\nThe resulting list, ppage, contains a list of 44 elements, matching the contents of the first page of the pdf file.\n\nppage &lt;- strsplit(pop.dat[[1]],split=\"\\n\")\nppage[[1]]\n\n [1] \"                              CITY OF CHICAGO\"                                \n [2] \"                            CENSUS 2010 AND 2000\"                             \n [3] \"\"                                                                             \n [4] \"                                                Population\"                   \n [5] \"Num        Community Area       2010        2,000     Difference   Percentage\"\n [6] \" 1    Rogers Park                54,991     63,484      -8,493       -13.4%\"  \n [7] \" 2    West Ridge                 71,942     73,199      -1,257        -1.7%\"  \n [8] \" 3    Uptown                     56,362     63,551      -7,189       -11.3%\"  \n [9] \" 4    Lincoln Square             39,493     44,574      -5,081       -11.4%\"  \n[10] \" 5    North Center               31,867     31,895        -28         -0.1%\"  \n[11] \" 6    Lake View                  94,368     94,817       -449         -0.5%\"  \n[12] \" 7    Lincoln Park               64,116     64,320       -204         -0.3%\"  \n[13] \" 8    Near North Side            80,484     72,811      7,673         10.5%\"  \n[14] \" 9    Edison Park                11,187     11,259        -72         -0.6%\"  \n[15] \" 10   Norwood Park               37,023     37,669       -646         -1.7%\"  \n[16] \" 11   Jefferson Park             25,448     25,859       -411         -1.6%\"  \n[17] \" 12   Forest Glen                18,508     18,165        343         1.9%\"   \n[18] \" 13   North Park                 17,931     18,514       -583         -3.1%\"  \n[19] \" 14   Albany Park                51,542     57,655      -6,113       -10.6%\"  \n[20] \" 15   Portage Park               64,124     65,340      -1,216        -1.9%\"  \n[21] \" 16   Irving Park                53,359     58,643      -5,284        -9.0%\"  \n[22] \" 17   Dunning                    41,932     42,164       -232         -0.6%\"  \n[23] \" 18   Montclare                  13,426     12,646        780         6.2%\"   \n[24] \" 19   Belmont Cragin             78,743     78,144        599         0.8%\"   \n[25] \" 20   Hermosa                    25,010     26,908      -1,898        -7.1%\"  \n[26] \" 21   Avondale                   39,262     43,083      -3,821        -8.9%\"  \n[27] \" 22   Logan Square               73,595     82,715      -9,120       -11.0%\"  \n[28] \" 23   Humboldt Park              56,323     65,836      -9,513       -14.4%\"  \n[29] \" 24   West Town                  81,432     87,435      -6,003        -6.9%\"  \n[30] \" 25   Austin                     98,514    117,527     -19,013       -16.2%\"  \n[31] \" 26   West Garfield Park         18,001     23,019      -5,018       -21.8%\"  \n[32] \" 27   East Garfield Park         20,567     20,881       -314         -1.5%\"  \n[33] \" 28   Near West Side             54,881     46,419      8,462         18.2%\"  \n[34] \" 29   North Lawndale             35,912     41,768      -5,856       -14.0%\"  \n[35] \" 30   South Lawndale             79,288     91,071     -11,783       -12.9%\"  \n[36] \" 31   Lower West Side            35,769     44,031      -8,262       -18.8%\"  \n[37] \" 32   Loop                       29,283     16,388      12,895        78.7%\"  \n[38] \" 33   Near South Side            21,390     9,509       11,881       124.9%\"  \n[39] \" 34   Armour Square              13,391     12,032      1,359         11.3%\"  \n[40] \" 35   Douglas                    18,238     26,470      -8,232       -31.1%\"  \n[41] \" 36   Oakland                     5,918     6,110        -192         -3.1%\"  \n[42] \" 37   Fuller Park                 2,876     3,420        -544        -15.9%\"  \n[43] \" 38   Grand Boulevard            21,929     28,006      -6,077       -21.7%\"  \n[44] \" 39   Kenwood                    17,841     18,363       -522         -2.8%\"  \n[45] \" 40   Washington Park            11,717     14,146      -2,429       -17.2%\"  \n\n\nEach element is one long string, corresponding to a table row. We remove the first four lines (using the - operation on the list elements 1 through 4). These first rows appear on each page, so we are safe to repeat this procedure for the second page (string) as well.\n\nnni &lt;- ppage[[1]]\nnni &lt;- nni[-(1:4)]\nnni\n\n [1] \"Num        Community Area       2010        2,000     Difference   Percentage\"\n [2] \" 1    Rogers Park                54,991     63,484      -8,493       -13.4%\"  \n [3] \" 2    West Ridge                 71,942     73,199      -1,257        -1.7%\"  \n [4] \" 3    Uptown                     56,362     63,551      -7,189       -11.3%\"  \n [5] \" 4    Lincoln Square             39,493     44,574      -5,081       -11.4%\"  \n [6] \" 5    North Center               31,867     31,895        -28         -0.1%\"  \n [7] \" 6    Lake View                  94,368     94,817       -449         -0.5%\"  \n [8] \" 7    Lincoln Park               64,116     64,320       -204         -0.3%\"  \n [9] \" 8    Near North Side            80,484     72,811      7,673         10.5%\"  \n[10] \" 9    Edison Park                11,187     11,259        -72         -0.6%\"  \n[11] \" 10   Norwood Park               37,023     37,669       -646         -1.7%\"  \n[12] \" 11   Jefferson Park             25,448     25,859       -411         -1.6%\"  \n[13] \" 12   Forest Glen                18,508     18,165        343         1.9%\"   \n[14] \" 13   North Park                 17,931     18,514       -583         -3.1%\"  \n[15] \" 14   Albany Park                51,542     57,655      -6,113       -10.6%\"  \n[16] \" 15   Portage Park               64,124     65,340      -1,216        -1.9%\"  \n[17] \" 16   Irving Park                53,359     58,643      -5,284        -9.0%\"  \n[18] \" 17   Dunning                    41,932     42,164       -232         -0.6%\"  \n[19] \" 18   Montclare                  13,426     12,646        780         6.2%\"   \n[20] \" 19   Belmont Cragin             78,743     78,144        599         0.8%\"   \n[21] \" 20   Hermosa                    25,010     26,908      -1,898        -7.1%\"  \n[22] \" 21   Avondale                   39,262     43,083      -3,821        -8.9%\"  \n[23] \" 22   Logan Square               73,595     82,715      -9,120       -11.0%\"  \n[24] \" 23   Humboldt Park              56,323     65,836      -9,513       -14.4%\"  \n[25] \" 24   West Town                  81,432     87,435      -6,003        -6.9%\"  \n[26] \" 25   Austin                     98,514    117,527     -19,013       -16.2%\"  \n[27] \" 26   West Garfield Park         18,001     23,019      -5,018       -21.8%\"  \n[28] \" 27   East Garfield Park         20,567     20,881       -314         -1.5%\"  \n[29] \" 28   Near West Side             54,881     46,419      8,462         18.2%\"  \n[30] \" 29   North Lawndale             35,912     41,768      -5,856       -14.0%\"  \n[31] \" 30   South Lawndale             79,288     91,071     -11,783       -12.9%\"  \n[32] \" 31   Lower West Side            35,769     44,031      -8,262       -18.8%\"  \n[33] \" 32   Loop                       29,283     16,388      12,895        78.7%\"  \n[34] \" 33   Near South Side            21,390     9,509       11,881       124.9%\"  \n[35] \" 34   Armour Square              13,391     12,032      1,359         11.3%\"  \n[36] \" 35   Douglas                    18,238     26,470      -8,232       -31.1%\"  \n[37] \" 36   Oakland                     5,918     6,110        -192         -3.1%\"  \n[38] \" 37   Fuller Park                 2,876     3,420        -544        -15.9%\"  \n[39] \" 38   Grand Boulevard            21,929     28,006      -6,077       -21.7%\"  \n[40] \" 39   Kenwood                    17,841     18,363       -522         -2.8%\"  \n[41] \" 40   Washington Park            11,717     14,146      -2,429       -17.2%\"  \n\n\nTo streamline the resulting data structure for further operations, we turn it into a simple vector by means of unlist. This then allows us to concatenate the result to the current nnlist vector (initially, this contains just a single element with an empty character, after the first step it contains the empty character and the first page).\n\nnnu &lt;- unlist(nni)\nnnlist &lt;- c(nnlist,nnu)\nnnlist\n\n [1] \"\"                                                                             \n [2] \"Num        Community Area       2010        2,000     Difference   Percentage\"\n [3] \" 1    Rogers Park                54,991     63,484      -8,493       -13.4%\"  \n [4] \" 2    West Ridge                 71,942     73,199      -1,257        -1.7%\"  \n [5] \" 3    Uptown                     56,362     63,551      -7,189       -11.3%\"  \n [6] \" 4    Lincoln Square             39,493     44,574      -5,081       -11.4%\"  \n [7] \" 5    North Center               31,867     31,895        -28         -0.1%\"  \n [8] \" 6    Lake View                  94,368     94,817       -449         -0.5%\"  \n [9] \" 7    Lincoln Park               64,116     64,320       -204         -0.3%\"  \n[10] \" 8    Near North Side            80,484     72,811      7,673         10.5%\"  \n[11] \" 9    Edison Park                11,187     11,259        -72         -0.6%\"  \n[12] \" 10   Norwood Park               37,023     37,669       -646         -1.7%\"  \n[13] \" 11   Jefferson Park             25,448     25,859       -411         -1.6%\"  \n[14] \" 12   Forest Glen                18,508     18,165        343         1.9%\"   \n[15] \" 13   North Park                 17,931     18,514       -583         -3.1%\"  \n[16] \" 14   Albany Park                51,542     57,655      -6,113       -10.6%\"  \n[17] \" 15   Portage Park               64,124     65,340      -1,216        -1.9%\"  \n[18] \" 16   Irving Park                53,359     58,643      -5,284        -9.0%\"  \n[19] \" 17   Dunning                    41,932     42,164       -232         -0.6%\"  \n[20] \" 18   Montclare                  13,426     12,646        780         6.2%\"   \n[21] \" 19   Belmont Cragin             78,743     78,144        599         0.8%\"   \n[22] \" 20   Hermosa                    25,010     26,908      -1,898        -7.1%\"  \n[23] \" 21   Avondale                   39,262     43,083      -3,821        -8.9%\"  \n[24] \" 22   Logan Square               73,595     82,715      -9,120       -11.0%\"  \n[25] \" 23   Humboldt Park              56,323     65,836      -9,513       -14.4%\"  \n[26] \" 24   West Town                  81,432     87,435      -6,003        -6.9%\"  \n[27] \" 25   Austin                     98,514    117,527     -19,013       -16.2%\"  \n[28] \" 26   West Garfield Park         18,001     23,019      -5,018       -21.8%\"  \n[29] \" 27   East Garfield Park         20,567     20,881       -314         -1.5%\"  \n[30] \" 28   Near West Side             54,881     46,419      8,462         18.2%\"  \n[31] \" 29   North Lawndale             35,912     41,768      -5,856       -14.0%\"  \n[32] \" 30   South Lawndale             79,288     91,071     -11,783       -12.9%\"  \n[33] \" 31   Lower West Side            35,769     44,031      -8,262       -18.8%\"  \n[34] \" 32   Loop                       29,283     16,388      12,895        78.7%\"  \n[35] \" 33   Near South Side            21,390     9,509       11,881       124.9%\"  \n[36] \" 34   Armour Square              13,391     12,032      1,359         11.3%\"  \n[37] \" 35   Douglas                    18,238     26,470      -8,232       -31.1%\"  \n[38] \" 36   Oakland                     5,918     6,110        -192         -3.1%\"  \n[39] \" 37   Fuller Park                 2,876     3,420        -544        -15.9%\"  \n[40] \" 38   Grand Boulevard            21,929     28,006      -6,077       -21.7%\"  \n[41] \" 39   Kenwood                    17,841     18,363       -522         -2.8%\"  \n[42] \" 40   Washington Park            11,717     14,146      -2,429       -17.2%\"  \n\n\nWe now repeat this operation for pop.dat[[2]]. More efficiently, we implement it as a loop, replacing i in turn by 1 and 2. This yields:\n\nnnlist &lt;- \"\"\nfor (i in 1:2) {\n  ppage &lt;- strsplit(pop.dat[[i]],split=\"\\n\")\n  nni &lt;- ppage[[1]]\n  nni &lt;- nni[-(1:4)]\n  nnu &lt;- unlist(nni)\n  nnlist &lt;- c(nnlist,nnu)\n}\n\nAt the end of the loop, we check the contents of the vector nnlist.\n\nnnlist\n\n [1] \"\"                                                                                 \n [2] \"Num        Community Area       2010        2,000     Difference   Percentage\"    \n [3] \" 1    Rogers Park                54,991     63,484      -8,493       -13.4%\"      \n [4] \" 2    West Ridge                 71,942     73,199      -1,257        -1.7%\"      \n [5] \" 3    Uptown                     56,362     63,551      -7,189       -11.3%\"      \n [6] \" 4    Lincoln Square             39,493     44,574      -5,081       -11.4%\"      \n [7] \" 5    North Center               31,867     31,895        -28         -0.1%\"      \n [8] \" 6    Lake View                  94,368     94,817       -449         -0.5%\"      \n [9] \" 7    Lincoln Park               64,116     64,320       -204         -0.3%\"      \n[10] \" 8    Near North Side            80,484     72,811      7,673         10.5%\"      \n[11] \" 9    Edison Park                11,187     11,259        -72         -0.6%\"      \n[12] \" 10   Norwood Park               37,023     37,669       -646         -1.7%\"      \n[13] \" 11   Jefferson Park             25,448     25,859       -411         -1.6%\"      \n[14] \" 12   Forest Glen                18,508     18,165        343         1.9%\"       \n[15] \" 13   North Park                 17,931     18,514       -583         -3.1%\"      \n[16] \" 14   Albany Park                51,542     57,655      -6,113       -10.6%\"      \n[17] \" 15   Portage Park               64,124     65,340      -1,216        -1.9%\"      \n[18] \" 16   Irving Park                53,359     58,643      -5,284        -9.0%\"      \n[19] \" 17   Dunning                    41,932     42,164       -232         -0.6%\"      \n[20] \" 18   Montclare                  13,426     12,646        780         6.2%\"       \n[21] \" 19   Belmont Cragin             78,743     78,144        599         0.8%\"       \n[22] \" 20   Hermosa                    25,010     26,908      -1,898        -7.1%\"      \n[23] \" 21   Avondale                   39,262     43,083      -3,821        -8.9%\"      \n[24] \" 22   Logan Square               73,595     82,715      -9,120       -11.0%\"      \n[25] \" 23   Humboldt Park              56,323     65,836      -9,513       -14.4%\"      \n[26] \" 24   West Town                  81,432     87,435      -6,003        -6.9%\"      \n[27] \" 25   Austin                     98,514    117,527     -19,013       -16.2%\"      \n[28] \" 26   West Garfield Park         18,001     23,019      -5,018       -21.8%\"      \n[29] \" 27   East Garfield Park         20,567     20,881       -314         -1.5%\"      \n[30] \" 28   Near West Side             54,881     46,419      8,462         18.2%\"      \n[31] \" 29   North Lawndale             35,912     41,768      -5,856       -14.0%\"      \n[32] \" 30   South Lawndale             79,288     91,071     -11,783       -12.9%\"      \n[33] \" 31   Lower West Side            35,769     44,031      -8,262       -18.8%\"      \n[34] \" 32   Loop                       29,283     16,388      12,895        78.7%\"      \n[35] \" 33   Near South Side            21,390     9,509       11,881       124.9%\"      \n[36] \" 34   Armour Square              13,391     12,032      1,359         11.3%\"      \n[37] \" 35   Douglas                    18,238     26,470      -8,232       -31.1%\"      \n[38] \" 36   Oakland                     5,918     6,110        -192         -3.1%\"      \n[39] \" 37   Fuller Park                 2,876     3,420        -544        -15.9%\"      \n[40] \" 38   Grand Boulevard            21,929     28,006      -6,077       -21.7%\"      \n[41] \" 39   Kenwood                    17,841     18,363       -522         -2.8%\"      \n[42] \" 40   Washington Park            11,717     14,146      -2,429       -17.2%\"      \n[43] \"Num       Community Area           2010         2,000     Difference   Percentage\"\n[44] \" 41   Hyde Park                      25,681     29,920       -4,239      -14.2%\"  \n[45] \" 42   Woodlawn                       25,983     27,086       -1,103       -4.1%\"  \n[46] \" 43   South Shore                    49,767     61,556      -11,789      -19.2%\"  \n[47] \" 44   Chatham                        31,028     37,275       -6,247      -16.8%\"  \n[48] \" 45   Avalon Park                    10,185     11,147        -962        -8.6%\"  \n[49] \" 46   South Chicago                  31,198     38,596       -7,398      -19.2%\"  \n[50] \" 47   Burnside                        2,916     3,294         -378       -11.5%\"  \n[51] \" 48   Calumet Heights                13,812     15,974       -2,162      -13.5%\"  \n[52] \" 49   Roseland                       44,619     52,723       -8,104      -15.4%\"  \n[53] \" 50   Pullman                         7,325     8,921        -1,596      -17.9%\"  \n[54] \" 51   South Deering                  15,109     16,990       -1,881      -11.1%\"  \n[55] \" 52   East Side                      23,042     23,653        -611        -2.6%\"  \n[56] \" 53   West Pullman                   29,651     36,649       -6,998      -19.1%\"  \n[57] \" 54   Riverdale                       6,482     9,809        -3,327      -33.9%\"  \n[58] \" 55   Hegewisch                       9,426     9,781         -355        -3.6%\"  \n[59] \" 56   Garfield Ridge                 34,513     36,101       -1,588       -4.4%\"  \n[60] \" 57   Archer Heights                 13,393     12,644        749         5.9%\"   \n[61] \" 58   Brighton Park                  45,368     44,912        456         1.0%\"   \n[62] \" 59   McKinley Park                  15,612     15,962        -350        -2.2%\"  \n[63] \" 60   Bridgeport                     31,977     33,694       -1,717       -5.1%\"  \n[64] \" 61   New City                       44,377     51,721       -7,344      -14.2%\"  \n[65] \" 62   West Elsdon                    18,109     15,921       2,188       13.7%\"   \n[66] \" 63   Gage Park                      39,894     39,193        701         1.8%\"   \n[67] \" 64   Clearing                       23,139     22,331        808         3.6%\"   \n[68] \" 65   West Lawn                      33,355     29,235       4,120       14.1%\"   \n[69] \" 66   Chicago Lawn                   55,628     61,412       -5,784       -9.4%\"  \n[70] \" 67   West Englewood                 35,505     45,282       -9,777      -21.6%\"  \n[71] \" 68   Englewood                      30,654     40,222       -9,568      -23.8%\"  \n[72] \" 69   Greater Grand Crossing         32,602     38,619       -6,017      -15.6%\"  \n[73] \" 70   Ashburn                        41,081     39,584       1,497        3.8%\"   \n[74] \" 71   Auburn Gresham                 48,743     55,928       -7,185      -12.8%\"  \n[75] \" 72   Beverly                        20,034     21,992       -1,958       -8.9%\"  \n[76] \" 73   Washington Heights             26,493     29,843       -3,350      -11.2%\"  \n[77] \" 74   Mount Greenwood                19,093     18,820        273         1.5%\"   \n[78] \" 75   Morgan Park                    22,544     25,226       -2,682      -10.6%\"  \n[79] \" 76   O'Hare                         12,756     11,956        800         6.7%\"   \n[80] \" 77   Edgewater                      56,521     62,198       -5,677       -9.1%\"  \n[81] \"      Total                       2,695,598   2,896,016    -200,418       -6.9%\"  \n\n\nThis is now a vector of 79 elements, each of which is a string. To clean things up, strip the first (empty) element, and the last element, which is nothing but the totals. We thus extract the elements from 2 to length - 1.\n\nnnlist &lt;- nnlist[2:(length(nnlist)-1)]\n\n\n\n8.2 Extracting the population values\nWe first initialize a vector of zeros to hold the population values. It is the preferred approach to initialize a vector first if one knows its size, rather than having it grow by appending rows or columns. We use the vector command and specify the mode=\"numeric\" and give the length as the length of the list.\n\nnnpop &lt;- vector(mode=\"numeric\",length=length(nnlist))\n\nWe again will use a loop to process each element of the list (each line of the table) one by one. We use the substr command to extract the characters between position 27 and 39 (these values were determined after taking a careful look at the structure of the table). However, there is still a problem, since the population values contain commas. We now do two things in one line of code. First, we use gsub to substitute the comma character by an empty ““. We turn the result into a numeric value by means of as.numeric. We then assign this number to position i of the vector. The resulting vector nnpop contains the population for each of the community areas.\n\nfor (i in (1:length(nnlist))) {\n     popchar &lt;- substr(nnlist[i],start=27,stop=39)\n     popval &lt;- as.numeric(gsub(\",\",\"\",popchar))\n     nnpop[i] &lt;- popval\n}\nnnpop\n\n [1]  2010 54991 71942 56362 39493 31867 94368 64116 80484 11187 37023 25448\n[13] 18508 17931 51542 64124 53359 41932 13426 78743 25010 39262 73595 56323\n[25] 81432 98514 18001 20567 54881 35912 79288 35769 29283 21390 13391 18238\n[37]  5918  2876 21929 17841 11717  2010    25    25    49    31    10    31\n[49]     2    13    44     7    15    23    29     6     9    34    13    45\n[61]    15    31    44    18    39    23    33    55    35    30    NA    41\n[73]    48    20    26    19    22    12    56\n\n\n\n\nCreating a data frame with population values\nAs a final step in the process of collecting the community area population information, we combine the vector with the population counts and a vector with community ID information into a data frame.\nSince the community area indicators are simple sequence numbers, we create such a vector to serve as the ID, again using the length of the vector to determine the extent.\n\nnnid &lt;- (1:length(nnlist))\nnnid\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50\n[51] 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75\n[76] 76 77 78 79\n\n\nWe turn the vectors nnid and nnpop into a data frame using the data.frame command. Since the variable names assigned automatically are not that informative, we next force them to NID and POP2010 using the names command. Also, as we did before, we make sure the ID variable is an integer (for merging in GeoDa) by means of as.integer.\n\nneighpop &lt;- data.frame(as.integer(nnid),nnpop)\nnames(neighpop) &lt;- c(\"NID\",\"POP2010\")\nhead(neighpop)\n\n  NID POP2010\n1   1    2010\n2   2   54991\n3   3   71942\n4   4   56362\n5   5   39493\n6   6   31867"
  },
  {
    "objectID": "Exploration/CSDS/01-spatial-data-handling/index.html#mapping-community-area-abandoned-vehicles-per-capita",
    "href": "Exploration/CSDS/01-spatial-data-handling/index.html#mapping-community-area-abandoned-vehicles-per-capita",
    "title": "01 Spatial Data Handline",
    "section": "Mapping Community Area Abandoned Vehicles Per Capita",
    "text": "Mapping Community Area Abandoned Vehicles Per Capita\n\nComputing abandoned vehicles per capita\nBefore proceeding further, we left_join the community population data to the community area layer, in the same way as we did for the vehicle counts.\n\nchicago.comm &lt;- left_join(chicago.comm,neighpop, by = c(\"area_num_1\" = \"NID\"))\nhead(chicago.comm)\n\nSimple feature collection with 6 features and 11 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 441440.4 ymin: 4627153 xmax: 451817.1 ymax: 4648971\nProjected CRS: WGS 84 / UTM zone 16N\n# A tibble: 6 × 12\n  community  area  shape_area perimeter area_num_1 area_numbe comarea_id comarea\n  &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;          &lt;int&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;  \n1 DOUGLAS    0     46004621.… 0                 35 35         0          0      \n2 OAKLAND    0     16913961.… 0                 36 36         0          0      \n3 FULLER PA… 0     19916704.… 0                 37 37         0          0      \n4 GRAND BOU… 0     48492503.… 0                 38 38         0          0      \n5 KENWOOD    0     29071741.… 0                 39 39         0          0      \n6 LINCOLN S… 0     71352328.… 0                  4 4          0          0      \n# ℹ 4 more variables: shape_len &lt;chr&gt;, geometry &lt;MULTIPOLYGON [m]&gt;,\n#   AGG.COUNT &lt;int&gt;, POP2010 &lt;dbl&gt;\n\n\nWe will now create a new variable using the tidyverse mutate command as the ratio of vehicle counts per 1000 population.\n\nchicago.comm &lt;- chicago.comm %&gt;% mutate(vehpcap = (AGG.COUNT / POP2010) * 1000) \nhead(chicago.comm)\n\nSimple feature collection with 6 features and 12 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 441440.4 ymin: 4627153 xmax: 451817.1 ymax: 4648971\nProjected CRS: WGS 84 / UTM zone 16N\n# A tibble: 6 × 13\n  community  area  shape_area perimeter area_num_1 area_numbe comarea_id comarea\n  &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;          &lt;int&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;  \n1 DOUGLAS    0     46004621.… 0                 35 35         0          0      \n2 OAKLAND    0     16913961.… 0                 36 36         0          0      \n3 FULLER PA… 0     19916704.… 0                 37 37         0          0      \n4 GRAND BOU… 0     48492503.… 0                 38 38         0          0      \n5 KENWOOD    0     29071741.… 0                 39 39         0          0      \n6 LINCOLN S… 0     71352328.… 0                  4 4          0          0      \n# ℹ 5 more variables: shape_len &lt;chr&gt;, geometry &lt;MULTIPOLYGON [m]&gt;,\n#   AGG.COUNT &lt;int&gt;, POP2010 &lt;dbl&gt;, vehpcap &lt;dbl&gt;\n\n\n\n\nFinal choropleth map\nFor our final choropleth, we use the same procedure as for the vehicle counts, but take vehpcap as the variable instead.\n\ntm_shape(chicago.comm) +\n  tm_polygons(\"vehpcap\")\n\n\n\n\n\n\n\n\nWhen compared to the total counts, we see quite a different spatial distribution. In particular, the locations of the highest ratios are quite different from those of the highest counts. As a rule, one should never create a choropleth map of a spatially extensive variable, unless the size of the areal units is somehow controlled for (e.g., equal area grid cells, or equal population zones).\n\n8.2.1 Optional - save the community area file as a shape file\nFinally, we can write the community area layer to the working directory. Note that, so far, all operations have been carried out in memory, and when you close the program, everything will be lost (unless you save your workspace).\nWe can write the community area to a shape file (actually, four files contained in a directory) by means of the sf command st_write. This command has many options, but we just use the minimal ones. The chicago.comm object will be written to a set of files in the directory chicago_vehicles using the ESRI Shapefile format. Note that if the directory already exists, it should be deleted or renamed first, since st_write only creates a new directory. Otherwise, there will be an error message.\n\nst_write(chicago.comm,\"chicago_vehicles\",driver=\"ESRI Shapefile\")\n\nWriting layer `chicago_vehicles' to data source \n  `chicago_vehicles' using driver `ESRI Shapefile'\nWriting 77 features with 12 fields and geometry type Multi Polygon.\n\n\nHowever, this map can be highly misleading since it pertains to a so-called spatially extensive variable, such as a count. Even if every area had the same risk of having abandoned vehicles, larger community areas would have higher counts. In other words, since the count is directly related to the size of the area, it does not provide a proper indication of the risk.\nInstead, we should map a spatially intensive variable, which is corrected for the size of the unit. For example, this can be achieved by expressing the variable as a density (counts per area), or as some other ratio, such as the counts per capita. In order to calculate this ratio, we first need to obtain the population for each community area."
  },
  {
    "objectID": "Exploration/CSDS/01-spatial-data-handling/index.html#footnotes",
    "href": "Exploration/CSDS/01-spatial-data-handling/index.html#footnotes",
    "title": "01 Spatial Data Handline",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA good resource on coordinate reference systems is the spatialreference.org site, which contains thousands of references in a variety of commonly used formats.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "️ Welcome to the ISSS626 Geospatial Analytics and Applications",
    "section": "",
    "text": "This website contains the coursework by Teng Kok Wai (Walter). It features a collection of projects, assignments, and notes from the class, complete with geospatial visualizations and analytical insights."
  },
  {
    "objectID": "index.html#hands-on-exercise",
    "href": "index.html#hands-on-exercise",
    "title": "️ Welcome to the ISSS626 Geospatial Analytics and Applications",
    "section": "✍️ Hands-on Exercise",
    "text": "✍️ Hands-on Exercise\n\n\n\n\n\n\n\n\n6A: Geographical Segmentation with Spatially Constrained Clustering Techniques\n\n\n38 min\n\n\n\nSep 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n5B: Local Measures of Spatial Autocorrelation\n\n\n26 min\n\n\n\nSep 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n5A: Global Measures of Spatial Autocorrelation\n\n\n24 min\n\n\n\nSep 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n4A: Spatial Weights and Applications\n\n\n22 min\n\n\n\nSep 8, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n3A: Network Constrained Spatial Point Patterns Analysis\n\n\n14 min\n\n\n\nSep 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n2B: 2nd Order Spatial Point Patterns Analysis\n\n\n28 min\n\n\n\nAug 29, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n2A: 1st Order Spatial Point Patterns Analysis\n\n\n27 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n1B: Thematic Mapping and GeoVisualisation with R\n\n\n24 min\n\n\n\nAug 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n1A: Geospatial Data Wrangling with R\n\n\n15 min\n\n\n\nAug 24, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#in-class-exercise",
    "href": "index.html#in-class-exercise",
    "title": "️ Welcome to the ISSS626 Geospatial Analytics and Applications",
    "section": "🏫 In-class Exercise",
    "text": "🏫 In-class Exercise\n\n\n\n\n\n\n\n\nIn-Class Exercise 4\n\n\n17 min\n\n\n\nSep 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nIn-Class Exercise 3\n\n\n5 min\n\n\n\nSep 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nIn-Class Exercise 2\n\n\n11 min\n\n\n\nSep 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nIn-Class Exercise 1\n\n\n23 min\n\n\n\nAug 26, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#take-home-exercise",
    "href": "index.html#take-home-exercise",
    "title": "️ Welcome to the ISSS626 Geospatial Analytics and Applications",
    "section": "🏠 Take-home Exercise",
    "text": "🏠 Take-home Exercise\n\n\n\n\n\n\n\n\nTake Home Exercise 1\n\n\n62 min\n\n\n\nSep 5, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01b.html",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01b.html",
    "title": "1B: Thematic Mapping and GeoVisualisation with R",
    "section": "",
    "text": "R for Geospatial Data Science and Analytics - 2  Thematic Mapping and GeoVisualisation with R"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01b.html#exercise-1b-reference",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01b.html#exercise-1b-reference",
    "title": "1B: Thematic Mapping and GeoVisualisation with R",
    "section": "",
    "text": "R for Geospatial Data Science and Analytics - 2  Thematic Mapping and GeoVisualisation with R"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01b.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01b.html#overview",
    "title": "1B: Thematic Mapping and GeoVisualisation with R",
    "section": "2 Overview",
    "text": "2 Overview\nIn general, thematic mapping involves the use of map symbols to visualize selected properties of geographic features that are not naturally visible, such as population, temperature, crime rate, and property prices.\nMeanwhile, geovisualization leverages graphical representation to make places, phenomena, or processes visible, tapping into our spatial cognition and visual processing abilities."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01b.html#learning-outcome",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01b.html#learning-outcome",
    "title": "1B: Thematic Mapping and GeoVisualisation with R",
    "section": "3 Learning Outcome",
    "text": "3 Learning Outcome\n\nUnderstanding thematic mapping and geovisualization concepts.\nInstalling and using the tmap package for creating choropleth maps.\nImporting and preparing geospatial and attribute data using sf and readr packages.\nWrangling data with dplyr and tidyr to prepare for mapping.\nPerforming georelational joins between geospatial and attribute data.\nCreating quick and advanced thematic maps using tmap elements and functions.\nCustomizing map layouts, color schemes, and data classification methods.\nPlotting small multiple maps and mapping spatial objects meeting specific criteria."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01b.html#the-data",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01b.html#the-data",
    "title": "1B: Thematic Mapping and GeoVisualisation with R",
    "section": "4 The Data",
    "text": "4 The Data\nThe following data sources will be used in this exercise:\n\n\n\nDataset\nSource\nDescription\nFormat\n\n\n\n\nMaster Plan 2014 Subzone Boundary (Web) (MP14_SUBZONE_WEB_PL)\ndata.gov.sg\nGeospatial data representing the geographical boundaries of Singapore’s planning subzones based on URA Master Plan 2014.\nESRI Shapefile\n\n\nSingapore Residents by Planning Area / Subzone, Age Group, Sex, and Type of Dwelling (2011-2020) (respopagesextod2011to2020.csv)\nDepartment of Statistics, Singapore\nAspatial data containing demographic information; fields PA and SZ can be used to link to the MP14_SUBZONE_WEB_PL shapefile.\nCSV\n\n\n\nThis table summarizes the datasets required, including their descriptions, sources, and formats."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01b.html#installing-and-loading-the-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01b.html#installing-and-loading-the-r-packages",
    "title": "1B: Thematic Mapping and GeoVisualisation with R",
    "section": "5 Installing and Loading the R Packages",
    "text": "5 Installing and Loading the R Packages\nThe following R packages will be used in this exercise:\n\n\n\n\n\n\n\n\nPackage\nPurpose\nUse Case in Exercise\n\n\n\n\ntmap\nThematic mapping for visualizing geospatial data.\nCreating and customizing thematic maps.\n\n\nsf\nHandling geospatial data in simple features format.\nImporting, managing, and processing geospatial data.\n\n\nreadr\nImporting delimited text files (part of tidyverse).\nLoading CSV data files.\n\n\ntidyr\nTidying data into a usable format (part of tidyverse).\nReshaping and organizing data.\n\n\ndplyr\nData wrangling and transformation tasks (part of tidyverse).\nManipulating and transforming datasets.\n\n\n\nTo install and load these packages in R, use the following code:\n\npacman::p_load(sf, tmap, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01b.html#import-geospatial-data-into-r",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01b.html#import-geospatial-data-into-r",
    "title": "1B: Thematic Mapping and GeoVisualisation with R",
    "section": "6 Import Geospatial Data into R",
    "text": "6 Import Geospatial Data into R\nThe code block below uses st_read() function of sf package to import the MP14_SUBZONE_WEB_PL shapefile into R as a polygon feature data frame.\n\nmpsz = st_read(dsn = \"data/geospatial\",\n                  layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/walter/code/isss626/isss626-gaa/Hands-on_Ex/Hands-on_Ex01/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nWhen you display mpsz, only the first ten records are shown because R, by default, limits the number of printed rows to make output more manageable and readable.\n\nmpsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\n\n6.1 Importing Attribute Data into R\nNext, respopagsex2011to2020.csv file is imported into RStudio and save the file into an R dataframe called popdata.\nThe task will be performed by using read_csv() function of readr package as shown in the code block below.\n\npopdata &lt;- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\n\n\n6.2 Data Preparation\nBefore a thematic map can be prepared, it is required to prepare a data table with year 2020 values. The data table should include the variables PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY.\n\nYOUNG: age group 0 to 4 until age groyup 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group\n\n\n6.2.1 Data wrangling\nThe following data wrangling and transformation functions will be used:\n\npivot_wider() of tidyr package, and\nmutate(), filter(), group_by() and select() of dplyr package\n\n\n# Step 1: Filter the data for the year 2020\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020)\n\n# Step 2: Group the data by PA (Planning Area), SZ (Subzone), and AG (Age Group)\npopdata2020 &lt;- popdata2020 %&gt;%\n  group_by(PA, SZ, AG)\n\n# Step 3: Summarize the data by calculating the total population (POP) for each group\npopdata2020 &lt;- popdata2020 %&gt;%\n  summarise(POP = sum(Pop)) %&gt;%\n  ungroup()\n\n# Step 4: Reshape the data from long to wide format, with age groups as column names\npopdata2020 &lt;- popdata2020 %&gt;%\n  pivot_wider(names_from = AG, values_from = POP)\n\n# Step 5: Calculate the YOUNG population (sum of selected age groups)\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate(YOUNG = rowSums(.[3:6]) + rowSums(.[14]))\n\n# Step 6: Calculate the ECONOMICALLY ACTIVE population (sum of selected age groups)\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate(`ECONOMY ACTIVE` = rowSums(.[7:11]) + rowSums(.[13:15]))\n\n# Step 7: Calculate the AGED population (sum of selected age groups)\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate(AGED = rowSums(.[16:21]))\n\n# Step 8: Calculate the TOTAL population (sum of all age groups)\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate(TOTAL = rowSums(.[3:21]))\n\n# Step 9: Calculate the DEPENDENCY ratio (ratio of YOUNG and AGED to ECONOMY ACTIVE)\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate(DEPENDENCY = (YOUNG + AGED) / `ECONOMY ACTIVE`)\n\n# Step 10: Select the relevant columns to be retained in the final dataset\npopdata2020 &lt;- popdata2020 %&gt;%\n  select(PA, SZ, YOUNG, `ECONOMY ACTIVE`, AGED, TOTAL, DEPENDENCY)\n\n\n\n6.2.2 Joining the attribute data and geospatial data\nBefore performing a georelational join, one additional step is necessary: converting the values in the PA and SZ fields to uppercase. This is because the PA and SZ fields contain both upper- and lowercase letters, whereas the SUBZONE_N and PLN_AREA_N fields are in uppercase.\n\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ),\n          .funs = list(toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\nNext, the left_join() function from the dplyr package is used to merge the geographic data and attribute table, using the planning subzone names (e.g. SUBZONE_N and SZ) as the common identifier.\n\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\n\n\n\n\n\n\nNote\n\n\n\nKey Takeaway:\nThe left_join() function from the dplyr package is applied to ensure that the resulting output remains a simple features data frame, with mpsz as the left table.\n\n\n\ndir.create(\"data/rds/\", recursive = TRUE)\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01b.html#choropleth-mapping-geospatial-data-using-tmap",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01b.html#choropleth-mapping-geospatial-data-using-tmap",
    "title": "1B: Thematic Mapping and GeoVisualisation with R",
    "section": "7 Choropleth Mapping Geospatial Data Using tmap",
    "text": "7 Choropleth Mapping Geospatial Data Using tmap\nChoropleth mapping involves the symbolisation of enumeration units, such as countries, provinces, states, counties or census units, using area patterns or graduated colors. For example, a social scientist may need to use a choropleth map to portray the spatial distribution of aged population of Singapore by Master Plan 2014 Subzone Boundary.\nThere are two methods for creating thematic maps using tmap:\n\nUsing qtm() for quick thematic map plotting.\nUsing individual tmap elements for creating highly customizable maps.\n\n\n7.1 Plotting a choropleth map quickly by using qtm()\nThe easiest and quickest to draw a choropleth map using tmap is using qtm(). It is concise and provides a good default visualisation in many cases.\nThe following code draws a standard choropleth map:\n\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020,\n    fill = \"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nKey Takeaway:\n\ntmap_mode(“plot”) is used to generate a static map. To enable interactive mode, use the “view” option.\nThe fill argument is used to map the attribute to be visualized (in this case, DEPENDENCY).\n\n\n\n\n\n7.2 Creating a choropleth map by using tmap’s elements\nWhile qtm() is useful for quickly and easily drawing choropleth maps, its drawback is the limited control over the aesthetics of individual layers. To create a high-quality, cartographic choropleth map like the one shown below, it’s recommended to use the individual elements provided by tmap.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45,\n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\",\n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\nIn the following sub-section, more tmap functions will be used to plot these elements.\n\n7.2.1 Drawing a base map\nThe basic building block of tmap is tm_shape() followed by one or more layer elemments such as tm_fill() and tm_polygons().\nIn the code block below, tm_shape() is used to define the input data (i.e mpsz_pop2020) and tm_polygons() is used to draw the planning subzone polygons\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\n\n\n\n\n\n7.2.2 Drawing a choropleth map using tm_polygons()\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we just need to assign the target variable such as Dependency to tm_polygons().\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nKey takeaways from tm_polygons():\n\nThe default method for interval binning when creating a choropleth map is “pretty.” A detailed explanation of the data classification methods available in tmap will be covered in sub-section 4.3.\nThe default color palette used is YlOrRd from ColorBrewer. More information on color schemes will be explored in sub-section 4.4.\nMissing values are automatically represented with a grey shade by default.\n\n\n\n\n\n7.2.3 Drawing a choropleth map using tm_fill() and *tm_border()**\nActually, tm_polygons() is a wraper of tm_fill() and tm_border(). tm_fill() shades the polygons by using the default colour scheme and tm_borders() adds the borders of the shapefile onto the choropleth map.\nThe code block below draws a choropleth map by using tm_fill() alone.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\nNotice that the planning subzones are shared according to the respective dependecy values\nTo add the boundary of the planning subzones, tm_borders will be used as shown in the code block below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\n\n\n\n\nNote that light-gray border lines have been added on the choropleth map.\nThe alpha option specifies a transparency value ranging from 0 (completely transparent) to 1 (not transparent). By default, the col’s alpha value is used (usually 1).\nBeside alpha argument, there are three other arguments for tm_borders(), they are:\n\ncol = border colour,\nlwd = border line width. The default is 1, and\nlty = border line type. The default is “solid”.\n\n\n\n\n7.3 Data classification methods of tmap\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\ntmap provides a total ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks.\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be used.\n\n7.3.1 Plotting choropleth maps with built-in classification methods\nThe code block below shows a quantile data classification that used 5 classes.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nIn the code block below, equal data classification method is used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nObservation: The distribution of quantile data classification method are more evenly distributed then equal data classification method.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWarning: Maps Lie!\n\n\nExercise: Using what you had learned, prepare choropleth maps by using different classification methods supported by tmap and compare their differences.\nThe tabs below shows the choropleth maps drawn using different classifications methods with the same number of classes (n=5).\n\nquantileequalprettyjenkssdhclustfisherbclustkmeans\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"pretty\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"sd\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"hclust\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"fisher\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"bclust\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nCommittee Member: 1(1) 2(1) 3(1) 4(1) 5(1) 6(1) 7(1) 8(1) 9(1) 10(1)\nComputing Hierarchical Clustering\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"kmeans\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nObservation: Different data classification methods produce varying color patterns. Methods like pretty, equal, and sd are more sensitive to outliers, often resulting in fewer distinct areas on the map. In contrast, methods like quantile, jenks, and kmeans create more evenly distributed colors. Therefore, choosing the right classification method based on data distribution is crucial to avoid misleading representations in choropleth maps.\n\n\nExercise: Preparing choropleth maps by using similar classification method but with different numbers of classes (i.e. 2, 6, 10, 20). Compare the output maps, what observation can you draw?\nThe tabs below show choropleth maps with kmeans classification method. The numbers of classes used are (2, 6, 10, 20).\n\nn=2n=6n=10n=20\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 2,\n          style = \"kmeans\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"kmeans\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 10,\n          style = \"kmeans\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 20,\n          style = \"kmeans\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nObservation: Using different class numbers within the same classification method affects the map’s coloring. With n=2, outliers dominate, resulting in only one distinct area. Increasing the class count distributes the data more evenly across the map.\n\n\n\n\n7.3.2 Plotting choropleth map with custom break\nFor all the built-in styles, the category breaks are computed internally. In order to override these defaults, the breakpoints can be set explicitly by means of the breaks argument to the tm_fill(). It is important to note that, in tmap the breaks include a minimum and maximum. As a result, in order to end up with n categories, n+1 elements must be specified in the breaks option (the values must be in increasing order).\nBefore we get started, it is always a good practice to get some descriptive statistics on the variable before setting the break points. Code block below will be used to compute and display the descriptive statistics of DEPENDENCY field.\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.0000  0.6751  0.7288  0.8084  0.8092 19.0000      92 \n\n\nBased on the results, we choose breakpoints at 0.60, 0.70, 0.80, and 0.90, with the minimum at 0 and maximum at 1. Our breaks vector is c(0, 0.60, 0.70, 0.80, 0.90, 1.00).\nNow, we will plot the choropleth map with the following code:\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n7.4 Colour Scheme\ntmap supports colour ramps either user-defined or a set of predefined colour ramps from the RColorBrewer package.\n\n7.4.1 Using ColourBrewer palette\nTo apply a specific color palette, assign it to the palette argument in tm_fill() as shown below:\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nNotice that the choropleth map is shaded in green.\nTo reverse the colour shading, add a “-” prefix.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nNotice that the colour scheme has been reversed.\n\n\n\n7.5 Map Layouts\nMap layout refers to the combination of all map elements into a cohensive map. Map elements include among others the objects to be mapped, the title, the scale bar, the compass, margins and aspects ratios. Colour settings and data classification methods covered in the previous section relate to the palette and break-points are used to affect how the map looks.\n\n7.5.1 Map Legend\nIn tmap, there are several legend options for adjusting the placement, format, and style of the legend.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"jenks\",\n          palette = \"Blues\",\n          legend.hist = TRUE,\n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45,\n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n7.5.2 Map style\ntmap allows extensive customization of layout settings, accessible through the tmap_style() function. The code below demonstrates the use of the classic style.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\n\n\n\n\n\n\n7.5.3 Cartographic Furniture\nBeside map style, tmap also also provides arguments to draw other map furniture such as compass, scale bar and grid lines. In the following example, tm_compass(), tm_scale_bar() and tm_grid() are used to add compass, scale bar and grid lines onto the choropleth map.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45,\n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\",\n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\nTo reset the default style, use the code below:\n\ntmap_style(\"white\")\n\n\n\n\n7.6 Drawing Small Multiple Choropleth Maps\nSmall multiple maps, also referred to as facet maps, are composed of many maps arrange side-by-side, and sometimes stacked vertically. Small multiple maps enable the visualisation of how spatial relationships change with respect to another variable, such as time.\nIn tmap, small multiple maps can be created through three methods:\n\nAssigning multiple values to an aesthetic argument.\nDefining a grouping variable with tm_facets().\nCreating multiple stand-alone maps with tmap_arrange().\n\n\n7.6.1 Assigning Multiple Values to an Aesthetic Argument\nIn this example, small multiple choropleth maps are created by defining ncols in tm_fill()\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\n\n\n\n\n\nThis example creates small multiple choropleth maps by assigning multiple values to at least one of the aesthetic arguments:\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"),\n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\n\n\n\n\n\n7.6.2 Grouping by a Variable with tm_facets()\nThis example generates multiple small maps by using tm_facets().\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) +\n  tm_facets(by=\"REGION_N\",\n            free.coords=TRUE,\n            drop.shapes=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"),\n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n7.6.3 Creating Stand-alone Maps with tmap_arrange()\nIn this example, multiple small choropleth maps are created by creating multiple stand-alone maps with tmap_arrange().\n\nyoungmap &lt;- tm_shape(mpsz_pop2020)+\n  tm_polygons(\"YOUNG\",\n              style = \"quantile\",\n              palette = \"Blues\")\n\nagedmap &lt;- tm_shape(mpsz_pop2020)+\n  tm_polygons(\"AGED\",\n              style = \"quantile\",\n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n7.7 Mapping Spatial Object by Meeting a Selection Criterion\nInstead of creating small multiple choropleth map, you can also use selection function to map spatial objects meeting the selection criterion.\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          legend.hist = TRUE,\n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45,\n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01b.html#references",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01b.html#references",
    "title": "1B: Thematic Mapping and GeoVisualisation with R",
    "section": "8 References",
    "text": "8 References\n\n8.1 All about tmap package\n\ntmap: Thematic Maps in R\ntmap\ntmap: get started!\ntmap: changes in version 2.0\ntmap: creating thematic maps in a flexible way (useR!2015)\nExploring and presenting maps with tmap (useR!2017)\n\n\n\n8.2 Geospatial Data Wrangling\n\nsf: Simple Features for R\nSimple Features for R: StandardizedSupport for Spatial Vector Data\nReading, Writing and Converting Simple Features\n\n\n\n8.3 Data Wrangling\n\ndplyr\nTidy data\ntidyr: Easily Tidy Data with ‘spread()’ and ‘gather()’ Functions"
  },
  {
    "objectID": "Hands-on_Ex/index.html",
    "href": "Hands-on_Ex/index.html",
    "title": "Hands-on Exercise",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n6A: Geographical Segmentation with Spatially Constrained Clustering Techniques\n\n\nIn this exercise, we will learn to delineate homogeneous regions using hierarchical and spatially constrained clustering techniques on geographically referenced multivariate data.\n\n\n38 min\n\n\n\nSep 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n5B: Local Measures of Spatial Autocorrelation\n\n\nIn this exercise, we will learn to compute Local Measures of Spatial Autocorrelation (LMSA) using the spdep package, including Local Moran’s I, Getis-Ord’s Gi-statistics, and their visualizations.\n\n\n26 min\n\n\n\nSep 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n5A: Global Measures of Spatial Autocorrelation\n\n\nIn this exercise, we will learn to compute Global Measures of Spatial Autocorrelation using the spdep package, including Moran’s I and Geary’s C tests, spatial correlograms, and their statistical interpretation.\n\n\n24 min\n\n\n\nSep 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n4A: Spatial Weights and Applications\n\n\nIn this exercise, we will learn to compute spatial weights, visualize spatial distributions, and create spatially lagged variables using various functions from R packages such as sf,spdep, and tmap.\n\n\n22 min\n\n\n\nSep 8, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n3A: Network Constrained Spatial Point Patterns Analysis\n\n\nIn this exercise, we will learn to use R and the spNetwork package for analyzing network-constrained spatial point patterns, focusing on kernel density estimation and G- and K-function analysis.\n\n\n14 min\n\n\n\nSep 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n2B: 2nd Order Spatial Point Patterns Analysis\n\n\nIn this exercise, we will learn to apply 2nd-order spatial point pattern analysis methods in R, including G, F, K, and L functions, to evaluate spatial point distributions and perform hypothesis testing using the spatstat package.\n\n\n28 min\n\n\n\nAug 29, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n2A: 1st Order Spatial Point Patterns Analysis\n\n\nIn this exercise, we will learn to analyze spatial point patterns in R, including importing geospatial data, performing kernel density estimation and nearest neighbor analysis, and visualizing results using spatstat, sf, and tmap packages.\n\n\n27 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n1B: Thematic Mapping and GeoVisualisation with R\n\n\nIn this exercise, we will learn to create thematic maps and perform geovisualization in R using the tmap package, including data preparation, classification, color schemes, and advanced mapping techniques.\n\n\n24 min\n\n\n\nAug 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n1A: Geospatial Data Wrangling with R\n\n\nIn this exercise, we will learn to use R for geospatial data handling, including importing, transforming, wrangling, and visualizing data with sf, tidyverse, and ggplot2.\n\n\n15 min\n\n\n\nAug 24, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html",
    "title": "5A: Global Measures of Spatial Autocorrelation",
    "section": "",
    "text": "R for Geospatial Data Science and Analytics - 9  Global Measures of Spatial Autocorrelation"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#exercise-5a-reference",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#exercise-5a-reference",
    "title": "5A: Global Measures of Spatial Autocorrelation",
    "section": "",
    "text": "R for Geospatial Data Science and Analytics - 9  Global Measures of Spatial Autocorrelation"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#overview",
    "title": "5A: Global Measures of Spatial Autocorrelation",
    "section": "2 Overview",
    "text": "2 Overview\nIn this exercise, we will learn to compute Global Measures of Spatial Autocorrelation using the spdep package, including Moran’s I and Geary’s C tests, spatial correlograms, and their statistical interpretation."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#learning-outcome",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#learning-outcome",
    "title": "5A: Global Measures of Spatial Autocorrelation",
    "section": "3 Learning Outcome",
    "text": "3 Learning Outcome\n\nImport geospatial data using the sf package\nImport CSV data using the readr package\nPerform relational joins using the dplyr package\nCompute Global Spatial Autocorrelation (GSA) statistics using the spdep package\n\nMoran’s I test and Monte Carlo simulation\nGeary’s C test and Monte Carlo simulation\n\nPlot Moran scatterplot and spatial correlograms\nInterpret GSA statistics correctly"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#the-analytical-question",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#the-analytical-question",
    "title": "5A: Global Measures of Spatial Autocorrelation",
    "section": "4 The Analytical Question",
    "text": "4 The Analytical Question\nIn spatial policy planning, one of the main development objective of the local government and planners is to ensure equal distribution of development in the province. In this study, we will apply spatial statistical methods to examine the distribution of development in Hunan Province, China, using a selected indicator (e.g., GDP per capita).\n\nOur key questions are:\n\nIs development evenly distributed geographically?\nIf not, is there evidence of spatial clustering?\nIf clustering exists, where are these clusters located?"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#the-data",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#the-data",
    "title": "5A: Global Measures of Spatial Autocorrelation",
    "section": "5 The Data",
    "text": "5 The Data\nThe following 2 datasets will be used in this exercise.\n\n\n\n\n\n\n\n\nData Set\nDescription\nFormat\n\n\n\n\nHunan county boundary layer\nGeospatial data set representing the county boundaries of Hunan\nESRI Shapefile\n\n\nHunan_2012.csv\nContains selected local development indicators for Hunan in 2012\nCSV"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#installing-and-launching-the-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#installing-and-launching-the-r-packages",
    "title": "5A: Global Measures of Spatial Autocorrelation",
    "section": "6 Installing and Launching the R Packages",
    "text": "6 Installing and Launching the R Packages\nThe following R packages will be used in this exercise:\n\n\n\n\n\n\n\n\nPackage\nPurpose\nUse Case in Exercise\n\n\n\n\nsf\nImports, manages, and processes vector-based geospatial data.\nHandling vector geospatial data such as the Hunan county boundary layer in shapefile format.\n\n\nspdep\nProvides functions for spatial dependence analysis, including spatial weights and spatial autocorrelation.\nComputing spatial weights and creating spatially lagged variables.\n\n\ntmap\nCreates static and interactive thematic maps using cartographic quality elements.\nVisualizing regional development indicators and plotting maps showing spatial relationships and patterns.\n\n\ntidyverse\nA collection of packages for data science tasks such as data manipulation, visualization, and modeling.\nImporting CSV files, wrangling data, and performing relational joins.\n\n\n\nTo install and load these packages, use the following code:\n\npacman::p_load(sf, spdep, tmap, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#import-data-and-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#import-data-and-preparation",
    "title": "5A: Global Measures of Spatial Autocorrelation",
    "section": "7 Import Data and Preparation",
    "text": "7 Import Data and Preparation\nIn this section, we will perform 3 necessary steps to prepare the data for analysis.\n\n\n\n\n\n\nNote\n\n\n\nThe data preparation is the same as previous exercise such as Exercise 4A.\n\n\n\n7.1 Import Geospatial Shapefile\nFirstly, we will use st_read() of sf package to import Hunan shapefile into R. The imported shapefile will be simple features Object of sf.\n\nhunan &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `/Users/walter/code/isss626/isss626-gaa/Hands-on_Ex/Hands-on_Ex05/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n7.2 Import Aspatial csv File\nNext, we will import Hunan_2012.csv into R by using read_csv() of readr package. The output is R dataframe class.\n\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\n\n\n7.3 Perform Relational Join\nThen, we will perform a left_join() to update the attribute table of hunan’s SpatialPolygonsDataFrame with the attribute fields of hunan2012 dataframe.\n\nhunan &lt;- left_join(hunan,hunan2012) %&gt;%\n  select(1:4, 7, 15)\n\n\n\n7.4 Visualizing Regional Development Indicator\nTo visualize the regional development indicator, we can prepare a base map and a choropleth map to show the distribution of GDPPC 2012 (GDP per capita) by using qtm() of tmap package.\n\nequal &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(col = \"grey\") +\n  tm_layout(main.title = \"Equal Interval Classification\")\n\nquantile &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(col = \"grey\") +\n  tm_layout(main.title = \"Equal Quantile Classification\")\n\ntmap_arrange(equal, \n             quantile, \n             asp=1, \n             ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nObservations\nOn the left plot, we perform equal interval classification which divides the range of GDPPC into five equal-sized intervals. This method ensures that the difference between the maximum and minimum value within each class is the same.\nThe equal interval classification method is best used for continuous datasets such as precipitation or temperature.\nThe advantage of the equal interval classification method is that it creates a legend that is easy to interpret and present to a nontechnical audience. The primary disadvantage is that certain datasets will end up with most of the data values falling into only one or two classes, while few to no values will occupy the other classes.\n\nOn the right plot, we perform equal quantile classification which divides the regions into five classes such that each class contains approximately the same number of regions. This method adjusts the intervals to ensure an equal number of regions per class, which might result in unequal interval sizes.\nThe equal quantile classification is best for data that is evenly distributed across its range.\nAs there are 88 counties in Hunan, each class in the quantile classification methodology will contain 88 / 5 = 17.6 different counties. The advantage to this method is that it often excels at emphasizing the relative position of the data values (i.e., which counties contain the top 20 percent of the Hunan population). The primary disadvantage of the quantile classification methodology is that features placed within the same class can have wildly differing values, particularly if the data are not evenly distributed across its range. In addition, the opposite can also happen whereby values with small range differences can be placed into different classes, suggesting a wider difference in the dataset than actually exists.\nFor more info, see Data Classification"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#global-measures-of-spatial-autocorrelation",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#global-measures-of-spatial-autocorrelation",
    "title": "5A: Global Measures of Spatial Autocorrelation",
    "section": "8 Global Measures of Spatial Autocorrelation",
    "text": "8 Global Measures of Spatial Autocorrelation\nIn this section, we will compute global spatial autocorrelation statistics and to perform spatial complete randomness test for global spatial autocorrelation.\n\n8.1 Computing Contiguity Spatial Weights\nBefore we can compute the global spatial autocorrelation statistics, we need to construct a spatial weights of the study area. The spatial weights is used to define the neighbourhood relationships between the geographical units (i.e. county) in the study area.\nIn the code block below, the poly2nb() function from the spdep package calculates contiguity weight matrices for the study area by identifying regions that share boundaries.\nBy default, poly2nb() uses the “Queen” criteria, which considers any shared boundary or corner as a neighbor (equivalent to setting queen = TRUE). If we want to restrict the criteria to shared boundaries only (excluding corners), set queen = FALSE.\n\nwm_q &lt;- poly2nb(hunan, \n                queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours. There are two area units with only one neighbours.\n\n\n8.2 Row-standardised Weights Matrix\nNext, we need to assign weights to each neighboring polygon. In this case, we’ll use equal weights (style=“W”), where each neighboring polygon gets a weight of 1/(number of neighbors). This means we take the value for each neighbor and divide it by the total number of neighbors, then sum these weighted values to calculate a summary measure, such as weighted income.\nWhile this equal weighting approach is straightforward and easy to understand, it has a limitation: polygons on the edges of the study area have fewer neighbors, which can lead to over- or underestimation of the actual spatial relationships (spatial autocorrelation) in the data.\n\n\n\n\n\n\nTip\n\n\n\nFor simplicity, we use the style=“W” option in this example, but keep in mind that other, potentially more accurate methods are available, such as style=“B”.\n\n\n\nrswm_q &lt;- nb2listw(wm_q, style=\"W\", zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nThe input of nb2listw() must be an object of class nb. The syntax of the function has two major arguments, namely style and zero.poly.\nstyle can take values “W”, “B”, “C”, “U”, “minmax” and “S”. B is the basic binary coding, W is row standardised (sums over all links to n), C is globally standardised (sums over all links to n), U is equal to C divided by the number of neighbours (sums over all links to unity), while S is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al. 1999, p. 167-168 (sums over all links to n).\nThe zero.policy=TRUE option allows for lists of non-neighbors. This should be used with caution since the user may not be aware of missing neighbors in their dataset however, a zero.policy of FALSE would return an error."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#global-measures-of-spatial-autocorrelation-morans-i",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#global-measures-of-spatial-autocorrelation-morans-i",
    "title": "5A: Global Measures of Spatial Autocorrelation",
    "section": "9 Global Measures of Spatial Autocorrelation: Moran’s I",
    "text": "9 Global Measures of Spatial Autocorrelation: Moran’s I\nIn this section, you will learn how to perform Moran’s I statistics testing by using moran.test() of spdep.\n\n\n\n\n\n\nNote\n\n\n\nSpatial Autocorrelation, specifically Global Moran’s I, is a statistical measure used to evaluate the degree to which similar values in a dataset are clustered together or dispersed across a geographic space.\nIn simpler terms, it measures whether similar values occur near each other (positive autocorrelation) or if dissimilar values are found near each other (negative autocorrelation).\nGlobal Moran’s I takes into account both the locations of features and the values associated with those features. It computes an index value (Moran’s I), a z-score, and a p-value to determine the statistical significance of the observed spatial pattern\nIntepreting Moran’s I index\nThe Moran’s I index ranges from -1 to +1. A value close to +1 indicates clustering of similar values, a value close to -1 indicates dispersion of similar values, and a value near 0 suggests a random spatial pattern2.\n\nPositive Moran’s I: Indicates that high values are near other high values, and low values are near other low values.\nNegative Moran’s I: Indicates that high values are near low values and vice versa.\nZero Moran’s I: Suggests no spatial autocorrelation, implying a random distribution.\n\nFor more info, see How Spatial Autocorrelation (Global Moran’s I) works—ArcGIS Pro | Documentation\n\n\n\n9.1 Moran’s I test\nTo assess whether there is significant spatial autocorrelation in the GDP per capita (GDPPC) across regions, we use Moran’s I test. The test is performed using the moran.test() function from the spdep package.\n\nNull Hypothesis (\\(H_0\\)): There is no spatial autocorrelation in GDP per capita across the regions (Moran’s I = 0).\nAlternative Hypothesis (\\(H_1\\)): There is positive spatial autocorrelation in GDP per capita (Moran’s I &gt; 0).\n\nWe will use an alpha value (α) of 0.05 (95% confidence level) to determine the statistical significance.\n\nmoran.test(hunan$GDPPC, \n           listw=rswm_q, \n           zero.policy = TRUE, \n           na.action=na.omit)\n\n\n    Moran I test under randomisation\n\ndata:  hunan$GDPPC  \nweights: rswm_q    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\n\n\n\n\n\n\nNote\n\n\n\nQuestion: What statistical conclusion can you draw from the output above?\n\nThe value of Moran’s I is 0.3007, a positive number, indicating positive spatial autocorrelation. This means that regions with similar GDP per capita (GDPPC) values tend to be geographically close to each other.\nThe p-value is 1.095e-06, which is much smaller than our alpha value of 0.05. This provides strong evidence against the null hypothesis of no spatial autocorrelation.\n\nTherefore, We will reject the null hypothesis at 95% confidence interval because the p-value is smaller than our chosen alpha value.\n\n\n\n\n9.2 Computing Monte Carlo Moran’s I\nIn this example, we explore the spatial distribution of Hunan GDPPC by county for the state of Hunan using the Monte Carlo approach. A total of 1000 simulation will be performed. We will perform the same hypothesis testing described above.\n\nset.seed(1234)\n\nbperm= moran.mc(hunan$GDPPC, \n                listw=rswm_q, \n                nsim=999, \n                zero.policy = TRUE, \n                na.action=na.omit)\nbperm\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  hunan$GDPPC \nweights: rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.30075, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\n\n\n\n\n\n\nNote\n\n\n\nQuestion: What statistical conclusion can you draw from the output above?\n\nThe value of Moran’s I is 0.30075, which is a positive number, indicating positive spatial autocorrelation. This suggests that regions with similar GDP per capita (GDPPC) values are geographically close to each other.\nThe p-value obtained from the Monte Carlo simulation is 0.001, which is much smaller than our alpha value of 0.05. This provides strong evidence against the null hypothesis of no spatial autocorrelation.\n\nTherefore, we will reject the null hypothesis at 95% confidence interval because the p-value is smaller than our chosen alpha value.\n\n\n\n\n9.3 Visualising Monte Carlo Moran’s I\nIt is always a good practice for us the examine the simulated Moran’s I test statistics in greater detail. This can be achieved by plotting the distribution of the statistical values as a histogram by using the code block below.\nWe will first observe the summary report of the Monte Carlo Moran’s I output before visualizing the plots using ggplot2 and base R.\n\n# Calculate the mean of the first 999 simulated Moran's I values\nmean(bperm$res[1:999])\n\n[1] -0.01504572\n\n# Calculate the variance of the first 999 simulated Moran's I values\nvar(bperm$res[1:999])\n\n[1] 0.004371574\n\nsummary(bperm$res[1:999])\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.18339 -0.06168 -0.02125 -0.01505  0.02611  0.27593 \n\n\n\nggplot2base R\n\n\n\nggplot(data.frame(x = bperm$res), aes(x = x)) + \n  geom_histogram(binwidth = diff(range(bperm$res)) / 20,\n                 fill = \"grey\",\n                 color = \"black\") +\n  geom_vline(xintercept = 0,\n             color = \"red\", \n             linetype = \"solid\") +\n  labs(x = \"Simulated Moran's I\",\n       y = \"Frequency\",\n       title = \"Histogram of Simulated Moran's I Values\")\n\n\n\n\n\n\n\n\n\n\n\nhist(bperm$res, \n     freq = TRUE,         # Show the frequency (count) on y-axis\n     breaks = 20,         # Set the number of bins\n     xlab = \"Simulated Moran's I\")\n\n# Add vertical red line at 0 to indicate the mean under the null hypothesis of no autocorrelation\nabline(v = 0, \n       col = \"red\") \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQuestion: What statistical conclusion can you draw from the output above?\nThe observed Moran’s I value (0.30075) lies outside the range of most simulated values, indicating that it is an outlier compared to the expected distribution under the null hypothesis, which are centered around the expected value of 0.0 under the null hypothesis of no spatial autocorrelation. The histogram shows that most of the simulated values of Moran’s I are clustered around the mean of -0.01504572, with a variance of 0.004371574.\nSince the observed Moran’s I value is significantly greater than the bulk of the simulated values and given the p-value from the test is very small (p &lt; 0.05), there is strong evidence against the null hypothesis.\nThere is significant positive spatial autocorrelation in the GDP per capita (GDPPC) across regions, as indicated by the Moran’s I test. This suggests that regions with similar GDPPC values are more likely to be geographically clustered."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#global-measures-of-spatial-autocorrelation-gearys-c",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#global-measures-of-spatial-autocorrelation-gearys-c",
    "title": "5A: Global Measures of Spatial Autocorrelation",
    "section": "10 Global Measures of Spatial Autocorrelation: Geary’s C",
    "text": "10 Global Measures of Spatial Autocorrelation: Geary’s C\nIn this section, we will perform Geary’s C statistics testing by using appropriate functions of spdep package.\n\n10.1 Geary’s C test\nAnother popular index of global spatial autocorrelation is Geary’s C which is a cousin to the Moran’s I.\n\n\n\n\n\n\nNote\n\n\n\nGeary’s C, also known as Geary’s contiguity ratio, is used to assess the degree of spatial autocorrelation in a dataset.\nIt is particularly sensitive to local variations in spatial data, making it suitable for analyzing patterns within smaller areas.\nInterpreting Geary’s C Values\nGeary’s C values range from 0 to 2. Under the null hypothesis of no spatial autocorrelation, the expected value of Geary’s C is 1.\n\nValues &lt; 1: Indicate positive spatial autocorrelation, meaning similar values are clustered together.\nValues = 1: Suggest a random spatial pattern with no autocorrelation.\nValues &gt; 1: Indicate negative spatial autocorrelation, meaning dissimilar values are clustered together.\n\n\n\n\nWhile Moran’s I and Geary’s C are both measures of global spatial autocorrelation, they are slightly different. Geary’s C uses the sum of squared distances whereas Moran’s I uses standardized spatial covariance.\nUnlike Moran’s I, which focuses on global patterns, Geary’s C emphasizes local variations and can reveal nuances in spatial relationships.\nFor more info: Geary’s C, Geary’s C\n\nSimilarly, to assess whether there is significant spatial autocorrelation in the GDP per capita (GDPPC) across regions, we can performGeary’s C test for spatial autocorrelation by using geary.test() of spdep.\n\nNull Hypothesis (\\(H_o\\)): There is no spatial autocorrelation in GDP per capita across the regions (Geary’s C = 1).\nAlternative Hypothesis (\\(H_1\\)): There is positive spatial autocorrelation in GDP per capita (Geary’s C &lt; 1).\n\nWe will use an alpha value (α) of 0.05 (95% confidence level) to determine the statistical significance.\n\ngeary.test(hunan$GDPPC, listw=rswm_q)\n\n\n    Geary C test under randomisation\n\ndata:  hunan$GDPPC \nweights: rswm_q   \n\nGeary C statistic standard deviate = 3.6108, p-value = 0.0001526\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n        0.6907223         1.0000000         0.0073364 \n\n\n\n\n\n\n\n\nNote\n\n\n\nQuestion: What statistical conclusion can you draw from the output above?\n\nThe value of Geary’s C statistic is 0.6907, which is less than the expected value of 1.0. This indicates positive spatial autocorrelation, meaning regions with similar GDP per capita (GDPPC) values tend to be geographically close to each other.\nThe p-value is 0.0001526, which is much smaller than our alpha value of 0.05. This provides strong evidence against the null hypothesis of no spatial autocorrelation.\n\nTherefore, we will reject the null hypothesis at 95% confidence interval because the p-value is smaller than our chosen alpha value.\n\n\n\n\n10.2 Computing Monte Carlo Geary’s C\nSimilar to Moran’s I, it is best to test the statistical significance of Geary’s C using a Monte Carlo simulation.\nTo perform permutation test for Geary’s C statistic by using geary.mc() of spdep:\n\nset.seed(1234)\nbperm=geary.mc(hunan$GDPPC, \n               listw=rswm_q, \n               nsim=999)\nbperm\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  hunan$GDPPC \nweights: rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.69072, observed rank = 1, p-value = 0.001\nalternative hypothesis: greater\n\n\n\n\n\n\n\n\nNote\n\n\n\nQuestion: What statistical conclusion can you draw from the output above?\n\nThe value of Geary’s C statistic is 0.6907, which is less than the expected value of 1.0. This indicates positive spatial autocorrelation, meaning regions with similar GDP per capita (GDPPC) values tend to be geographically close to each other.\nThe p-value from the Monte Carlo simulation is 0.001, which is much smaller than our alpha value of 0.05. This provides strong evidence against the null hypothesis of no spatial autocorrelation.\n\nTherefore, we will reject the null hypothesis at 95% confidence because the p-value is smaller than our chosen alpha value.\n\n\n\n\n10.3 Visualising the Monte Carlo Geary’s C\nNext, we will plot a histogram to reveal the distribution of the simulated values by using the code block below.\nWe will first observe the summary report of the Geary’s C output before visualizing the plots using ggplot2 and base R.\n\n# Calculate the mean of the first 999 simulated geary's c values\nmean(bperm$res[1:999])\n\n[1] 1.004402\n\n# Calculate the variance of the first 999 simulated geary's c values\nvar(bperm$res[1:999])\n\n[1] 0.007436493\n\nsummary(bperm$res[1:999])\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.7142  0.9502  1.0052  1.0044  1.0595  1.2722 \n\n\n\nggplot2base R\n\n\n\nggplot(data.frame(x = bperm$res), aes(x = x)) + \n  geom_histogram(binwidth = diff(range(bperm$res)) / 20,\n                 color = \"black\", \n                 fill = \"grey\") + \n  geom_vline(xintercept = 1, \n             color = \"red\") + \n  labs(x = \"Simulated Geary's C\", \n       y = \"Frequency\",\n       title = \"Histogram of Simulated Geary's C Values\")       \n\n\n\n\n\n\n\n\n\n\n\nhist(bperm$res, freq=TRUE, breaks=20, xlab=\"Simulated Geary's C\")\nabline(v=1, col=\"red\") \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQuestion: What statistical conclusion can you draw from the output above?\nThe observed Geary’s C value (0.69072) lies outside the range of most simulated values, which are centered around the expected value of 1.0 under the null hypothesis of no spatial autocorrelation. The histogram shows that most of the simulated values of Geary’s C are clustered around the mean of 1.0044, with a variance of 0.0074.\nSince the observed Geary’s C value is significantly lower than the bulk of the simulated values and the p-value from the test is very small (p &lt; 0.05), there is strong evidence against the null hypothesis.\nThere is significant positive spatial autocorrelation in the GDP per capita (GDPPC) across regions, as indicated by the Geary’s C test. This suggests that regions with similar GDPPC values are more likely to be geographically clustered."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#spatial-correlogram",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#spatial-correlogram",
    "title": "5A: Global Measures of Spatial Autocorrelation",
    "section": "11 Spatial Correlogram",
    "text": "11 Spatial Correlogram\nSpatial correlograms are useful for examining patterns of spatial autocorrelation in your data or model residuals. They show how the correlation between pairs of spatial observations changes as the distance (lag) between them increases. Essentially, they are plots of a spatial autocorrelation index (such as Moran’s I or Geary’s C) against distance.\nWhile correlograms are not as fundamental as variograms—a core concept in geostatistics—they serve as powerful exploratory and descriptive tools. In fact, for examining spatial patterns, correlograms can provide more detailed insights than variograms.\n\n11.1 Compute Moran’s I Correlogram\nIn the code below, we use the sp.correlogram() function from the spdep package to compute a 6-lag spatial correlogram for GDP per capita (GDPPC). This function calculates global spatial autocorrelation using Moran’s I. The base R plot() function is then used to visualize the correlogram.\n\nMI_corr &lt;- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order = 6, \n                          method = \"I\", \n                          style = \"W\")\nplot(MI_corr)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nHowever, simply plotting the output does not provide a complete interpretation because not all autocorrelation values may be statistically significant. Therefore, it is important to examine the full analysis report by printing the results.\n\n\n\nprint(MI_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Moran's I\n         estimate expectation   variance standard deviate Pr(I) two sided    \n1 (88)  0.3007500  -0.0114943  0.0043484           4.7351       2.189e-06 ***\n2 (88)  0.2060084  -0.0114943  0.0020962           4.7505       2.029e-06 ***\n3 (88)  0.0668273  -0.0114943  0.0014602           2.0496        0.040400 *  \n4 (88)  0.0299470  -0.0114943  0.0011717           1.2107        0.226015    \n5 (88) -0.1530471  -0.0114943  0.0012440          -4.0134       5.984e-05 ***\n6 (88) -0.1187070  -0.0114943  0.0016791          -2.6164        0.008886 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nNote\n\n\n\nQuestion: What statistical observation can you draw from the plot above?\nThe spatial correlogram shows Moran’s I values for different distance lags, which indicate how spatial autocorrelation in GDP per capita (GDPPC) changes as the distance between regions increases.\n\nSignificant Positive Spatial Autocorrelation at Shorter Distances:\n\nFor lags 1 and 2, Moran’s I values are significantly positive (0.30075 and 0.20601, respectively). The p-values for these lags are very small (p &lt; 0.001). This suggests that regions with similar GDPPC values tend to be clustered together at these shorter distances.\n\nDecreasing and Insignificant Spatial Autocorrelation at Moderate Distances:\n\nAt lag 3, Moran’s I value is 0.06683, with the confidence interval still above zero, indicating weak but significant positive spatial autocorrelation (p &lt; 0.05).\nAt lag 4, Moran’s I value further decreases to 0.02995. The p-value (0.226) is not significant, suggesting that spatial autocorrelation is not statistically significant at this distance.\n\nSignificant Negative Spatial Autocorrelation at Longer Distances:\n\nFor lags 5 and 6, Moran’s I values become negative (-0.15305 and -0.11871, respectively). The p-values for these lags are very small (p &lt; 0.001 and p &lt; 0.01, respectively), indicating statistically significant negative spatial autocorrelation. This means that regions with dissimilar GDPPC values tend to be found farther apart.\n\n\n\n\n\n\n11.2 Compute Geary’s C Correlogram and Plot\nSimilarly, we can use sp.correlogram() of spdep package to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used in Geary’s C and the plot() of base Graph is then used to plot the output.\n\nGC_corr &lt;- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"C\", \n                          style=\"W\")\nplot(GC_corr)\n\n\n\n\n\n\n\n\nSimilar to the previous step, we will print out the analysis report by using the code block below.\n\nprint(GC_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Geary's C\n        estimate expectation  variance standard deviate Pr(I) two sided    \n1 (88) 0.6907223   1.0000000 0.0073364          -3.6108       0.0003052 ***\n2 (88) 0.7630197   1.0000000 0.0049126          -3.3811       0.0007220 ***\n3 (88) 0.9397299   1.0000000 0.0049005          -0.8610       0.3892612    \n4 (88) 1.0098462   1.0000000 0.0039631           0.1564       0.8757128    \n5 (88) 1.2008204   1.0000000 0.0035568           3.3673       0.0007592 ***\n6 (88) 1.0773386   1.0000000 0.0058042           1.0151       0.3100407    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nNote\n\n\n\nQuestion: What statistical observation can you draw from the plot above?\nThe spatial correlogram shows Geary’s C values for different distance lags, illustrating how spatial autocorrelation in GDP per capita (GDPPC) changes with distance:\n\nSignificant Positive Spatial Autocorrelation at Shorter Distances:\n\nFor lags 1 and 2, Geary’s C values (0.6907 and 0.7630, respectively). This indicates significant positive spatial autocorrelation, suggesting that similar GDPPC values are clustered together at shorter distances (p &lt; 0.05).\n\nInsignificant Autocorrelation at Moderate Distances:\n\nAt lags 3 and 4, Geary’s C values are close to 1.0 (0.9397 and 1.0098), and their confidence intervals include 1, indicating no significant spatial autocorrelation (p &gt; 0.05).\n\nSignificant Negative Spatial Autocorrelation at Longer Distances:\n\nAt lag 5, Geary’s C value (1.2008) is significantly greater than 1.0, with the confidence interval not crossing 1, indicating significant negative spatial autocorrelation (p &lt; 0.05). This suggests that dissimilar GDPPC values are more likely to be found at longer distances.\n\nNo Significant Spatial Autocorrelation at the Furthest Distance:\n\nAt lag 6, Geary’s C value (1.0773) is slightly above 1, but the confidence interval includes 1, indicating no significant spatial autocorrelation (p &gt; 0.05)."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02a.html",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02a.html",
    "title": "2A: 1st Order Spatial Point Patterns Analysis",
    "section": "",
    "text": "R for Geospatial Data Science and Analytics - 4  1st Order Spatial Point Patterns Analysis Methods"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02a.html#exercise-2a-reference",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02a.html#exercise-2a-reference",
    "title": "2A: 1st Order Spatial Point Patterns Analysis",
    "section": "",
    "text": "R for Geospatial Data Science and Analytics - 4  1st Order Spatial Point Patterns Analysis Methods"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02a.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02a.html#overview",
    "title": "2A: 1st Order Spatial Point Patterns Analysis",
    "section": "2 Overview",
    "text": "2 Overview\nSpatial Point Pattern Analysis is the evaluation of the pattern or distribution, of a set of points on a surface. The point can be location of:\n\nevents such as crime, traffic accident and disease onset, or\nbusiness services (coffee and fastfood outlets) or facilities such as childcare and eldercare.\n\nUsing appropriate functions of spatstat, this hands-on exercise aims to discover the spatial point processes of childecare centres in Singapore.\nThe specific questions we would like to answer are as follows:\n\nare the childcare centres in Singapore randomly distributed throughout the country?\n\nif the answer is not, then the next logical question is where are the locations with higher concentration of childcare centres?"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02a.html#learning-outcome",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02a.html#learning-outcome",
    "title": "2A: 1st Order Spatial Point Patterns Analysis",
    "section": "3 Learning Outcome",
    "text": "3 Learning Outcome\n\nImporting and managing geospatial data using sf and spatstat packages.\nConverting simple feature data frames to various spatial data formats.\nPerforming kernel density estimation (KDE) using different methods and bandwidths.\nConducting nearest neighbor analysis to test spatial point distribution.\nVisualizing spatial patterns and KDE results using tmap and spatstat.\nHandling duplicate spatial points and creating point patterns for analysis.\nApplying appropriate statistical methods for confirmatory spatial point pattern analysis."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02a.html#the-data",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02a.html#the-data",
    "title": "2A: 1st Order Spatial Point Patterns Analysis",
    "section": "4 The Data",
    "text": "4 The Data\n\n\n\n\n\n\n\n\n\nDataset\nDescription\nSource\nFormat\n\n\n\n\nCHILDCARE\nPoint data containing location and attributes of childcare centers.\nData.gov.sg\nGeoJSON\n\n\nMP14_SUBZONE_WEB_PL\nPolygon data with URA 2014 Master Plan Planning Subzone boundaries.\nData.gov.sg\nESRI Shapefile\n\n\nCoastalOutline\nPolygon data representing Singapore’s national boundary.\nSingapore Land Authority (SLA)\nESRI Shapefile"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02a.html#installing-and-loading-the-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02a.html#installing-and-loading-the-r-packages",
    "title": "2A: 1st Order Spatial Point Patterns Analysis",
    "section": "5 Installing and Loading the R Packages",
    "text": "5 Installing and Loading the R Packages\nThe following R packages will be used in this exercise:\n\n\n\n\n\n\n\n\nPackage\nPurpose\nUse Case in Exercise\n\n\n\n\nsf\nImports, manages, and processes vector-based geospatial data.\nHandling vector geospatial data in R.\n\n\nspatstat\nProvides tools for point pattern analysis.\nPerforming 1st- and 2nd-order spatial point pattern analysis and deriving kernel density estimation (KDE).\n\n\nraster\nReads, writes, and manipulates gridded spatial data (raster).\nConverting image outputs from spatstat into raster format.\n\n\nmaptools\nOffers tools for manipulating geographic data.\nConverting spatial objects into ppp format for use with spatstat.\n\n\ntmap\nCreates static and interactive thematic maps using cartographic quality elements.\nPlotting static and interactive point pattern maps.\n\n\n\nTo install and load these packages in R, use the following code:\n\npacman::p_load(sf, raster, spatstat, tmap, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02a.html#reproducibility",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02a.html#reproducibility",
    "title": "2A: 1st Order Spatial Point Patterns Analysis",
    "section": "6 Reproducibility",
    "text": "6 Reproducibility\nAs this document involves Monte Carlo simulations, we will set the seed to ensure reproducibility\n\nset.seed(1234)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02a.html#spatial-data-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02a.html#spatial-data-wrangling",
    "title": "2A: 1st Order Spatial Point Patterns Analysis",
    "section": "7 Spatial Data Wrangling",
    "text": "7 Spatial Data Wrangling\n\n7.1 Importing the Spatial Data\nTo import the three geographical datasets, we will use st_read() from sf.\n\nchildcare_sf &lt;- st_read(\"data/child-care-services-geojson.geojson\")\n\nReading layer `child-care-services-geojson' from data source \n  `/Users/walter/code/isss626/isss626-gaa/Hands-on_Ex/Hands-on_Ex02/data/child-care-services-geojson.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1545 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6824 ymin: 1.248403 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\nsg_sf &lt;- st_read(dsn = \"data\", layer=\"CostalOutline\")\n\nReading layer `CostalOutline' from data source \n  `/Users/walter/code/isss626/isss626-gaa/Hands-on_Ex/Hands-on_Ex02/data' \n  using driver `ESRI Shapefile'\nSimple feature collection with 60 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2663.926 ymin: 16357.98 xmax: 56047.79 ymax: 50244.03\nProjected CRS: SVY21\n\n\n\nmpsz_sf &lt;- st_read(dsn = \"data\",\n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/walter/code/isss626/isss626-gaa/Hands-on_Ex/Hands-on_Ex02/data' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\n\n7.2 Inspect and Reproject to Same Projection System\nBefore we can use these data for analysis, it is important for us to ensure that they are projected in same projection system.\nFirst, we check the childcare dataset.\n\nst_crs(childcare_sf)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n\nThis dataset is using the WGS84 crs. We will reproject all the dataset to SVY21 crs for standardization and analysis.\n\nchildcare_sf &lt;- st_transform(childcare_sf , crs = 3414)\nst_crs(childcare_sf)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nThe childcare dataset has been reprojected to SVY21 successfully.\nNext, we inspect the Coastal Outline dataset.\n\nst_crs(sg_sf)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\nNotice that this dataset is using SVY21 crs, however the ID provided is EPSG:9001 does not match the intended ID, EPSG:3414 of SVY21. In this case, we will set the crs to the correct ID using the code block below.\n\nsg_sf &lt;- st_set_crs(sg_sf, 3414)\nst_crs(sg_sf)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nSimilarly, we will inspect the Master Plan Subzone Dataset.\n\nst_crs(mpsz_sf)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\nSince the ID is also EPSG:9001, we will set the crs to EPSG:3414 too.\n\nmpsz_sf &lt;- st_set_crs(mpsz_sf, 3414)\nst_crs(mpsz_sf)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02a.html#mapping-the-geospatial-datasets",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02a.html#mapping-the-geospatial-datasets",
    "title": "2A: 1st Order Spatial Point Patterns Analysis",
    "section": "8 Mapping the Geospatial Datasets",
    "text": "8 Mapping the Geospatial Datasets\nAfter checking the referencing system of each geospatial data data frame, it is also useful for us to plot a map to show their spatial patterns.\n\n8.1 Static Map\n\ntmap_options(check.and.fix = TRUE)\n# add polygon layer of the coastal outline of sg island\ntm_shape(sg_sf)+ tm_polygons() +\n# add polygon layer of the subzone based on sg masterplan\ntm_shape(mpsz_sf) + tm_polygons() +\n# add dot layer to show the locations of childcare centres\ntm_shape(childcare_sf) + tm_dots() +\ntm_layout()\n\n\n\n\n\n\n\n\nWhen all the 3 datasets are overlayed together, it shows the locations of childcare centres on the Singapore island. Since all the geospatial layers are within the same map context, it means their referencing system and coordinate values are referred to similar spatial context. This consistency is crucial for accurate geospatial analysis.\n\n\n8.2 Interactive Map\nAlternatively, we can also prepare a pin map by using the code block below.\n\ntmap_mode('view')\n\n# tm_basemap(\"Esri.WorldGrayCanvas\") +\n# tm_basemap(\"OpenStreetMap\") +\ntm_basemap(\"Esri.WorldTopoMap\") +\ntm_shape(childcare_sf) +\n  tm_dots(alpha = 0.5)\n\n\n\n\n\n\ntmap_mode('plot')\n\nIn interactive mode, tmap uses the Leaflet for R API, allowing you to freely navigate, zoom, and click on features for detailed information. You can also change the map’s background using layers like ESRI.WorldGrayCanvas, OpenStreetMap, and ESRI.WorldTopoMap, with ESRI.WorldGrayCanvas as the default.\n\n\n\n\n\n\nTip\n\n\n\nRemember to switch back to plot mode after interacting to avoid connection issues and limit interactive maps to fewer than 10 in RMarkdown documents for Netlify publishing."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02a.html#geospatial-data-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02a.html#geospatial-data-wrangling",
    "title": "2A: 1st Order Spatial Point Patterns Analysis",
    "section": "9 Geospatial Data Wrangling",
    "text": "9 Geospatial Data Wrangling\nWhile simple feature data frames are becoming more popular compared to sp’s Spatial* classes, many geospatial analysis packages still require data in the Spatial* format. This section will show you how to convert a simple feature data frame to sp’s Spatial* class.\n\n9.1 Converting sf data frames to sp’s Spatial* class\nThe code block below uses as_Spatial() of sf package to convert the three geospatial data from simple feature data frame to sp’s Spatial* class.\n\nchildcare &lt;- as_Spatial(childcare_sf)\nmpsz &lt;- as_Spatial(mpsz_sf)\nsg &lt;- as_Spatial(sg_sf)\n\nAfter the sf dataframe to sp Spatial* conversion, let’s inspect the Spatial* classes.\n\nchildcare\n\nclass       : SpatialPointsDataFrame \nfeatures    : 1545 \nextent      : 11203.01, 45404.24, 25667.6, 49300.88  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 2\nnames       :    Name,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Description \nmin values  :   kml_1, &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;018989&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;1, MARINA BOULEVARD, #B1 - 01, ONE MARINA BOULEVARD, SINGAPORE 018989&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;THE LITTLE SKOOL-HOUSE INTERNATIONAL PTE. LTD.&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;08F73931F4A691F4&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20200826094036&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt; \nmax values  : kml_999,                  &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;829646&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;200, PONGGOL SEVENTEENTH AVENUE, SINGAPORE 829646&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Child Care Services&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;RAFFLES KIDZ @ PUNGGOL PTE LTD&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;379D017BF244B0FA&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20200826094036&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt; \n\n\n\nmpsz\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 323 \nextent      : 2667.538, 56396.44, 15748.72, 50256.33  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 15\nnames       : OBJECTID, SUBZONE_NO, SUBZONE_N, SUBZONE_C, CA_IND, PLN_AREA_N, PLN_AREA_C,       REGION_N, REGION_C,          INC_CRC, FMEL_UPD_D,     X_ADDR,     Y_ADDR,    SHAPE_Leng,    SHAPE_Area \nmin values  :        1,          1, ADMIRALTY,    AMSZ01,      N, ANG MO KIO,         AM, CENTRAL REGION,       CR, 00F5E30B5C9B7AD8,      16409,  5092.8949,  19579.069, 871.554887798, 39437.9352703 \nmax values  :      323,         17,    YUNNAN,    YSSZ09,      Y,     YISHUN,         YS,    WEST REGION,       WR, FFCCF172717C2EAF,      16409, 50424.7923, 49552.7904, 68083.9364708,  69748298.792 \n\n\n\nsg\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 60 \nextent      : 2663.926, 56047.79, 16357.98, 50244.03  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 4\nnames       : GDO_GID, MSLINK, MAPID,              COSTAL_NAM \nmin values  :       1,      1,     0,             ISLAND LINK \nmax values  :      60,     67,     0, SINGAPORE - MAIN ISLAND \n\n\nThe geospatial data have been converted into their respective sp’s Spatial* classes.\n\n\n9.2 Converting the Spatial* Class into Generic sp Format\nspatstat requires the analytical data in ppp object form. There is no straightforward way to convert a Spatial* classes into ppp object. We need to convert the Spatial classes* into Spatial object first.\n\n\n\n\n\n\nTip\n\n\n\nppp refers to planar point pattern. It is used to represent spatial point patterns in spatstat; it contains the event locations with possibly associated marks, and the observation window where the events occur.\nsee Chapter 18 The spatstat package | Spatial Statistics for Data Science: Theory and Practice with R\n\n\nThe codes block below converts the Spatial* classes into generic sp objects.\n\nchildcare_sp &lt;- as(childcare, \"SpatialPoints\")\nsg_sp &lt;- as(sg, \"SpatialPolygons\")\n\nNext, we can display the sp objects properties.\n\nchildcare_sp\n\nclass       : SpatialPoints \nfeatures    : 1545 \nextent      : 11203.01, 45404.24, 25667.6, 49300.88  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \n\nsg_sp\n\nclass       : SpatialPolygons \nfeatures    : 60 \nextent      : 2663.926, 56047.79, 16357.98, 50244.03  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \n\n\nLet’s further inspect the differences between Spatial* classes and generic sp object with the example of childcare and childcare_sp object.\n\nhead(childcare)\n\n   Name\n1 kml_1\n2 kml_2\n3 kml_3\n4 kml_4\n5 kml_5\n6 kml_6\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Description\n1 &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;760742&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;742, YISHUN AVENUE 5, #01 - 470, SINGAPORE 760742&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Child Care Services&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;AVERBEL CHILD DEVELOPMENT CENTRE PTE LTD&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;AEA27114446235CE&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20200826094036&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;\n2                                    &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;159053&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;20, LENGKOK BAHRU, #02 - 05, SINGAPORE 159053&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Child Care Services&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;AWWA LTD.&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;86B24416FB1663C6&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20200826094036&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;\n3        &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;556912&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;22, LI HWAN VIEW, GOLDEN HILL ESTATE, SINGAPORE 556912&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Child Care Services&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;BABIES BY-THE-PARK PTE. LTD.&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;F971CBBA973E1AE5&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20200826094036&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;\n4 &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;569139&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;3, ANG MO KIO STREET 62, #01 - 36, LINK@AMK, SINGAPORE 569139&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Child Care Services&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;Baby Elk Infant Care Pte Ltd&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;86A4F25D1C7C9D85&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20200826094036&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;\n5                           &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;467961&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;22A, KEW DRIVE, SINGAPORE 467961&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Child Care Services&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;BABYPLANET MONTESSORI PTE. LTD.&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;CFE3F056F8171C7B&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20200826094036&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;\n6                       &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;598523&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;3 Jalan Kakatua, JURONG PARK, SINGAPORE 598523&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Child Care Services&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;BAMBINI CHILDCARE LLP&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;2B4F0B285ED28C4A&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20200826094036&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;\n\n\n\nhead(childcare_sp)\n\nclass       : SpatialPoints \nfeatures    : 1 \nextent      : 27976.73, 27976.73, 45716.7, 45716.7  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \n\n\nNote that the Spatial* classes contain more attribute data as compared its generic sp object counterpart.\n\n\n\n\n\n\nTip\n\n\n\nDifferences between Spatial* classes and generic sp object\n\nData Storage: SpatialPoints stores only the coordinates, while SpatialPointsDataFrame stores both coordinates and additional attribute data\nFunctionality: SpatialPointsDataFrame allows for more complex operations and analyses due to the additional data it holds\n\nsee Introduction to spatial points in R - Michael T. Hallworth, Ph.D.\n\n\n\n\n9.3 Converting the Generic sp Format into spatstat’s ppp Format\nNow, we will use as.ppp() function of spatstat to convert the spatial data into spatstat’s ppp object format.\n\nchildcare_ppp &lt;- as.ppp(childcare_sf)\nchildcare_ppp\n\nMarked planar point pattern: 1545 points\nmarks are of storage type  'character'\nwindow: rectangle = [11203.01, 45404.24] x [25667.6, 49300.88] units\n\n\nLet’s examine the difference by plotting childcare_ppp:\n\nplot(childcare_ppp)\n\n\n\n\n\n\n\n\nWe can also view the summary statistics of the newly created ppp object by using the code block below.\n\nsummary(childcare_ppp)\n\nMarked planar point pattern:  1545 points\nAverage intensity 1.91145e-06 points per square unit\n\nCoordinates are given to 11 decimal places\n\nmarks are of type 'character'\nSummary:\n   Length     Class      Mode \n     1545 character character \n\nWindow: rectangle = [11203.01, 45404.24] x [25667.6, 49300.88] units\n                    (34200 x 23630 units)\nWindow area = 808287000 square units\n\n\n\n\n\n\n\n\nTip\n\n\n\nBe aware of the warning message regarding duplicates. In spatial point pattern analysis, duplicates can be a significant issue. The statistical methods used for analyzing spatial point patterns often assume that the points are distinct and non-coincident.\n\n\n\n\n9.4 Handling Duplicated Points\nWe can check the duplication in a ppp object by using the duplicated function with different configurations.\n\n\n\n\n\n\nTip\n\n\n\nThe duplicated function has an argument rule:\n\nDefault Behavior (rule = \"spatstat\"):\n\nPoints are considered identical if both their coordinates (like x and y positions) and their marks (additional data or labels attached to the points) are exactly the same.\nThis is the strictest check, requiring everything about the points to match.\n\nOnly Checking Coordinates (rule = \"unmark\"):\n\nPoints are considered duplicates if their coordinates are the same, regardless of their marks.\nMarks are ignored, so only the positions are compared.\n\nUsing deldir Package (rule = \"deldir\"):\n\nPoints are considered duplicates based on their coordinates, but the comparison is done using a specific method (duplicatedxy) from the deldir package.\nThis approach ensures the check is consistent with other functions in the deldir package, which is often used for spatial data analysis (like creating Delaunay triangulations).\n\n\nIn other words,\n\nrule = \"spatstat\": Strict check (coordinates and marks).\nrule = \"unmark\": Less strict (coordinates only).\nrule = \"deldir\": Coordinate check, consistent with the deldir package methods.\n\nsee R: Determine Duplicated Points in a Spatial Point Pattern\n\n\n\n# duplicated(childcare_ppp)\n# any(duplicated(childcare_ppp))\nrules &lt;- c(\"spatstat\", \"deldir\", \"unmark\")\n\nduplicate_counts &lt;- list()\nfor (rule in rules) {\n  duplicates &lt;- duplicated(childcare_ppp, rule = rule)\n  num_duplicates &lt;- sum(duplicates)\n  duplicate_counts[[rule]] &lt;- num_duplicates\n}\n\nprint(duplicate_counts)\n\n$spatstat\n[1] 0\n\n$deldir\n[1] 0\n\n$unmark\n[1] 74\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that this behavior happens because the data contains marked points with the same coordinates but different properties.\nUpon manual inspection, a set of example is “39, WOODLANDS CLOSE, #01 - 62, MEGA@WOODLANDS, SINGAPORE 737856” and “39, WOODLANDS CLOSE, #01 - 59, MEGA@WOODLANDS, SINGAPORE 737856”.\nThese are 2 childcare centres that resides in the same building. Thus, it can only be picked up using the “unmark” rule which only examine for exact match of the point coordinate only.\n\n\nTo count the number of coincident points, we will use the multiplicity() function as shown in the code block below. see R: Multiplicity for more info.\n\nmultiplicity(childcare_ppp)\n\nIf we want to know how many locations have more than one point event:\n\nsum(multiplicity(childcare_ppp) &gt; 1)\n\n[1] 0\n\n\n\n# double check\ncoincident_points &lt;- duplicated(childcare_ppp,  rule=\"unmark\")\ncoincident_coordinates &lt;- childcare_ppp[coincident_points]\nprint(coincident_coordinates)\n\nMarked planar point pattern: 74 points\nmarks are of storage type  'character'\nwindow: rectangle = [11203.01, 45404.24] x [25667.6, 49300.88] units\n\n\nThe output shows that there are 74 duplicated point events.\n\n\n9.5 How to Spot Duplicate Points on the Map\nThere are three ways to overcome this problem. The easiest way is to delete the duplicates. But, that will also mean that some useful point events will be lost.\nThe second solution is use jittering, which will add a small perturbation to the duplicate points so that they do not occupy the exact same space.\nThe third solution is to make each point “unique” and then attach the duplicates of the points to the patterns as marks, as attributes of the points. Then you would need analytical techniques that take into account these marks.\n\n9.5.1 Jittering\nThe code block below implements the jittering approach.\n\nchildcare_ppp_jit &lt;- rjitter(childcare_ppp,\n                             retry=TRUE,\n                             nsim=1,\n                             drop=TRUE)\n\nplot(childcare_ppp_jit, pch = 16, cex = 0.5, main = \"Jittered Points\")\n\n\n\n\n\n\n\n\n\nany(duplicated(childcare_ppp_jit))\n\n[1] FALSE\n\n\n\n\n\n9.6 Creating owin Object\nWhen analysing spatial point patterns, it is a good practice to confine the analysis with a geographical area like Singapore boundary. In spatstat, an object called owin is specially designed to represent this polygonal region.\nThe code block below is used to covert sg SpatialPolygon object into owin object of spatstat.\n\nsg_owin &lt;- as.owin(sg_sf)\n\nThe output object can be displayed by using plot() function:\n\nplot(sg_owin)\n\n\n\n\n\n\n\n\nAnd using summary() function of Base R:\n\nsummary(sg_owin)\n\nWindow: polygonal boundary\n50 separate polygons (1 hole)\n                 vertices         area relative.area\npolygon 1 (hole)       30     -7081.18     -9.76e-06\npolygon 2              55     82537.90      1.14e-04\npolygon 3              90    415092.00      5.72e-04\npolygon 4              49     16698.60      2.30e-05\npolygon 5              38     24249.20      3.34e-05\npolygon 6             976  23344700.00      3.22e-02\npolygon 7             721   1927950.00      2.66e-03\npolygon 8            1992   9992170.00      1.38e-02\npolygon 9             330   1118960.00      1.54e-03\npolygon 10            175    925904.00      1.28e-03\npolygon 11            115    928394.00      1.28e-03\npolygon 12             24      6352.39      8.76e-06\npolygon 13            190    202489.00      2.79e-04\npolygon 14             37     10170.50      1.40e-05\npolygon 15             25     16622.70      2.29e-05\npolygon 16             10      2145.07      2.96e-06\npolygon 17             66     16184.10      2.23e-05\npolygon 18           5195 636837000.00      8.78e-01\npolygon 19             76    312332.00      4.31e-04\npolygon 20            627  31891300.00      4.40e-02\npolygon 21             20     32842.00      4.53e-05\npolygon 22             42     55831.70      7.70e-05\npolygon 23             67   1313540.00      1.81e-03\npolygon 24            734   4690930.00      6.47e-03\npolygon 25             16      3194.60      4.40e-06\npolygon 26             15      4872.96      6.72e-06\npolygon 27             15      4464.20      6.15e-06\npolygon 28             14      5466.74      7.54e-06\npolygon 29             37      5261.94      7.25e-06\npolygon 30            111    662927.00      9.14e-04\npolygon 31             69     56313.40      7.76e-05\npolygon 32            143    145139.00      2.00e-04\npolygon 33            397   2488210.00      3.43e-03\npolygon 34             90    115991.00      1.60e-04\npolygon 35             98     62682.90      8.64e-05\npolygon 36            165    338736.00      4.67e-04\npolygon 37            130     94046.50      1.30e-04\npolygon 38             93    430642.00      5.94e-04\npolygon 39             16      2010.46      2.77e-06\npolygon 40            415   3253840.00      4.49e-03\npolygon 41             30     10838.20      1.49e-05\npolygon 42             53     34400.30      4.74e-05\npolygon 43             26      8347.58      1.15e-05\npolygon 44             74     58223.40      8.03e-05\npolygon 45            327   2169210.00      2.99e-03\npolygon 46            177    467446.00      6.44e-04\npolygon 47             46    699702.00      9.65e-04\npolygon 48              6     16841.00      2.32e-05\npolygon 49             13     70087.30      9.66e-05\npolygon 50              4      9459.63      1.30e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 725376000 square units\nFraction of frame area: 0.401"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02a.html#combining-point-events-object-and-owin-object",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02a.html#combining-point-events-object-and-owin-object",
    "title": "2A: 1st Order Spatial Point Patterns Analysis",
    "section": "10 Combining Point Events Object and Owin Object",
    "text": "10 Combining Point Events Object and Owin Object\nFor the last step of geospatial data wrangling, we will extract childcare events that are located within Singapore by using the code block below.\n\n\n\n\n\n\nImportant\n\n\n\nSince the dataset contains duplicated points, we will use the jittered ppp object for downstream analysis.\n\n\n\nchildcareSG_ppp &lt;- childcare_ppp_jit[sg_owin]\n\nThe output object combined both the point and polygon feature in one ppp object class as shown below.\n\nsummary(childcareSG_ppp)\n\nMarked planar point pattern:  1545 points\nAverage intensity 2.129929e-06 points per square unit\n\nCoordinates are given to 11 decimal places\n\nmarks are of type 'character'\nSummary:\n   Length     Class      Mode \n     1545 character character \n\nWindow: polygonal boundary\n50 separate polygons (1 hole)\n                 vertices         area relative.area\npolygon 1 (hole)       30     -7081.18     -9.76e-06\npolygon 2              55     82537.90      1.14e-04\npolygon 3              90    415092.00      5.72e-04\npolygon 4              49     16698.60      2.30e-05\npolygon 5              38     24249.20      3.34e-05\npolygon 6             976  23344700.00      3.22e-02\npolygon 7             721   1927950.00      2.66e-03\npolygon 8            1992   9992170.00      1.38e-02\npolygon 9             330   1118960.00      1.54e-03\npolygon 10            175    925904.00      1.28e-03\npolygon 11            115    928394.00      1.28e-03\npolygon 12             24      6352.39      8.76e-06\npolygon 13            190    202489.00      2.79e-04\npolygon 14             37     10170.50      1.40e-05\npolygon 15             25     16622.70      2.29e-05\npolygon 16             10      2145.07      2.96e-06\npolygon 17             66     16184.10      2.23e-05\npolygon 18           5195 636837000.00      8.78e-01\npolygon 19             76    312332.00      4.31e-04\npolygon 20            627  31891300.00      4.40e-02\npolygon 21             20     32842.00      4.53e-05\npolygon 22             42     55831.70      7.70e-05\npolygon 23             67   1313540.00      1.81e-03\npolygon 24            734   4690930.00      6.47e-03\npolygon 25             16      3194.60      4.40e-06\npolygon 26             15      4872.96      6.72e-06\npolygon 27             15      4464.20      6.15e-06\npolygon 28             14      5466.74      7.54e-06\npolygon 29             37      5261.94      7.25e-06\npolygon 30            111    662927.00      9.14e-04\npolygon 31             69     56313.40      7.76e-05\npolygon 32            143    145139.00      2.00e-04\npolygon 33            397   2488210.00      3.43e-03\npolygon 34             90    115991.00      1.60e-04\npolygon 35             98     62682.90      8.64e-05\npolygon 36            165    338736.00      4.67e-04\npolygon 37            130     94046.50      1.30e-04\npolygon 38             93    430642.00      5.94e-04\npolygon 39             16      2010.46      2.77e-06\npolygon 40            415   3253840.00      4.49e-03\npolygon 41             30     10838.20      1.49e-05\npolygon 42             53     34400.30      4.74e-05\npolygon 43             26      8347.58      1.15e-05\npolygon 44             74     58223.40      8.03e-05\npolygon 45            327   2169210.00      2.99e-03\npolygon 46            177    467446.00      6.44e-04\npolygon 47             46    699702.00      9.65e-04\npolygon 48              6     16841.00      2.32e-05\npolygon 49             13     70087.30      9.66e-05\npolygon 50              4      9459.63      1.30e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 725376000 square units\nFraction of frame area: 0.401\n\n\n\nplot(childcareSG_ppp)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02a.html#first-order-spatial-point-patterns-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02a.html#first-order-spatial-point-patterns-analysis",
    "title": "2A: 1st Order Spatial Point Patterns Analysis",
    "section": "11 First-order Spatial Point Patterns Analysis",
    "text": "11 First-order Spatial Point Patterns Analysis\nIn this section, you will learn how to perform first-order SPPA by using spatstat package. The hands-on exercise will focus on:\n\nderiving kernel density estimation (KDE) layer for visualising and exploring the intensity of point processes,\nperforming Confirmatory Spatial Point Patterns Analysis by using Nearest Neighbour statistics.\n\n\n11.1 Kernel Density Estimation\nIn this section, you will learn how to compute the kernel density estimation (KDE) of childcare services in Singapore.\n\n11.1.1 Computing Kernel Density Estimation Using Automatic Bandwidth Selection Method\n\nbw.diggle() automatic bandwidth selection method. Other recommended methods are bw.CvL(), bw.scott() or bw.ppl().\n\nThe smoothing kernel used is gaussian, which is the default. Other smoothing methods are: “epanechnikov”, “quartic” or “disc”.\n\nThe intensity estimate is corrected for edge effect bias by using method described by Jones (1993) and Diggle (2010, equation 18.9). The default is FALSE.\n\n\nkde_childcareSG_bw &lt;- density(childcareSG_ppp,\n                              sigma=bw.diggle,\n                              edge=TRUE,\n                            kernel=\"gaussian\")\n\nThe plot() function of Base R is then used to display the kernel density derived.\n\nplot(kde_childcareSG_bw)\n\n\n\n\n\n\n\n\nThe density values of the output range from 0 to 0.000035 which is way too small to comprehend. This is because the default unit of measurement of SVY21 is in meter. As a result, the density values computed is in “number of points per square meter”.\nBefore we move on to next section, it is good to know that you can retrieve the bandwidth used to compute the kde layer by using the code block below.\n\nbw &lt;- bw.diggle(childcareSG_ppp)\nbw\n\n   sigma \n414.4576 \n\n\n\n\n11.1.2 Rescaling KDE values\nrescale.ppp() is used below to convert the unit of measurement from meter to kilometer:\n\nchildcareSG_ppp.km &lt;- rescale.ppp(childcareSG_ppp, 1000, \"km\")\n\nNow, we can re-run density() using the resale data set and plot the output kde map.\n\nkde_childcareSG.bw &lt;- density(childcareSG_ppp.km, sigma=bw.diggle, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG.bw)\n\n\n\n\n\n\n\n\nSince we just did a rescaling operation, the output image looks identical to the earlier version with the only changes in terms of data values.\n\n\n\n11.2 Working with Different Automatic Bandwidth Methods\nBeside bw.diggle(), there are 3 other spatstat functions can be used to determine the bandwidth, they are: bw.CvL(), bw.scott(), and bw.ppl().\nLet us take a look at the bandwidth return by these automatic bandwidth calculation methods by using:\n\n bw.CvL(childcareSG_ppp.km)\n\n   sigma \n5.009037 \n\n\n\nbw.scott(childcareSG_ppp.km)\n\n sigma.x  sigma.y \n2.224958 1.451103 \n\n\n\nbw.ppl(childcareSG_ppp.km)\n\n   sigma \n0.273598 \n\n\n\nbw.diggle(childcareSG_ppp.km)\n\n    sigma \n0.4144576 \n\n\nBaddeley et al. (2016) suggest using the bw.ppl() algorithm, as it tends to produce more appropriate values when the pattern consists predominantly of tight clusters. However, they also note that if the aim of a study is to detect a single tight cluster amidst random noise, the bw.diggle() method is likely to be more effective.\nTo compare the output of using bw.diggle and bw.ppl methods:\n\nkde_childcareSG.ppl &lt;- density(childcareSG_ppp.km,\n                               sigma=bw.ppl,\n                               edge=TRUE,\n                               kernel=\"gaussian\")\npar(mfrow=c(1,2))\n\nplot(kde_childcareSG.bw, main = \"bw.diggle\")\nplot(kde_childcareSG.ppl, main = \"bw.ppl\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02a.html#working-with-different-kernel-methods",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02a.html#working-with-different-kernel-methods",
    "title": "2A: 1st Order Spatial Point Patterns Analysis",
    "section": "12 6.3 Working with Different Kernel Methods",
    "text": "12 6.3 Working with Different Kernel Methods\nBy default, the kernel method used in density.ppp() is gaussian. But there are three other options, namely: Epanechnikov, Quartic and Dics. Let us take a look at what they look like:\n\npar(mfrow=c(2,2))\nplot(density(childcareSG_ppp.km,\n             sigma=bw.ppl,\n             edge=TRUE,\n             kernel=\"gaussian\"),\n     main=\"Gaussian\")\n\nplot(density(childcareSG_ppp.km,\n             sigma=bw.ppl,\n             edge=TRUE,\n             kernel=\"epanechnikov\"),\n     main=\"Epanechnikov\")\n\nplot(density(childcareSG_ppp.km,\n             sigma=bw.ppl,\n             edge=TRUE,\n             kernel=\"quartic\"),\n     main=\"Quartic\")\n\nplot(density(childcareSG_ppp.km,\n             sigma=bw.ppl,\n             edge=TRUE,\n             kernel=\"disc\"),\n     main=\"Disc\")\n\n\n\n\n\n\n\n\nObservations: In this dataset, the choice of kernel function has only a minor impact on the overall density plots. The Gaussian, Epanechnikov, and Quartic kernels produce smoother transitions and distribute the density over a broader area. In contrast, the Disc kernel provides a more localized density estimation with sharper boundaries and less smoothness."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02a.html#fixed-and-adaptive-kde",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02a.html#fixed-and-adaptive-kde",
    "title": "2A: 1st Order Spatial Point Patterns Analysis",
    "section": "13 Fixed and Adaptive KDE",
    "text": "13 Fixed and Adaptive KDE\n\n13.1 Computing KDE by using Fixed Bandwidth\nNext, we will compute a KDE layer by defining a bandwidth of 600 meter. Notice that in the code block below, the sigma value used is 0.6. This is because the unit of measurement of childcareSG_ppp.km object is in kilometer, hence the 600m is 0.6km.\nIn this section, we will learn how to derive adaptive kernel density estimation by using density.adaptive() of spatstat.\n\nkde_childcareSG_adaptive &lt;- adaptive.density(childcareSG_ppp.km, method=\"kernel\")\nplot(kde_childcareSG_adaptive)\n\n\n\n\n\n\n\n\nWe can compare the fixed and adaptive kernel density estimation outputs by using:\n\npar(mfrow=c(1,2))\n\nplot(kde_childcareSG.bw, main = \"Fixed bandwidth\")\nplot(kde_childcareSG_adaptive, main = \"Adaptive bandwidth\")\n\n\n\n\n\n\n\n\n\n\n13.2 Converting KDE Output into Grid Object\nTo achieve the same result, we convert the object to a format suitable for mapping:\n\ngridded_kde_childcareSG_bw &lt;- as.SpatialGridDataFrame.im(kde_childcareSG.bw)\nspplot(gridded_kde_childcareSG_bw)\n\n\n13.2.1 Converting Grided Output into Raster\nNext, we will convert the gridded kernel density objects into RasterLayer object by using raster() of raster package.\n\nkde_childcareSG_bw_raster &lt;- raster(kde_childcareSG.bw)\n\nLet us take a look at the properties of kde_childcareSG_bw_raster RasterLayer.\n\nkde_childcareSG_bw_raster\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 0.4170614, 0.2647348  (x, y)\nextent     : 2.663926, 56.04779, 16.35798, 50.24403  (xmin, xmax, ymin, ymax)\ncrs        : NA \nsource     : memory\nnames      : layer \nvalues     : -1.006362e-14, 21.11878  (min, max)\n\n\nNote that the crs property is NA.\n\n\n13.2.2 Assigning Projection Systems\nTo include the CRS information on kde_childcareSG_bw_raster RasterLayer, we will do the following:\n\nprojection(kde_childcareSG_bw_raster) &lt;- CRS(\"+init=EPSG:3414\")\nkde_childcareSG_bw_raster\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 0.4170614, 0.2647348  (x, y)\nextent     : 2.663926, 56.04779, 16.35798, 50.24403  (xmin, xmax, ymin, ymax)\ncrs        : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +units=m +no_defs \nsource     : memory\nnames      : layer \nvalues     : -1.006362e-14, 21.11878  (min, max)\n\n\nNow, the crs property is completed.\n\n\n\n13.3 Visualising the output in tmap\nFinally, we will display the raster in cartographic quality map using tmap package.\n\ntm_shape(kde_childcareSG_bw_raster) +\n  tm_raster(\"layer\", palette = \"viridis\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\"), frame = FALSE)\n\n\n\n\n\n\n\n\n\nvalues(kde_childcareSG_bw_raster)\n\nNote that the raster values are encoded explicitly onto the raster pixel using the values in “v” field.\n\n\n13.4 Comparing Spatial Point Patterns using KDE\nIn this section, we will learn how to compare KDE of childcare at Punggol, Tampines, Chua Chu Kang and Jurong West planning areas.\n\n13.4.1 Extracting Study Area\nThe code block below will be used to extract the target planning areas.\n\npg &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"PUNGGOL\")\ntm &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"TAMPINES\")\nck &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"CHOA CHU KANG\")\njw &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"JURONG WEST\")\n\nPlotting the target planning areas:\n\npar(mfrow=c(2,2))\nplot(pg, main = \"Punggol\")\n\n\n\n\n\n\n\nplot(tm, main = \"Tampines\")\n\n\n\n\n\n\n\nplot(ck, main = \"Choa Chu Kang\")\n\n\n\n\n\n\n\nplot(jw, main = \"Jurong West\")\n\n\n\n\n\n\n\n\n\n\n13.4.2 Creating owin object\nNow, we will convert these sf objects into owin objects that is required by spatstat.\n\npg_owin = as.owin(pg)\ntm_owin = as.owin(tm)\nck_owin = as.owin(ck)\njw_owin = as.owin(jw)\n\n\n\n13.4.3 Combining Childcare Points and the Study Area\nTo extract childcare that is within the specific region for analysis, we can use:\n\nchildcare_pg_ppp = childcare_ppp_jit[pg_owin]\nchildcare_tm_ppp = childcare_ppp_jit[tm_owin]\nchildcare_ck_ppp = childcare_ppp_jit[ck_owin]\nchildcare_jw_ppp = childcare_ppp_jit[jw_owin]\n\nNext, rescale.ppp() function is used to transform the unit of measurement from meter to kilometer.\n\nchildcare_pg_ppp.km = rescale.ppp(childcare_pg_ppp, 1000, \"km\")\nchildcare_tm_ppp.km = rescale.ppp(childcare_tm_ppp, 1000, \"km\")\nchildcare_ck_ppp.km = rescale.ppp(childcare_ck_ppp, 1000, \"km\")\nchildcare_jw_ppp.km = rescale.ppp(childcare_jw_ppp, 1000, \"km\")\n\nThe code block below is used to plot the four study areas and the locations of the childcare centres.\n\npar(mfrow=c(2,2))\n\nplot(childcare_pg_ppp.km, main=\"Punggol\")\nplot(childcare_tm_ppp.km, main=\"Tampines\")\nplot(childcare_ck_ppp.km, main=\"Choa Chu Kang\")\nplot(childcare_jw_ppp.km, main=\"Jurong West\")\n\n\n\n\n\n\n\n\n\n\n13.4.4 Computing KDE\nThe code chunk below will be used to compute the KDE of these four planning area. bw.diggle method is used to derive the bandwidth of each area.\n\npar(mfrow=c(2,2))\nplot(density(childcare_pg_ppp.km,\n             sigma=bw.diggle,\n             edge=TRUE,\n             kernel=\"gaussian\"),\n     main=\"Punggol\")\nplot(density(childcare_tm_ppp.km,\n             sigma=bw.diggle,\n             edge=TRUE,\n             kernel=\"gaussian\"),\n     main=\"Tampines\")\nplot(density(childcare_ck_ppp.km,\n             sigma=bw.diggle,\n             edge=TRUE,\n             kernel=\"gaussian\"),\n     main=\"Choa Chu Kang\")\nplot(density(childcare_jw_ppp.km,\n             sigma=bw.diggle,\n             edge=TRUE,\n             kernel=\"gaussian\"),\n     main=\"Jurong West\")\n\n\n\n\n\n\n\n\n\n\n13.4.5 Computing Fixed Bandwidth KDE\nWe will set the bandwidth as 250m for comparison purposes.\n\npar(mfrow=c(2,2))\n\nplot(density(childcare_ck_ppp.km,\n             sigma=0.25,\n             edge=TRUE,\n             kernel=\"gaussian\"),\n     main=\"Chou Chu Kang\")\n\nplot(density(childcare_jw_ppp.km,\n             sigma=0.25,\n             edge=TRUE,\n             kernel=\"gaussian\"),\n     main=\"Jurong West\")\n\nplot(density(childcare_pg_ppp.km,\n             sigma=0.25,\n             edge=TRUE,\n             kernel=\"gaussian\"),\n     main=\"Punggol\")\n\nplot(density(childcare_tm_ppp.km,\n             sigma=0.25,\n             edge=TRUE,\n             kernel=\"gaussian\"),\n     main=\"Tampines\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02a.html#nearest-neighbour-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02a.html#nearest-neighbour-analysis",
    "title": "2A: 1st Order Spatial Point Patterns Analysis",
    "section": "14 Nearest Neighbour Analysis",
    "text": "14 Nearest Neighbour Analysis\nIn this section, we will perform the Clark-Evans test of aggregation for a spatial point pattern by using clarkevans.test() of statspat.\n\n\n\n\n\n\nNote\n\n\n\nClarks-Evans Test\nThe Clark-Evans test is a statistical test used to determine whether points in a given space are randomly distributed, clustered, or regularly spaced.\nThe Clark-Evans test is a statistical test used to determine whether points in a given space are randomly distributed, clustered, or regularly spaced. It is commonly used in spatial analysis, particularly in nearest neighbor analysis, to understand the pattern of points (such as locations of trees, animals, or buildings) in a study area.\nHow the Clark-Evans Test Works:\n\nThe test calculates the average distance from each point in the dataset to its nearest neighbor.\nThis observed average distance is compared to the expected average distance for a random distribution of points in the same area.\nThe result is a Clark-Evans ratio (R):\n\nR = 1: Points are randomly distributed.\nR &lt; 1: Points are clustered together.\nR &gt; 1: Points are more evenly spaced or regularly distributed.\n\n\nThe alternative argument: The null hypothesis is Complete Spatial Randomness, i.e. a uniform Poisson process. The alternative hypothesis is specified by the argument alternative:\n\nalternative=\"less\" or alternative=\"clustered\": the alternative hypothesis is that corresponding to a clustered point pattern;\nalternative=\"greater\" or alternative=\"regular\": the alternative hypothesis is that corresponding to a regular or ordered point pattern;\nalternative=\"two.sided\": the alternative hypothesis is that corresponding to a clustered or regular pattern.\n\nsee Tests for Spatial Randomness - The R Book [Book]\n\n\n\n14.1 Testing Spatial Point Patterns using Clark and Evans Test\nThe test hypotheses are:\n\\(H_o\\) = The distribution of childcare services are randomly distributed.\n\\(H_1\\) = The distribution of childcare services are not randomly distributed.\nThe 95% confidence interval will be used.\n\nclarkevans.test(childcareSG_ppp,\n                correction=\"none\",\n                clipregion=\"sg_owin\",\n                alternative=c(\"clustered\"),\n                nsim=39)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcareSG_ppp\nR = 0.56703, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n\n\n\n\n\n\n\nNote\n\n\n\nResult Interpretation\n\nGiven that R, the Clark-Evans ratio, is 0.56703, it indicates that the childcare services are clustered rather than randomly distributed. The further R is from 1 (and closer to 0), the stronger the clustering.\nThe p-value is extremely small (much less than the standard significance level of 0.05).\nAt a 95% confidence level, we can reject the null hypothesis of random distribution and accept the alternative hypothesis that the services are clustered.\n\n\n\n\n\n14.2 Clark and Evans Test: Choa Chu Kang Planning Area\nNow, we perform the similar test on individual subzone areas. We will analyse the Choa Chu Kang area first.\n\n\n\n\n\n\nNote\n\n\n\nChoice of nsim: The choice of 39 simulations (nsim = 39) in Monte Carlo techniques for spatial analysis is often a practical compromise between computational efficiency and statistical robustness.\n\nComputational Efficiency: Running a large number of simulations can be computationally expensive, especially for complex spatial analyses. We can strike a balance between obtaining reliable results and keeping computational costs manageable by selecting absolute minimum sample size.\n\nThe minimum number of simulations \\(m\\) required for a Monte Carlo test at a particular significance level can be determined by:\n\\(\\alpha = \\frac{1}{m+1}\\)\nfor a one-tailed test and\n\\(\\alpha = \\frac{2}{m+1}\\)\nThus, a one-tailed test at a significance level of 5% would require a minimum of 19 simulations, a two-tailed test at a significance level of 5% would require a minimum of 39 simulations.\nsee Going beyond the required number of simulations required for a particular significance level when conducting a Monte Carlo test - Cross Validated, spatial statistics - Simulation envelopes and significance levels - Geographic Information Systems Stack Exchange\n\n\n\nclarkevans.test(childcare_ck_ppp,\n                correction=\"none\",\n                clipregion=NULL,\n                alternative=c(\"two.sided\"),\n                nsim=39)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcare_ck_ppp\nR = 0.95041, p-value = 0.4587\nalternative hypothesis: two-sided\n\n\n\n\n\n\n\n\nNote\n\n\n\nResult Interpretation\n\nGiven that R, the Clark-Evans ratio, is 0.95041, it indicates that the distribution of childcare services is closer to random with minimal clustering. Since R is close to 1 and not significantly less, there is little evidence of clustering or regular spacing.\n\nThe p-value is 0.4587, which is much larger than the standard significance level of 0.05.\nAt a 95% confidence level, we fail to reject the null hypothesis of random distribution. The two-sided alternative hypothesis suggests that the distribution could be either clustered or regularly spaced, but the data does not provide enough evidence to support significant clustering or regularity.\n\n\n\n\n14.3 Clark and Evans Test: Tampines Planning Area\nNext, we will analyse the spatial point patterns of childcare centre in Tampines planning area using the Clark-Evans test\n\nclarkevans.test(childcare_tm_ppp,\n                correction=\"none\",\n                clipregion=NULL,\n                alternative=c(\"two.sided\"),\n                nsim=39)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcare_tm_ppp\nR = 0.80393, p-value = 0.0004022\nalternative hypothesis: two-sided\n\n\n\n\n\n\n\n\nNote\n\n\n\nResult Interpretation\nGiven that R, the Clark-Evans ratio, is 0.80393, it indicates that the distribution of childcare services shows a significant level of clustering. Since R is notably less than 1, there is strong evidence of clustering.\nThe p-value is 0.0004022, which is much smaller than the standard significance level of 0.05.\nAt a 95% confidence level, we reject the null hypothesis of random distribution and accept the alternative hypothesis that the distribution is not random. The two-sided alternative hypothesis suggests that the distribution could be either clustered or regularly spaced, but in this case, the significantly lower R value supports the conclusion of clustering."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html",
    "title": "4A: Spatial Weights and Applications",
    "section": "",
    "text": "R for Geospatial Data Science and Analytics - 8  Spatial Weights and Applications"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#exercise-4a-reference",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#exercise-4a-reference",
    "title": "4A: Spatial Weights and Applications",
    "section": "",
    "text": "R for Geospatial Data Science and Analytics - 8  Spatial Weights and Applications"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#overview",
    "title": "4A: Spatial Weights and Applications",
    "section": "2 Overview",
    "text": "2 Overview\nIn this exercise, we will learn to compute spatial weights, visualize spatial distributions, and create spatially lagged variables using various functions from R packages such as sf, spdep, and tmap."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#learning-outcome",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#learning-outcome",
    "title": "4A: Spatial Weights and Applications",
    "section": "3 Learning Outcome",
    "text": "3 Learning Outcome\n\nImport geospatial data using functions from the sf package.\nImport CSV data using functions from the readr package.\nPerform relational joins using functions from the dplyr package.\nCompute spatial weights with functions from the spdep package.\nCalculate spatially lagged variables using functions from the spdep package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#the-data",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#the-data",
    "title": "4A: Spatial Weights and Applications",
    "section": "4 The Data",
    "text": "4 The Data\nThe following 2 datasets will be used in this exercise.\n\n\n\n\n\n\n\n\nData Set\nDescription\nFormat\n\n\n\n\nHunan county boundary layer\nGeospatial data set representing the county boundaries of Hunan\nESRI Shapefile\n\n\nHunan_2012.csv\nContains selected local development indicators for Hunan in 2012\nCSV"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#installing-and-loading-the-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#installing-and-loading-the-r-packages",
    "title": "4A: Spatial Weights and Applications",
    "section": "5 Installing and Loading the R Packages",
    "text": "5 Installing and Loading the R Packages\nThe following R packages will be used in this exercise:\n\n\n\n\n\n\n\n\nPackage\nPurpose\nUse Case in Exercise\n\n\n\n\nsf\nImports, manages, and processes vector-based geospatial data.\nHandling vector geospatial data such as the Hunan county boundary layer in shapefile format.\n\n\nspdep\nProvides functions for spatial dependence analysis, including spatial weights and spatial autocorrelation.\nComputing spatial weights and creating spatially lagged variables.\n\n\ntmap\nCreates static and interactive thematic maps using cartographic quality elements.\nVisualizing regional development indicators and plotting maps showing spatial relationships and patterns.\n\n\ntidyverse\nA collection of packages for data science tasks such as data manipulation, visualization, and modeling.\nImporting CSV files, wrangling data, and performing relational joins.\n\n\nknitr\nEnables dynamic report generation and integration of R code with documents.\nFormatting output, creating tables, and generating reports for the exercise.\n\n\n\nTo install and load these packages, use the following code:\n\npacman::p_load(sf, spdep, tmap, tidyverse, knitr)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#import-data-and-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#import-data-and-preparation",
    "title": "4A: Spatial Weights and Applications",
    "section": "6 Import Data and Preparation",
    "text": "6 Import Data and Preparation\nIn this section, we will perform 3 necessary steps to prepare the data for analysis.\n\n6.1 Import Geospatial Shapefile\nFirstly, we will use st_read() of sf package to import Hunan shapefile into R. The imported shapefile will be simple features Object of sf.\n\nhunan &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `/Users/walter/code/isss626/isss626-gaa/Hands-on_Ex/Hands-on_Ex04/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\ndim(hunan)\n\n[1] 88  8\n\n\n\n\n6.2 Import Aspatial csv File\nNext, we will import Hunan_2012.csv into R by using read_csv() of readr package. The output is R dataframe class.\n\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\n\ndim(hunan2012)\n\n[1] 88 29\n\n\n\n\n6.3 Perform Relational Join\nBefore we perform relational join, let’s observe the columns in each dataset and only select the columns that we need.\n\nhunan columns:\n\n\nprint(colnames(hunan))\n\n[1] \"NAME_2\"     \"ID_3\"       \"NAME_3\"     \"ENGTYPE_3\"  \"Shape_Leng\"\n[6] \"Shape_Area\" \"County\"     \"geometry\"  \n\n\n\nhunan2012 columns:\n\n\nprint(colnames(hunan2012))\n\n [1] \"County\"      \"City\"        \"avg_wage\"    \"deposite\"    \"FAI\"        \n [6] \"Gov_Rev\"     \"Gov_Exp\"     \"GDP\"         \"GDPPC\"       \"GIO\"        \n[11] \"Loan\"        \"NIPCR\"       \"Bed\"         \"Emp\"         \"EmpR\"       \n[16] \"EmpRT\"       \"Pri_Stu\"     \"Sec_Stu\"     \"Household\"   \"Household_R\"\n[21] \"NOIP\"        \"Pop_R\"       \"RSCG\"        \"Pop_T\"       \"Agri\"       \n[26] \"Service\"     \"Disp_Inc\"    \"RORP\"        \"ROREmp\"     \n\n\nAfter merging:\n\nhunan_joined &lt;- left_join(hunan,hunan2012)\nprint(colnames(hunan_joined))\n\n [1] \"NAME_2\"      \"ID_3\"        \"NAME_3\"      \"ENGTYPE_3\"   \"Shape_Leng\" \n [6] \"Shape_Area\"  \"County\"      \"City\"        \"avg_wage\"    \"deposite\"   \n[11] \"FAI\"         \"Gov_Rev\"     \"Gov_Exp\"     \"GDP\"         \"GDPPC\"      \n[16] \"GIO\"         \"Loan\"        \"NIPCR\"       \"Bed\"         \"Emp\"        \n[21] \"EmpR\"        \"EmpRT\"       \"Pri_Stu\"     \"Sec_Stu\"     \"Household\"  \n[26] \"Household_R\" \"NOIP\"        \"Pop_R\"       \"RSCG\"        \"Pop_T\"      \n[31] \"Agri\"        \"Service\"     \"Disp_Inc\"    \"RORP\"        \"ROREmp\"     \n[36] \"geometry\"   \n\n\nThen only select the columns that we need:\n\nhunan &lt;- hunan_joined %&gt;%\n  select(1:4, 7, 15)\n\nprint(colnames(hunan))\n\n[1] \"NAME_2\"    \"ID_3\"      \"NAME_3\"    \"ENGTYPE_3\" \"County\"    \"GDPPC\"    \n[7] \"geometry\" \n\n\nIn the code above, we use the left_join() function to merge the hunan SpatialPolygonsDataFrame with the hunan2012 dataframe. The join is based on the column named County, which is common to both datasets. This allows us to match rows by their corresponding counties.\nAfter the join, the select() function is used to retain a subset of columns from the merged dataset. We can briefly observe the joined output below.\n\nhead(hunan)\n\nSimple feature collection with 6 features and 6 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 110.4922 ymin: 28.61762 xmax: 112.3013 ymax: 30.12812\nGeodetic CRS:  WGS 84\n   NAME_2  ID_3  NAME_3   ENGTYPE_3  County GDPPC\n1 Changde 21098 Anxiang      County Anxiang 23667\n2 Changde 21100 Hanshou      County Hanshou 20981\n3 Changde 21101  Jinshi County City  Jinshi 34592\n4 Changde 21102      Li      County      Li 24473\n5 Changde 21103   Linli      County   Linli 25554\n6 Changde 21104  Shimen      County  Shimen 27137\n                        geometry\n1 POLYGON ((112.0625 29.75523...\n2 POLYGON ((112.2288 29.11684...\n3 POLYGON ((111.8927 29.6013,...\n4 POLYGON ((111.3731 29.94649...\n5 POLYGON ((111.6324 29.76288...\n6 POLYGON ((110.8825 30.11675..."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#visualising-regional-development-indicator",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#visualising-regional-development-indicator",
    "title": "4A: Spatial Weights and Applications",
    "section": "7 Visualising Regional Development Indicator",
    "text": "7 Visualising Regional Development Indicator\nTo visualize the regional development indicator, we can prepare a base map and a choropleth map to show the distribution of GDPPC 2012 (GDP per capita) by using qtm() of tmap package.\n\nbasemap &lt;- tm_shape(hunan) +\n  tm_polygons() +\n  tm_text(\"NAME_3\", size=0.5)\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\ntmap_arrange(basemap, gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIntepretation The choropleth map on the right visualizes the distribution of GDP per capita (GDPPC) for the year 2012 across the different counties in Hunan.\nThe counties are shaded in varying colors, ranging from light to dark, to represent different GDP per capita ranges. Darker shades indicate higher GDP per capita values, while lighter shades represent lower values. This visualization helps to identify regional economic disparities and highlights areas with higher or lower economic activity within Hunan province.\nFor example, we can observe that Changsha has the highest GDP per capital values in the Hunan region."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#computing-contiguity-spatial-weights",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#computing-contiguity-spatial-weights",
    "title": "4A: Spatial Weights and Applications",
    "section": "8 Computing Contiguity Spatial Weights",
    "text": "8 Computing Contiguity Spatial Weights\nIn this section, we will use poly2nb() of spdep package to compute contiguity weight matrices for the study area. This function builds a neighbours list based on regions with contiguous boundaries.\n\n\n\n\n\n\nNote\n\n\n\nContiguity means that two spatial units share a common border of non-zero length.\nOperationally, we can further distinguish between a rook and a queen criterion of contiguity, in analogy to the moves allowed for the such-named pieces on a chess board.\nThe rook criterion defines neighbors by the existence of a common edge between two spatial units. The queen criterion is somewhat more encompassing and defines neighbors as spatial units sharing a common edge or a common vertex.\nUsing poly2nb() we can use the queen flag to toggle between queen and rook criteria.\nFor more info, see Chapter 6 Contiguity-Based Spatial Weights | Hands-On Spatial Data Science with R\n\n\n\nThe number of neighbors according to the queen criterion will always be at least as large as for the rook criterion.\n\nFirst, we will compute the Queen contiguity weight matrix.\n\nwm_q &lt;- poly2nb(hunan, queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\n\nwm_r &lt;- poly2nb(hunan, queen=FALSE)\nsummary(wm_r)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 440 \nPercentage nonzero weights: 5.681818 \nAverage number of links: 5 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 \n 2  2 12 20 21 14 11  3  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 10 links\n\n\n\n\n\n\n\n\nNote\n\n\n\nIntepretation of Summary Reports\n\nBoth reports shows that there are 88 area units in Hunan.\nAs expected, the total number of links (neighbor relationships) is slightly higher for the queen criterion (448) than for the rook criterion (440).\nBased on both criteria, the most connected region is Region 85 with 11 links (using Queen criteria) and 10 links (using Rook criteria)\nSimilarly, based on both criteria, the least connected region is Region 30 and 65 with 1 links (using Queen and Rook criteria)\n\n\n\nFor each polygon in the polygon object, wm_q and wm_r lists all neighboring polygons. For example, we can identify the most connected region.\n\ncat(\"The most connected county is\", hunan$County[85])\n\nThe most connected county is Taoyuan\n\n\nTo reveal the county names of the neighboring polygons, we can do the following:\n\nneighbour_counties &lt;- wm_q[[85]]\nprint(neighbour_counties)\n\n [1]  1  2  3  5  6 32 56 57 69 75 78\n\ncat(\"Using Queen's method, the neighbours of \", hunan$County[85],\" is\", hunan$NAME_3[neighbour_counties])\n\nUsing Queen's method, the neighbours of  Taoyuan  is Anxiang Hanshou Jinshi Linli Shimen Yuanling Anhua Nan Cili Sangzhi Taojiang\n\n\nWe can also retrieve the GDPPC of these counties:\n\nhunan$GDPPC[neighbour_counties]\n\n [1] 23667 20981 34592 25554 27137 24194 14567 21311 18714 14624 19509\n\n\nThe printed output above shows that the GDPPC of Taoyuan’s neighbouring counties.\nTo display the complete weight matrix, we can use str().\n\nstr(wm_q)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#visualising-contiguity-weights",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#visualising-contiguity-weights",
    "title": "4A: Spatial Weights and Applications",
    "section": "9 Visualising Contiguity Weights",
    "text": "9 Visualising Contiguity Weights\nTo create a connectivity graph, we need points that represent each polygon, and we’ll draw lines to connect neighboring points. Since we’re working with polygons, we first need to find their central points, called centroids. We’ll calculate these centroids using the sf package before creating the connectivity graph.\nGetting Latitude and Longitude of Polygon Centroids\nTo make the connectivity graph, we must first obtain the points (centroids) for each polygon. This is more than just running st_centroid on our spatial object (us.bound). We need to store the coordinates in a separate data frame.\nWe’ll use a mapping function to achieve this. The mapping function applies a specific function to each element in a vector and returns a vector of the same length. In this case, our input vector will be the geometry column of us.bound, and the function will be st_centroid. We’ll use the map_dbl function from the purrr package to do this.\nFor longitude, we access the first coordinate value using [[1]], and for latitude, we access the second coordinate value using [[2]].\n\nlongitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])\nlatitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\n\nThen, we use cbind() to combine longitude and lattude into the same object.\n\ncoords &lt;- cbind(longitude, latitude)\n\nTo verify that, the data is formatted correctly, we can observe the first few instances.\n\nhead(coords)\n\n     longitude latitude\n[1,]  112.1531 29.44362\n[2,]  112.0372 28.86489\n[3,]  111.8917 29.47107\n[4,]  111.7031 29.74499\n[5,]  111.6138 29.49258\n[6,]  111.0341 29.79863\n\n\nTo plot the continguity-based neighbours map, we can do the following:\n\npar(mfrow=c(1,2))\n\nplot(hunan$geometry, \n     main=\"Queen Contiguity\")\nplot(wm_q, \n     coords, \n     pch = 19, \n     cex = 0.6, \n     add = TRUE, \n     col= \"red\")\n\nplot(hunan$geometry, \n     main=\"Rook Contiguity\")\nplot(wm_r, \n     coords, \n     pch = 19, \n     cex = 0.6, \n     add = TRUE, \n     col = \"red\")\n\n\n\n\n\n\n\n\n ::: callout-note As observed from the previous sections, we understand that more links will be formed with the Queen’s method. This is evident in the plot above.Some of these differences has been marked with blue boxes for better visualization. :::"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#computing-distance-based-neighbours",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#computing-distance-based-neighbours",
    "title": "4A: Spatial Weights and Applications",
    "section": "10 Computing Distance-based Neighbours",
    "text": "10 Computing Distance-based Neighbours\nIn this section, we will create distance-based weight matrices using the dnearneigh() function from the spdep package.\nThis function identifies neighboring region points based on their Euclidean distance. We can specify a range for the distances using the bounds argument, which takes lower (d1=) and upper (d2=) limits.\nIf the coordinates are not projected (i.e., in latitude and longitude) and are specified in the x object or provided as a two-column matrix with longlat=TRUE, the function will calculate great circle distances in kilometers, assuming the WGS84 reference ellipsoid.\n\n10.1 Determine the Cut-off Distance\nFirstly, we need to determine the upper limit for distance band by using the steps below:\n\nUse the knearneigh() function from the spdep package to create a matrix with the indices of the k nearest neighbors for each point.\n\n\n# get k nearest neighbour where k = 1 (default)\nknearneigh(coords, k=1)\n\n$nn\n      [,1]\n [1,]    3\n [2,]   78\n [3,]    1\n [4,]    5\n [5,]    4\n [6,]   69\n [7,]   67\n [8,]   46\n [9,]   84\n[10,]   70\n[11,]   72\n[12,]   63\n[13,]   12\n[14,]   17\n[15,]   13\n[16,]   22\n[17,]   16\n[18,]   20\n[19,]   21\n[20,]   82\n[21,]   19\n[22,]   16\n[23,]   41\n[24,]   54\n[25,]   81\n[26,]   81\n[27,]   29\n[28,]   49\n[29,]   27\n[30,]   33\n[31,]   24\n[32,]   50\n[33,]   28\n[34,]   45\n[35,]   47\n[36,]   34\n[37,]   42\n[38,]   44\n[39,]   43\n[40,]   39\n[41,]   23\n[42,]   37\n[43,]   44\n[44,]   43\n[45,]   34\n[46,]   47\n[47,]   46\n[48,]   51\n[49,]   28\n[50,]   52\n[51,]   48\n[52,]   54\n[53,]   55\n[54,]   52\n[55,]   50\n[56,]   36\n[57,]   58\n[58,]   57\n[59,]   87\n[60,]   13\n[61,]   63\n[62,]   61\n[63,]   12\n[64,]   57\n[65,]   76\n[66,]   68\n[67,]    7\n[68,]   66\n[69,]    6\n[70,]   10\n[71,]   74\n[72,]   11\n[73,]   70\n[74,]   71\n[75,]   55\n[76,]   65\n[77,]   38\n[78,]    2\n[79,]   45\n[80,]   34\n[81,]   25\n[82,]   21\n[83,]   12\n[84,]    9\n[85,]    5\n[86,]   74\n[87,]   61\n[88,]   87\n\n$np\n[1] 88\n\n$k\n[1] 1\n\n$dimension\n[1] 2\n\n$x\n      longitude latitude\n [1,]  112.1531 29.44362\n [2,]  112.0372 28.86489\n [3,]  111.8917 29.47107\n [4,]  111.7031 29.74499\n [5,]  111.6138 29.49258\n [6,]  111.0341 29.79863\n [7,]  113.7065 28.23215\n [8,]  112.3460 28.13081\n [9,]  112.8169 28.28918\n[10,]  113.3534 26.57906\n[11,]  113.8942 25.98122\n[12,]  112.4006 25.63215\n[13,]  112.5542 25.33880\n[14,]  113.6636 25.54967\n[15,]  112.9206 25.26722\n[16,]  113.1883 26.21248\n[17,]  113.4521 25.93480\n[18,]  112.4209 26.36132\n[19,]  113.0152 27.08120\n[20,]  112.6350 26.75969\n[21,]  112.7087 27.27930\n[22,]  112.9095 26.42079\n[23,]  111.9522 26.80117\n[24,]  110.2606 27.89384\n[25,]  110.0921 27.54115\n[26,]  109.7985 26.91321\n[27,]  109.5765 26.54507\n[28,]  109.7211 27.78801\n[29,]  109.7339 26.21157\n[30,]  109.1537 27.22941\n[31,]  110.6442 27.83407\n[32,]  110.5916 28.57282\n[33,]  109.5984 27.39828\n[34,]  111.4783 27.67997\n[35,]  112.1745 27.46256\n[36,]  111.2315 27.86930\n[37,]  110.3149 26.32113\n[38,]  111.3248 26.48991\n[39,]  110.5859 27.10164\n[40,]  110.9593 27.34884\n[41,]  111.8296 27.18765\n[42,]  110.1926 26.70972\n[43,]  110.7334 26.78494\n[44,]  110.9123 26.54354\n[45,]  111.4599 27.42910\n[46,]  112.5268 27.92456\n[47,]  112.3406 27.77407\n[48,]  109.5602 28.66808\n[49,]  109.5071 28.01142\n[50,]  109.9954 28.60033\n[51,]  109.4273 28.42749\n[52,]  109.7587 28.31518\n[53,]  109.5044 29.21940\n[54,]  109.9899 28.16053\n[55,]  109.9664 29.01206\n[56,]  111.3785 28.28449\n[57,]  112.4350 29.23817\n[58,]  112.5558 28.97135\n[59,]  111.7379 24.97087\n[60,]  112.1831 25.31559\n[61,]  111.9743 25.65101\n[62,]  111.7009 25.91101\n[63,]  112.2196 25.88615\n[64,]  112.6472 29.48614\n[65,]  113.5102 29.49285\n[66,]  113.1172 28.79707\n[67,]  113.7089 28.76024\n[68,]  112.7963 28.71653\n[69,]  110.9276 29.39439\n[70,]  113.6420 26.80361\n[71,]  113.4577 27.66123\n[72,]  113.8404 26.37989\n[73,]  113.4758 27.17064\n[74,]  113.1428 27.62875\n[75,]  110.3017 29.39053\n[76,]  113.1957 29.25343\n[77,]  111.7410 26.36035\n[78,]  112.1831 28.49854\n[79,]  111.3390 27.01465\n[80,]  111.8208 27.75124\n[81,]  110.0753 27.23539\n[82,]  112.3965 27.08323\n[83,]  112.7683 25.82828\n[84,]  113.1679 28.30074\n[85,]  111.4495 28.95406\n[86,]  112.7956 27.68910\n[87,]  111.5896 25.49530\n[88,]  111.2393 25.19355\n\nattr(,\"class\")\n[1] \"knn\"\nattr(,\"call\")\nknearneigh(x = coords, k = 1)\n\n\n\nConvert the knn object returned by knearneigh() into a neighbor list (nb class) using the knn2nb() function. This list contains integer vectors representing the IDs of neighboring regions.\n\n\n# convert knn matrix to neighbour list for k = 1\nk1 &lt;- knn2nb(knearneigh(coords))\nk1\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 88 \nPercentage nonzero weights: 1.136364 \nAverage number of links: 1 \n25 disjoint connected subgraphs\nNon-symmetric neighbours list\n\n\n\nCalculate the length of neighbor relationship edges with the nbdists() function from spdep. The distances will be in the units of the coordinates if projected, or in kilometers if not.\n\n\nnbdists(k1, coords, longlat = TRUE)\n\n[[1]]\n[1] 25.53398\n\n[[2]]\n[1] 43.03114\n\n[[3]]\n[1] 25.53398\n\n[[4]]\n[1] 29.2848\n\n[[5]]\n[1] 29.2848\n\n[[6]]\n[1] 45.98097\n\n[[7]]\n[1] 58.52704\n\n[[8]]\n[1] 28.95985\n\n[[9]]\n[1] 34.45062\n\n[[10]]\n[1] 37.99885\n\n[[11]]\n[1] 44.49442\n\n[[12]]\n[1] 33.48816\n\n[[13]]\n[1] 35.98123\n\n[[14]]\n[1] 47.65184\n\n[[15]]\n[1] 37.73556\n\n[[16]]\n[1] 36.16613\n\n[[17]]\n[1] 40.53569\n\n[[18]]\n[1] 49.02492\n\n[[19]]\n[1] 37.47543\n\n[[20]]\n[1] 42.97316\n\n[[21]]\n[1] 37.47543\n\n[[22]]\n[1] 36.16613\n\n[[23]]\n[1] 44.51898\n\n[[24]]\n[1] 39.7744\n\n[[25]]\n[1] 33.9218\n\n[[26]]\n[1] 45.03425\n\n[[27]]\n[1] 40.15056\n\n[[28]]\n[1] 32.50795\n\n[[29]]\n[1] 40.15056\n\n[[30]]\n[1] 47.83345\n\n[[31]]\n[1] 38.35439\n\n[[32]]\n[1] 58.39365\n\n[[33]]\n[1] 44.85211\n\n[[34]]\n[1] 27.85864\n\n[[35]]\n[1] 38.2151\n\n[[36]]\n[1] 32.12293\n\n[[37]]\n[1] 44.74688\n\n[[38]]\n[1] 41.53815\n\n[[39]]\n[1] 38.02669\n\n[[40]]\n[1] 46.029\n\n[[41]]\n[1] 44.51898\n\n[[42]]\n[1] 44.74688\n\n[[43]]\n[1] 32.1334\n\n[[44]]\n[1] 32.1334\n\n[[45]]\n[1] 27.85864\n\n[[46]]\n[1] 24.79082\n\n[[47]]\n[1] 24.79082\n\n[[48]]\n[1] 29.66852\n\n[[49]]\n[1] 32.50795\n\n[[50]]\n[1] 39.19375\n\n[[51]]\n[1] 29.66852\n\n[[52]]\n[1] 28.43598\n\n[[53]]\n[1] 50.50645\n\n[[54]]\n[1] 28.43598\n\n[[55]]\n[1] 45.721\n\n[[56]]\n[1] 48.22649\n\n[[57]]\n[1] 31.82332\n\n[[58]]\n[1] 31.82332\n\n[[59]]\n[1] 59.98421\n\n[[60]]\n[1] 37.44866\n\n[[61]]\n[1] 35.83248\n\n[[62]]\n[1] 39.77577\n\n[[63]]\n[1] 33.48816\n\n[[64]]\n[1] 34.34758\n\n[[65]]\n[1] 40.45791\n\n[[66]]\n[1] 32.58547\n\n[[67]]\n[1] 58.52704\n\n[[68]]\n[1] 32.58547\n\n[[69]]\n[1] 45.98097\n\n[[70]]\n[1] 37.99885\n\n[[71]]\n[1] 31.27538\n\n[[72]]\n[1] 44.49442\n\n[[73]]\n[1] 43.88878\n\n[[74]]\n[1] 31.27538\n\n[[75]]\n[1] 53.12656\n\n[[76]]\n[1] 40.45791\n\n[[77]]\n[1] 43.93382\n\n[[78]]\n[1] 43.03114\n\n[[79]]\n[1] 47.45858\n\n[[80]]\n[1] 34.68711\n\n[[81]]\n[1] 33.9218\n\n[[82]]\n[1] 37.80739\n\n[[83]]\n[1] 42.81869\n\n[[84]]\n[1] 34.45062\n\n[[85]]\n[1] 61.79116\n\n[[86]]\n[1] 34.90929\n\n[[87]]\n[1] 42.32891\n\n[[88]]\n[1] 48.59005\n\nattr(,\"class\")\n[1] \"nbdist\"\nattr(,\"call\")\nnbdists(nb = k1, coords = coords, longlat = TRUE)\n\n\n\nSimplify the list structure of the returned object using the unlist() function from the base R package.\n\n\nk1dists &lt;- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79 \n\n\n\n\n\n\n\n\nNote\n\n\n\nUsing the summary report, we can observe that the largest first nearest neighbour distance is 61.79 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour.\n\n\n\n\n10.2 Plotting Fixed Distance Weight Matrix\n\n# get max dist from k1dists rounded up to integer\nmax_dist &lt;- as.integer(ceiling(max(k1dists)))\n\nwm_d62 &lt;- dnearneigh(x=coords, d1=0, d2=max_dist, longlat = TRUE)\nwm_d62\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \n\n\n\n\n\n\n\n\nNote\n\n\n\nOutput Intepretation\n\nThe weight matrix shows 88 regions (counties).\nThere are a total of 324 connections between all regions.\nThe formula for Percentage nonzero weights:\n\n\\(\\text{Percentage nonzero weights} = \\left( \\frac{\\text{Number of nonzero links}}{\\text{Total possible links}} \\right) \\times 100\\)\nand the total possible links can be computed as \\(n \\times n=88 \\times 88 = 7744\\)\nPlugging in the numbers, we will get \\(\\text{Percentage nonzero weights} = \\left( \\frac{324}{7744} \\right) \\times 100 \\approx 4.18\\%\\)\n\nThe formula for average number of links: \\(\\text{Average number of links} = \\frac{\\text{Total number of nonzero links}}{\\text{Number of regions}} = \\frac{324}{88} \\approx 3.68\\)\n\n\n\nWe can also use str() to display the content of wm_d62 weight matrix.\n\nstr(wm_d62)\n\nAnother way to display the structure of the weight matrix is to combine table() and card() of spdep.\n\ntable(hunan$County, \n      card(wm_d62))\n\n               \n                1 2 3 4 5 6\n  Anhua         1 0 0 0 0 0\n  Anren         0 0 0 1 0 0\n  Anxiang       0 0 0 0 1 0\n  Baojing       0 0 0 0 1 0\n  Chaling       0 0 1 0 0 0\n  Changning     0 0 1 0 0 0\n  Changsha      0 0 0 1 0 0\n  Chengbu       0 1 0 0 0 0\n  Chenxi        0 0 0 1 0 0\n  Cili          0 1 0 0 0 0\n  Dao           0 0 0 1 0 0\n  Dongan        0 0 1 0 0 0\n  Dongkou       0 0 0 1 0 0\n  Fenghuang     0 0 0 1 0 0\n  Guidong       0 0 1 0 0 0\n  Guiyang       0 0 0 1 0 0\n  Guzhang       0 0 0 0 0 1\n  Hanshou       0 0 0 1 0 0\n  Hengdong      0 0 0 0 1 0\n  Hengnan       0 0 0 0 1 0\n  Hengshan      0 0 0 0 0 1\n  Hengyang      0 0 0 0 0 1\n  Hongjiang     0 0 0 0 1 0\n  Huarong       0 0 0 1 0 0\n  Huayuan       0 0 0 1 0 0\n  Huitong       0 0 0 1 0 0\n  Jiahe         0 0 0 0 1 0\n  Jianghua      0 0 1 0 0 0\n  Jiangyong     0 1 0 0 0 0\n  Jingzhou      0 1 0 0 0 0\n  Jinshi        0 0 0 1 0 0\n  Jishou        0 0 0 0 0 1\n  Lanshan       0 0 0 1 0 0\n  Leiyang       0 0 0 1 0 0\n  Lengshuijiang 0 0 1 0 0 0\n  Li            0 0 1 0 0 0\n  Lianyuan      0 0 0 0 1 0\n  Liling        0 1 0 0 0 0\n  Linli         0 0 0 1 0 0\n  Linwu         0 0 0 1 0 0\n  Linxiang      1 0 0 0 0 0\n  Liuyang       0 1 0 0 0 0\n  Longhui       0 0 1 0 0 0\n  Longshan      0 1 0 0 0 0\n  Luxi          0 0 0 0 1 0\n  Mayang        0 0 0 0 0 1\n  Miluo         0 0 0 0 1 0\n  Nan           0 0 0 0 1 0\n  Ningxiang     0 0 0 1 0 0\n  Ningyuan      0 0 0 0 1 0\n  Pingjiang     0 1 0 0 0 0\n  Qidong        0 0 1 0 0 0\n  Qiyang        0 0 1 0 0 0\n  Rucheng       0 1 0 0 0 0\n  Sangzhi       0 1 0 0 0 0\n  Shaodong      0 0 0 0 1 0\n  Shaoshan      0 0 0 0 1 0\n  Shaoyang      0 0 0 1 0 0\n  Shimen        1 0 0 0 0 0\n  Shuangfeng    0 0 0 0 0 1\n  Shuangpai     0 0 0 1 0 0\n  Suining       0 0 0 0 1 0\n  Taojiang      0 1 0 0 0 0\n  Taoyuan       0 1 0 0 0 0\n  Tongdao       0 1 0 0 0 0\n  Wangcheng     0 0 0 1 0 0\n  Wugang        0 0 1 0 0 0\n  Xiangtan      0 0 0 1 0 0\n  Xiangxiang    0 0 0 0 1 0\n  Xiangyin      0 0 0 1 0 0\n  Xinhua        0 0 0 0 1 0\n  Xinhuang      1 0 0 0 0 0\n  Xinning       0 1 0 0 0 0\n  Xinshao       0 0 0 0 0 1\n  Xintian       0 0 0 0 1 0\n  Xupu          0 1 0 0 0 0\n  Yanling       0 0 1 0 0 0\n  Yizhang       1 0 0 0 0 0\n  Yongshun      0 0 0 1 0 0\n  Yongxing      0 0 0 1 0 0\n  You           0 0 0 1 0 0\n  Yuanjiang     0 0 0 0 1 0\n  Yuanling      1 0 0 0 0 0\n  Yueyang       0 0 1 0 0 0\n  Zhijiang      0 0 0 0 1 0\n  Zhongfang     0 0 0 1 0 0\n  Zhuzhou       0 0 0 0 1 0\n  Zixing        0 0 1 0 0 0\n\n\nn.comp.nb() finds the number of disjoint connected subgraphs in the graph depicted by nb.obj see Graph Components function - RDocumentation\n\nn_comp &lt;- n.comp.nb(wm_d62)\nn_comp$nc\n\n[1] 1\n\n\nAbove shows the number of connected components in the spatial neighbour network. The output of 1 indicates that is 1 connected component.\n\ntable(n_comp$comp.id)\n\n\n 1 \n88 \n\n\nAnd this connected component comprises of 88 regions, indicating all regions are part of a single interconnected group.\n\n\n10.3 Plotting Fixed Distance Weight Matrix\nWe can plot the distance weight matrix as shown below:\n\npar(mfrow=c(1,2))\n\nplot(hunan$geometry, border=\"lightgrey\", main=\"1st Nearest Neighbours\")\nplot(k1, coords, add=TRUE, col=\"red\", length=0.08)\n\nplot(hunan$geometry, border=\"lightgrey\", main=\"Distance Link\")\nplot(wm_d62, coords, add=TRUE)\nplot(k1, coords, add=TRUE, col=\"red\", length=0.08)\n\n\n\n\n\n\n\n\nOn the left plot, the red lines show the links of 1st nearest neighbours.\nOn the right plot, the red lines show the links of 1st nearest neighbours and the black lines show the links of neighbours within the cut-off distance of 62km.\n\n\n10.4 Computing Adaptive Distance Weight Matrix\nA fixed distance weight matrix typically shows that densely populated areas (like urban regions) have more neighbors, while sparsely populated areas (such as rural counties) have fewer neighbors. When there are many neighbors, the relationships between them are spread across a larger number of connections, creating a smoother effect.\nTo control the number of neighbours, we can set the k value.\n\nknn6 &lt;- knn2nb(knearneigh(coords, k=6))\nknn6\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 528 \nPercentage nonzero weights: 6.818182 \nAverage number of links: 6 \nNon-symmetric neighbours list\n\n\nEach county has exactly 6 neighbours.\n\nstr(knn6)\n\n\n10.4.1 Computing Adaptive Distance Weight Matrix\n\nplot(hunan$geometry, border=\"lightgrey\", main=\"6 Nearest Neighbours\")\nplot(knn6, coords, add = TRUE, col = \"red\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#weights-based-on-inversed-distance-weight-idw-method",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#weights-based-on-inversed-distance-weight-idw-method",
    "title": "4A: Spatial Weights and Applications",
    "section": "11 Weights based on Inversed Distance Weight (IDW) Method",
    "text": "11 Weights based on Inversed Distance Weight (IDW) Method\nIn this section, you will learn how to derive a spatial weight matrix based on Inversed Distance Weights method.\n\n\n\n\n\n\nNote\n\n\n\nIn order to conform to Tobler’s first law of geography, a distance decay effect must be respected.\nThe inverse distance weight method assigns weights to neighbors based on their distance: closer neighbors get higher weights, and further ones get lower weights.\nIt works by taking the distance between two locations and calculating the weight as 1 divided by that distance.\nFor more info, see Spatial Weights as Distance Functions.\n\n\n\ndist &lt;- nbdists(wm_q, \n                coords, \n                longlat = TRUE)\n# apply 1/x on every element on list objects\nids &lt;- lapply(dist, function(x) 1/(x))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#row-standardised-weights-matrix",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#row-standardised-weights-matrix",
    "title": "4A: Spatial Weights and Applications",
    "section": "12 Row-standardised Weights Matrix",
    "text": "12 Row-standardised Weights Matrix\nNext, we need to assign weights to each neighboring polygon. In this case, we’ll use equal weights (style=“W”), where each neighboring polygon gets a weight of 1/(number of neighbors). This means we take the value for each neighbor and divide it by the total number of neighbors, then sum these weighted values to calculate a summary measure, such as weighted income.\nWhile this equal weighting approach is straightforward and easy to understand, it has a limitation: polygons on the edges of the study area have fewer neighbors, which can lead to over- or underestimation of the actual spatial relationships (spatial autocorrelation) in the data.\n\n\n\n\n\n\nTip\n\n\n\nFor simplicity, we use the style=“W” option in this example, but keep in mind that other, potentially more accurate methods are available, such as style=“B”.\n\n\n\n# note we are using queen's method here\nrswm_q &lt;- nb2listw(wm_q, style=\"W\", zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe zero.policy=TRUE option allows for lists of non-neighbors. This should be used with caution since the user may not be aware of missing neighbors in their dataset however, a zero.policy of FALSE would return an error.\n\n\nTo see the weight of the 85th polygon’s 11 neighbors type:\n\nrswm_q$weights[85]\n\n[[1]]\n [1] 0.09090909 0.09090909 0.09090909 0.09090909 0.09090909 0.09090909\n [7] 0.09090909 0.09090909 0.09090909 0.09090909 0.09090909\n\n\nEach neighbor is assigned a 0.0909 of the total weight. This means that when R computes the average neighboring income values, each neighbor’s income will be multiplied by 0.0909 before being tallied.\nUsing the same method, we can also derive a row standardised distance weight matrix by using the code block below.\n\nrswm_ids &lt;- nb2listw(wm_q, glist=ids, style=\"B\", zero.policy=TRUE)\nrswm_ids\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn       S0        S1     S2\nB 88 7744 8.786867 0.3776535 3.8137\n\n\n\nrswm_ids$weights[85]\n\n[[1]]\n [1] 0.011451133 0.017193502 0.013957649 0.016183544 0.009810297 0.010656545\n [7] 0.013416965 0.009903702 0.014199260 0.008217581 0.011407794\n\n\nNotice that the output has different weight of each neighbour. We can use summary report to observe the differences.\n\nsummary(unlist(rswm_q$weights))\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n0.09091 0.14286 0.20000 0.19643 0.20000 1.00000 \n\ncat(\"\\n-----------------------------\\n\")\n\n\n-----------------------------\n\nsummary(unlist(rswm_ids$weights))\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n0.008218 0.015088 0.018739 0.019614 0.022823 0.040338"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#application-of-spatial-weight-matrix",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#application-of-spatial-weight-matrix",
    "title": "4A: Spatial Weights and Applications",
    "section": "13 Application of Spatial Weight Matrix",
    "text": "13 Application of Spatial Weight Matrix\nIn this section, we will create 4 different spatial lagged variables:\n\nspatial lag with row-standardized weights,\nspatial lag as a sum of neighbouring values,\nspatial window average, and\nspatial window sum.\n\n\n13.1 Spatial Lag with Row-standardized Weights\nWe’ll compute the average neighbor GDPPC value for each polygon. These values are often referred to as spatially lagged values.\n\nGDPPC.lag &lt;- lag.listw(rswm_q, hunan$GDPPC)\nGDPPC.lag\n\n [1] 24847.20 22724.80 24143.25 27737.50 27270.25 21248.80 43747.00 33582.71\n [9] 45651.17 32027.62 32671.00 20810.00 25711.50 30672.33 33457.75 31689.20\n[17] 20269.00 23901.60 25126.17 21903.43 22718.60 25918.80 20307.00 20023.80\n[25] 16576.80 18667.00 14394.67 19848.80 15516.33 20518.00 17572.00 15200.12\n[33] 18413.80 14419.33 24094.50 22019.83 12923.50 14756.00 13869.80 12296.67\n[41] 15775.17 14382.86 11566.33 13199.50 23412.00 39541.00 36186.60 16559.60\n[49] 20772.50 19471.20 19827.33 15466.80 12925.67 18577.17 14943.00 24913.00\n[57] 25093.00 24428.80 17003.00 21143.75 20435.00 17131.33 24569.75 23835.50\n[65] 26360.00 47383.40 55157.75 37058.00 21546.67 23348.67 42323.67 28938.60\n[73] 25880.80 47345.67 18711.33 29087.29 20748.29 35933.71 15439.71 29787.50\n[81] 18145.00 21617.00 29203.89 41363.67 22259.09 44939.56 16902.00 16930.00\n\n\nIn the previous section, we retrieved the GDPPC of these 11 counties by using the code block below.\n\nnb1 &lt;- wm_q[[85]]\nnb1 &lt;- hunan$GDPPC[nb1]\nnb1\n\n [1] 23667 20981 34592 25554 27137 24194 14567 21311 18714 14624 19509\n\n\n\nQuestion: Can you see the meaning of Spatial lag with row-standardized weights now? Spatial lag represents the average or sum of a variable (GDPPC) for a region’s neighbors. In this context, the spatial lag for a region gives an idea of how that region’s GDPPC relates to the GDPPC of its neighboring regions.\nWith row-standardized weights (style=“W”), each neighboring region is assigned an equal weight of 1/(number of neighbors). This ensures that the weights for all neighbors of a region sum to 1.\n\nWe can append the spatially lag GDPPC values onto hunan sf data frame by using the code block below:\n\nlag.list &lt;- list(hunan$NAME_3, lag.listw(rswm_q, hunan$GDPPC))\nlag.res &lt;- as.data.frame(lag.list)\ncolnames(lag.res) &lt;- c(\"NAME_3\", \"lag GDPPC\")\nhunan &lt;- left_join(hunan,lag.res)\n\nThe following table shows the average neighboring income values (stored in the Inc.lag object) for each county.\n\nhead(hunan)\n\nSimple feature collection with 6 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 110.4922 ymin: 28.61762 xmax: 112.3013 ymax: 30.12812\nGeodetic CRS:  WGS 84\n   NAME_2  ID_3  NAME_3   ENGTYPE_3  County GDPPC lag GDPPC\n1 Changde 21098 Anxiang      County Anxiang 23667  24847.20\n2 Changde 21100 Hanshou      County Hanshou 20981  22724.80\n3 Changde 21101  Jinshi County City  Jinshi 34592  24143.25\n4 Changde 21102      Li      County      Li 24473  27737.50\n5 Changde 21103   Linli      County   Linli 25554  27270.25\n6 Changde 21104  Shimen      County  Shimen 27137  21248.80\n                        geometry\n1 POLYGON ((112.0625 29.75523...\n2 POLYGON ((112.2288 29.11684...\n3 POLYGON ((111.8927 29.6013,...\n4 POLYGON ((111.3731 29.94649...\n5 POLYGON ((111.6324 29.76288...\n6 POLYGON ((110.8825 30.11675...\n\n\nNext, we will plot both the GDPPC and spatial lag GDPPC for comparison using the code block below.\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\nlag_gdppc &lt;- qtm(hunan, \"lag GDPPC\")\ntmap_arrange(gdppc, lag_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n\n\nRecall that in our previous observation, we made the statement that Changsha has the highest GDP per capital values in the Hunan region.\nUsing spatial lag values, we can observe that Yueyang has the highest spatial lag GDP per capita, meaning its neighbors (on average) have the highest GDP per capita.\n\n\n13.2 Spatial Lag as a Sum of Neighboring Values\nTo calculate spatial lag as the sum of neighboring values, we can assign binary weights (where each neighbor gets a weight of 1).\nTo do this, we go back to our list of neighbors and use a function to assign these binary weights. We then use the glist argument in the nb2listw function to set these weights explicitly.\nWe start by using lapply to assign a weight of 1 to each neighbor. lapply is a function we have been using to work with the neighbors list in previous notebooks; it applies a specified function to each value in the neighbors list.\n\n# Create binary weights for each neighbor\n# uses lapply to go through each element of the neighbors list (wm_q).\n# The function '0 * x + 1' assigns a weight of 1 to each neighbor.\nb_weights &lt;- lapply(wm_q, function(x) 0*x + 1)\n\n# Convert the neighbor list to a spatial weights list object\n# 'nb2listw' converts the neighbors list (wm_q) into a weights list.\n# 'glist = b_weights' explicitly sets the weights to the binary weights created above.\n# 'style = \"B\"' specifies binary weighting style, where each neighbor has equal weight (1).\nb_weights2 &lt;- nb2listw(wm_q, \n                       glist = b_weights, \n                       style = \"B\")\nb_weights2\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1    S2\nB 88 7744 448 896 10224\n\n\nWith the proper weights assigned, we can use lag.listw to compute a lag variable from our weight and GDPPC.\n\nlag_sum &lt;- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))\nlag.res &lt;- as.data.frame(lag_sum)\ncolnames(lag.res) &lt;- c(\"NAME_3\", \"lag_sum GDPPC\")\nlag_sum\n\n[[1]]\n [1] \"Anxiang\"       \"Hanshou\"       \"Jinshi\"        \"Li\"           \n [5] \"Linli\"         \"Shimen\"        \"Liuyang\"       \"Ningxiang\"    \n [9] \"Wangcheng\"     \"Anren\"         \"Guidong\"       \"Jiahe\"        \n[13] \"Linwu\"         \"Rucheng\"       \"Yizhang\"       \"Yongxing\"     \n[17] \"Zixing\"        \"Changning\"     \"Hengdong\"      \"Hengnan\"      \n[21] \"Hengshan\"      \"Leiyang\"       \"Qidong\"        \"Chenxi\"       \n[25] \"Zhongfang\"     \"Huitong\"       \"Jingzhou\"      \"Mayang\"       \n[29] \"Tongdao\"       \"Xinhuang\"      \"Xupu\"          \"Yuanling\"     \n[33] \"Zhijiang\"      \"Lengshuijiang\" \"Shuangfeng\"    \"Xinhua\"       \n[37] \"Chengbu\"       \"Dongan\"        \"Dongkou\"       \"Longhui\"      \n[41] \"Shaodong\"      \"Suining\"       \"Wugang\"        \"Xinning\"      \n[45] \"Xinshao\"       \"Shaoshan\"      \"Xiangxiang\"    \"Baojing\"      \n[49] \"Fenghuang\"     \"Guzhang\"       \"Huayuan\"       \"Jishou\"       \n[53] \"Longshan\"      \"Luxi\"          \"Yongshun\"      \"Anhua\"        \n[57] \"Nan\"           \"Yuanjiang\"     \"Jianghua\"      \"Lanshan\"      \n[61] \"Ningyuan\"      \"Shuangpai\"     \"Xintian\"       \"Huarong\"      \n[65] \"Linxiang\"      \"Miluo\"         \"Pingjiang\"     \"Xiangyin\"     \n[69] \"Cili\"          \"Chaling\"       \"Liling\"        \"Yanling\"      \n[73] \"You\"           \"Zhuzhou\"       \"Sangzhi\"       \"Yueyang\"      \n[77] \"Qiyang\"        \"Taojiang\"      \"Shaoyang\"      \"Lianyuan\"     \n[81] \"Hongjiang\"     \"Hengyang\"      \"Guiyang\"       \"Changsha\"     \n[85] \"Taoyuan\"       \"Xiangtan\"      \"Dao\"           \"Jiangyong\"    \n\n[[2]]\n [1] 124236 113624  96573 110950 109081 106244 174988 235079 273907 256221\n[11]  98013 104050 102846  92017 133831 158446 141883 119508 150757 153324\n[21] 113593 129594 142149 100119  82884  74668  43184  99244  46549  20518\n[31] 140576 121601  92069  43258 144567 132119  51694  59024  69349  73780\n[41]  94651 100680  69398  52798 140472 118623 180933  82798  83090  97356\n[51]  59482  77334  38777 111463  74715 174391 150558 122144  68012  84575\n[61] 143045  51394  98279  47671  26360 236917 220631 185290  64640  70046\n[71] 126971 144693 129404 284074 112268 203611 145238 251536 108078 238300\n[81] 108870 108085 262835 248182 244850 404456  67608  33860\n\n\n\nQuestion: Can you understand the meaning of Spatial lag as a sum of neighboring values now?\nBy assigning binary weights (where each neighbor is given a weight of 1), we calculate the spatial lag by summing the values of this variable for all neighbors. This means that the spatial lag reflects the combined influence or total value contributed by the neighboring regions, providing an idea of the overall regional context or neighborhood effect around a specific area.\n\nNext, we will append the lag_sum GDPPC field into hunan sf data frame by using the code block below.\n\nhunan &lt;- left_join(hunan, lag.res)\n\nNow, we can plot the GDPPC, Spatial Lag GDPPC, Spatial Lag Sum GDPPC for comparison using the code block below.\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\nlag_gdppc &lt;- qtm(hunan, \"lag GDPPC\")\nlag_sum_gdppc &lt;- qtm(hunan, \"lag_sum GDPPC\")\ntmap_arrange(gdppc, lag_gdppc, lag_sum_gdppc, asp=1, ncol=3)\n\n\n\n\n\n\n\n\n\n\n13.3 Spatial Window Average\nThe spatial window average uses row-standardized weights and includes the diagonal element. To do this in R, we need to go back to the neighbors structure and add the diagonal element before assigning weights.\nTo add the diagonal element to the neighbour list, we just need to use include.self() from spdep.\n\nwm_qs &lt;- include.self(wm_q)\n\nLet us take a good look at the neighbour list of area 85 by using the code block below.\n\nwm_qs[[85]]\n\n [1]  1  2  3  5  6 32 56 57 69 75 78 85\n\n\nNotice that now region 85 has 12 neighbours instead of 11.\nNow we obtain weights with nb2listw().\n\nwm_qs &lt;- nb2listw(wm_qs)\nwm_qs\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 30.90265 357.5308\n\n\nAgain, we use nb2listw() and glist() to explicitly assign weight values.\nLastly, we just need to create the lag variable from our weight structure and GDPPC variable.\n\nlag_w_avg_gpdpc &lt;- lag.listw(wm_qs, \n                             hunan$GDPPC)\nlag_w_avg_gpdpc\n\n [1] 24650.50 22434.17 26233.00 27084.60 26927.00 22230.17 47621.20 37160.12\n [9] 49224.71 29886.89 26627.50 22690.17 25366.40 25825.75 30329.00 32682.83\n[17] 25948.62 23987.67 25463.14 21904.38 23127.50 25949.83 20018.75 19524.17\n[25] 18955.00 17800.40 15883.00 18831.33 14832.50 17965.00 17159.89 16199.44\n[33] 18764.50 26878.75 23188.86 20788.14 12365.20 15985.00 13764.83 11907.43\n[41] 17128.14 14593.62 11644.29 12706.00 21712.29 43548.25 35049.00 16226.83\n[49] 19294.40 18156.00 19954.75 18145.17 12132.75 18419.29 14050.83 23619.75\n[57] 24552.71 24733.67 16762.60 20932.60 19467.75 18334.00 22541.00 26028.00\n[65] 29128.50 46569.00 47576.60 36545.50 20838.50 22531.00 42115.50 27619.00\n[73] 27611.33 44523.29 18127.43 28746.38 20734.50 33880.62 14716.38 28516.22\n[81] 18086.14 21244.50 29568.80 48119.71 22310.75 43151.60 17133.40 17009.33\n\n\nNext, we will convert the lag variable listw object into a data.frame by using as.data.frame().\n\nlag.list.wm_qs &lt;- list(hunan$NAME_3, lag.listw(wm_qs, hunan$GDPPC))\nlag_wm_qs.res &lt;- as.data.frame(lag.list.wm_qs)\ncolnames(lag_wm_qs.res) &lt;- c(\"NAME_3\", \"lag_window_avg GDPPC\")\n\n\n\n\n\n\n\nNote\n\n\n\nThe third command line on the code chunk above renames the field names of lag_wm_q1.res object into NAME_3 and lag_window_avg GDPPC respectively.\n\n\nNext, the code chunk below will be used to append lag_window_avg GDPPC values onto hunan sf data.frame by using left_join() of dplyr package.\n\nhunan &lt;- left_join(hunan, lag_wm_qs.res)\n\nTo compare the values of lag GDPPC and Spatial window average, kable() of Knitr package is used to prepare a table using the code chunk below.\n\nhunan %&gt;%\n  select(\"County\", \n         \"lag GDPPC\", \n         \"lag_window_avg GDPPC\") %&gt;%\n  kable()\n\n\n\n\n\n\n\n\n\n\nCounty\nlag GDPPC\nlag_window_avg GDPPC\ngeometry\n\n\n\n\nAnxiang\n24847.20\n24650.50\nPOLYGON ((112.0625 29.75523…\n\n\nHanshou\n22724.80\n22434.17\nPOLYGON ((112.2288 29.11684…\n\n\nJinshi\n24143.25\n26233.00\nPOLYGON ((111.8927 29.6013,…\n\n\nLi\n27737.50\n27084.60\nPOLYGON ((111.3731 29.94649…\n\n\nLinli\n27270.25\n26927.00\nPOLYGON ((111.6324 29.76288…\n\n\nShimen\n21248.80\n22230.17\nPOLYGON ((110.8825 30.11675…\n\n\nLiuyang\n43747.00\n47621.20\nPOLYGON ((113.9905 28.5682,…\n\n\nNingxiang\n33582.71\n37160.12\nPOLYGON ((112.7181 28.38299…\n\n\nWangcheng\n45651.17\n49224.71\nPOLYGON ((112.7914 28.52688…\n\n\nAnren\n32027.62\n29886.89\nPOLYGON ((113.1757 26.82734…\n\n\nGuidong\n32671.00\n26627.50\nPOLYGON ((114.1799 26.20117…\n\n\nJiahe\n20810.00\n22690.17\nPOLYGON ((112.4425 25.74358…\n\n\nLinwu\n25711.50\n25366.40\nPOLYGON ((112.5914 25.55143…\n\n\nRucheng\n30672.33\n25825.75\nPOLYGON ((113.6759 25.87578…\n\n\nYizhang\n33457.75\n30329.00\nPOLYGON ((113.2621 25.68394…\n\n\nYongxing\n31689.20\n32682.83\nPOLYGON ((113.3169 26.41843…\n\n\nZixing\n20269.00\n25948.62\nPOLYGON ((113.7311 26.16259…\n\n\nChangning\n23901.60\n23987.67\nPOLYGON ((112.6144 26.60198…\n\n\nHengdong\n25126.17\n25463.14\nPOLYGON ((113.1056 27.21007…\n\n\nHengnan\n21903.43\n21904.38\nPOLYGON ((112.7599 26.98149…\n\n\nHengshan\n22718.60\n23127.50\nPOLYGON ((112.607 27.4689, …\n\n\nLeiyang\n25918.80\n25949.83\nPOLYGON ((112.9996 26.69276…\n\n\nQidong\n20307.00\n20018.75\nPOLYGON ((111.7818 27.0383,…\n\n\nChenxi\n20023.80\n19524.17\nPOLYGON ((110.2624 28.21778…\n\n\nZhongfang\n16576.80\n18955.00\nPOLYGON ((109.9431 27.72858…\n\n\nHuitong\n18667.00\n17800.40\nPOLYGON ((109.9419 27.10512…\n\n\nJingzhou\n14394.67\n15883.00\nPOLYGON ((109.8186 26.75842…\n\n\nMayang\n19848.80\n18831.33\nPOLYGON ((109.795 27.98008,…\n\n\nTongdao\n15516.33\n14832.50\nPOLYGON ((109.9294 26.46561…\n\n\nXinhuang\n20518.00\n17965.00\nPOLYGON ((109.227 27.43733,…\n\n\nXupu\n17572.00\n17159.89\nPOLYGON ((110.7189 28.30485…\n\n\nYuanling\n15200.12\n16199.44\nPOLYGON ((110.9652 28.99895…\n\n\nZhijiang\n18413.80\n18764.50\nPOLYGON ((109.8818 27.60661…\n\n\nLengshuijiang\n14419.33\n26878.75\nPOLYGON ((111.5307 27.81472…\n\n\nShuangfeng\n24094.50\n23188.86\nPOLYGON ((112.263 27.70421,…\n\n\nXinhua\n22019.83\n20788.14\nPOLYGON ((111.3345 28.19642…\n\n\nChengbu\n12923.50\n12365.20\nPOLYGON ((110.4455 26.69317…\n\n\nDongan\n14756.00\n15985.00\nPOLYGON ((111.4531 26.86812…\n\n\nDongkou\n13869.80\n13764.83\nPOLYGON ((110.6622 27.37305…\n\n\nLonghui\n12296.67\n11907.43\nPOLYGON ((110.985 27.65983,…\n\n\nShaodong\n15775.17\n17128.14\nPOLYGON ((111.9054 27.40254…\n\n\nSuining\n14382.86\n14593.62\nPOLYGON ((110.389 27.10006,…\n\n\nWugang\n11566.33\n11644.29\nPOLYGON ((110.9878 27.03345…\n\n\nXinning\n13199.50\n12706.00\nPOLYGON ((111.0736 26.84627…\n\n\nXinshao\n23412.00\n21712.29\nPOLYGON ((111.6013 27.58275…\n\n\nShaoshan\n39541.00\n43548.25\nPOLYGON ((112.5391 27.97742…\n\n\nXiangxiang\n36186.60\n35049.00\nPOLYGON ((112.4549 28.05783…\n\n\nBaojing\n16559.60\n16226.83\nPOLYGON ((109.7015 28.82844…\n\n\nFenghuang\n20772.50\n19294.40\nPOLYGON ((109.5239 28.19206…\n\n\nGuzhang\n19471.20\n18156.00\nPOLYGON ((109.8968 28.74034…\n\n\nHuayuan\n19827.33\n19954.75\nPOLYGON ((109.5647 28.61712…\n\n\nJishou\n15466.80\n18145.17\nPOLYGON ((109.8375 28.4696,…\n\n\nLongshan\n12925.67\n12132.75\nPOLYGON ((109.6337 29.62521…\n\n\nLuxi\n18577.17\n18419.29\nPOLYGON ((110.1067 28.41835…\n\n\nYongshun\n14943.00\n14050.83\nPOLYGON ((110.0003 29.29499…\n\n\nAnhua\n24913.00\n23619.75\nPOLYGON ((111.6034 28.63716…\n\n\nNan\n25093.00\n24552.71\nPOLYGON ((112.3232 29.46074…\n\n\nYuanjiang\n24428.80\n24733.67\nPOLYGON ((112.4391 29.1791,…\n\n\nJianghua\n17003.00\n16762.60\nPOLYGON ((111.6461 25.29661…\n\n\nLanshan\n21143.75\n20932.60\nPOLYGON ((112.2286 25.61123…\n\n\nNingyuan\n20435.00\n19467.75\nPOLYGON ((112.0715 26.09892…\n\n\nShuangpai\n17131.33\n18334.00\nPOLYGON ((111.8864 26.11957…\n\n\nXintian\n24569.75\n22541.00\nPOLYGON ((112.2578 26.0796,…\n\n\nHuarong\n23835.50\n26028.00\nPOLYGON ((112.9242 29.69134…\n\n\nLinxiang\n26360.00\n29128.50\nPOLYGON ((113.5502 29.67418…\n\n\nMiluo\n47383.40\n46569.00\nPOLYGON ((112.9902 29.02139…\n\n\nPingjiang\n55157.75\n47576.60\nPOLYGON ((113.8436 29.06152…\n\n\nXiangyin\n37058.00\n36545.50\nPOLYGON ((112.9173 28.98264…\n\n\nCili\n21546.67\n20838.50\nPOLYGON ((110.8822 29.69017…\n\n\nChaling\n23348.67\n22531.00\nPOLYGON ((113.7666 27.10573…\n\n\nLiling\n42323.67\n42115.50\nPOLYGON ((113.5673 27.94346…\n\n\nYanling\n28938.60\n27619.00\nPOLYGON ((113.9292 26.6154,…\n\n\nYou\n25880.80\n27611.33\nPOLYGON ((113.5879 27.41324…\n\n\nZhuzhou\n47345.67\n44523.29\nPOLYGON ((113.2493 28.02411…\n\n\nSangzhi\n18711.33\n18127.43\nPOLYGON ((110.556 29.40543,…\n\n\nYueyang\n29087.29\n28746.38\nPOLYGON ((113.343 29.61064,…\n\n\nQiyang\n20748.29\n20734.50\nPOLYGON ((111.5563 26.81318…\n\n\nTaojiang\n35933.71\n33880.62\nPOLYGON ((112.0508 28.67265…\n\n\nShaoyang\n15439.71\n14716.38\nPOLYGON ((111.5013 27.30207…\n\n\nLianyuan\n29787.50\n28516.22\nPOLYGON ((111.6789 28.02946…\n\n\nHongjiang\n18145.00\n18086.14\nPOLYGON ((110.1441 27.47513…\n\n\nHengyang\n21617.00\n21244.50\nPOLYGON ((112.7144 26.98613…\n\n\nGuiyang\n29203.89\n29568.80\nPOLYGON ((113.0811 26.04963…\n\n\nChangsha\n41363.67\n48119.71\nPOLYGON ((112.9421 28.03722…\n\n\nTaoyuan\n22259.09\n22310.75\nPOLYGON ((112.0612 29.32855…\n\n\nXiangtan\n44939.56\n43151.60\nPOLYGON ((113.0426 27.8942,…\n\n\nDao\n16902.00\n17133.40\nPOLYGON ((111.498 25.81679,…\n\n\nJiangyong\n16930.00\n17009.33\nPOLYGON ((111.3659 25.39472…\n\n\n\n\n\nLastly, qtm() of tmap package is used to plot the lag_gdppc and w_ave_gdppc maps next to each other for quick comparison.\n\nw_avg_gdppc &lt;- qtm(hunan, \"lag_window_avg GDPPC\")\ntmap_arrange(lag_gdppc, w_avg_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFor more effective comparison, it is advisable to use the core tmap mapping functions.\n\n\n\n\n13.4 Spatial Window Sum\n\nThe spatial window sum is the counter part of the window average, but without using row-standardized weights.\n\nTo add the diagonal element to the neighbour list, we just need to use include.self() from spdep.\n\nwm_qs &lt;- include.self(wm_q)\nwm_qs\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\n\nNext, we will assign binary weights to the neighbour structure that includes the diagonal element.\n\nb_weights &lt;- lapply(wm_qs, function(x) 0*x + 1)\nb_weights[1]\n\n[[1]]\n[1] 1 1 1 1 1 1\n\n\nNotice that now region 85 has 12 neighbours instead of 11.\nAgain, we use nb2listw() and glist() to explicitly assign weight values.\n\nb_weights2 &lt;- nb2listw(wm_qs, \n                       glist = b_weights, \n                       style = \"B\")\nb_weights2\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 88 7744 536 1072 14160\n\n\nWith our new weight structure, we can compute the lag variable with lag.listw().\n\nw_sum_gdppc &lt;- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))\nw_sum_gdppc\n\n[[1]]\n [1] \"Anxiang\"       \"Hanshou\"       \"Jinshi\"        \"Li\"           \n [5] \"Linli\"         \"Shimen\"        \"Liuyang\"       \"Ningxiang\"    \n [9] \"Wangcheng\"     \"Anren\"         \"Guidong\"       \"Jiahe\"        \n[13] \"Linwu\"         \"Rucheng\"       \"Yizhang\"       \"Yongxing\"     \n[17] \"Zixing\"        \"Changning\"     \"Hengdong\"      \"Hengnan\"      \n[21] \"Hengshan\"      \"Leiyang\"       \"Qidong\"        \"Chenxi\"       \n[25] \"Zhongfang\"     \"Huitong\"       \"Jingzhou\"      \"Mayang\"       \n[29] \"Tongdao\"       \"Xinhuang\"      \"Xupu\"          \"Yuanling\"     \n[33] \"Zhijiang\"      \"Lengshuijiang\" \"Shuangfeng\"    \"Xinhua\"       \n[37] \"Chengbu\"       \"Dongan\"        \"Dongkou\"       \"Longhui\"      \n[41] \"Shaodong\"      \"Suining\"       \"Wugang\"        \"Xinning\"      \n[45] \"Xinshao\"       \"Shaoshan\"      \"Xiangxiang\"    \"Baojing\"      \n[49] \"Fenghuang\"     \"Guzhang\"       \"Huayuan\"       \"Jishou\"       \n[53] \"Longshan\"      \"Luxi\"          \"Yongshun\"      \"Anhua\"        \n[57] \"Nan\"           \"Yuanjiang\"     \"Jianghua\"      \"Lanshan\"      \n[61] \"Ningyuan\"      \"Shuangpai\"     \"Xintian\"       \"Huarong\"      \n[65] \"Linxiang\"      \"Miluo\"         \"Pingjiang\"     \"Xiangyin\"     \n[69] \"Cili\"          \"Chaling\"       \"Liling\"        \"Yanling\"      \n[73] \"You\"           \"Zhuzhou\"       \"Sangzhi\"       \"Yueyang\"      \n[77] \"Qiyang\"        \"Taojiang\"      \"Shaoyang\"      \"Lianyuan\"     \n[81] \"Hongjiang\"     \"Hengyang\"      \"Guiyang\"       \"Changsha\"     \n[85] \"Taoyuan\"       \"Xiangtan\"      \"Dao\"           \"Jiangyong\"    \n\n[[2]]\n [1] 147903 134605 131165 135423 134635 133381 238106 297281 344573 268982\n[11] 106510 136141 126832 103303 151645 196097 207589 143926 178242 175235\n[21] 138765 155699 160150 117145 113730  89002  63532 112988  59330  35930\n[31] 154439 145795 112587 107515 162322 145517  61826  79925  82589  83352\n[41] 119897 116749  81510  63530 151986 174193 210294  97361  96472 108936\n[51]  79819 108871  48531 128935  84305 188958 171869 148402  83813 104663\n[61] 155742  73336 112705  78084  58257 279414 237883 219273  83354  90124\n[71] 168462 165714 165668 311663 126892 229971 165876 271045 117731 256646\n[81] 126603 127467 295688 336838 267729 431516  85667  51028\n\n\nNext, we will convert the lag variable listw object into a data.frame by using as.data.frame().\n\nw_sum_gdppc.res &lt;- as.data.frame(w_sum_gdppc)\ncolnames(w_sum_gdppc.res) &lt;- c(\"NAME_3\", \"w_sum GDPPC\")\n\nNote: The second command line on the code chunk above renames the field names of w_sum_gdppc.res object into NAME_3 and w_sum GDPPC respectively.\nNext, the code chunk below will be used to append w_sum GDPPC values onto hunan sf data.frame by using left_join() of dplyr package.\n\nhunan &lt;- left_join(hunan, w_sum_gdppc.res)\n\nTo compare the values of lag GDPPC and Spatial window average, kable() of Knitr package is used to prepare a table using the code chunk below.\n\nhunan %&gt;%\n  select(\"County\", \"lag_sum GDPPC\", \"w_sum GDPPC\") %&gt;%\n  kable()\n\n\n\n\n\n\n\n\n\n\nCounty\nlag_sum GDPPC\nw_sum GDPPC\ngeometry\n\n\n\n\nAnxiang\n124236\n147903\nPOLYGON ((112.0625 29.75523…\n\n\nHanshou\n113624\n134605\nPOLYGON ((112.2288 29.11684…\n\n\nJinshi\n96573\n131165\nPOLYGON ((111.8927 29.6013,…\n\n\nLi\n110950\n135423\nPOLYGON ((111.3731 29.94649…\n\n\nLinli\n109081\n134635\nPOLYGON ((111.6324 29.76288…\n\n\nShimen\n106244\n133381\nPOLYGON ((110.8825 30.11675…\n\n\nLiuyang\n174988\n238106\nPOLYGON ((113.9905 28.5682,…\n\n\nNingxiang\n235079\n297281\nPOLYGON ((112.7181 28.38299…\n\n\nWangcheng\n273907\n344573\nPOLYGON ((112.7914 28.52688…\n\n\nAnren\n256221\n268982\nPOLYGON ((113.1757 26.82734…\n\n\nGuidong\n98013\n106510\nPOLYGON ((114.1799 26.20117…\n\n\nJiahe\n104050\n136141\nPOLYGON ((112.4425 25.74358…\n\n\nLinwu\n102846\n126832\nPOLYGON ((112.5914 25.55143…\n\n\nRucheng\n92017\n103303\nPOLYGON ((113.6759 25.87578…\n\n\nYizhang\n133831\n151645\nPOLYGON ((113.2621 25.68394…\n\n\nYongxing\n158446\n196097\nPOLYGON ((113.3169 26.41843…\n\n\nZixing\n141883\n207589\nPOLYGON ((113.7311 26.16259…\n\n\nChangning\n119508\n143926\nPOLYGON ((112.6144 26.60198…\n\n\nHengdong\n150757\n178242\nPOLYGON ((113.1056 27.21007…\n\n\nHengnan\n153324\n175235\nPOLYGON ((112.7599 26.98149…\n\n\nHengshan\n113593\n138765\nPOLYGON ((112.607 27.4689, …\n\n\nLeiyang\n129594\n155699\nPOLYGON ((112.9996 26.69276…\n\n\nQidong\n142149\n160150\nPOLYGON ((111.7818 27.0383,…\n\n\nChenxi\n100119\n117145\nPOLYGON ((110.2624 28.21778…\n\n\nZhongfang\n82884\n113730\nPOLYGON ((109.9431 27.72858…\n\n\nHuitong\n74668\n89002\nPOLYGON ((109.9419 27.10512…\n\n\nJingzhou\n43184\n63532\nPOLYGON ((109.8186 26.75842…\n\n\nMayang\n99244\n112988\nPOLYGON ((109.795 27.98008,…\n\n\nTongdao\n46549\n59330\nPOLYGON ((109.9294 26.46561…\n\n\nXinhuang\n20518\n35930\nPOLYGON ((109.227 27.43733,…\n\n\nXupu\n140576\n154439\nPOLYGON ((110.7189 28.30485…\n\n\nYuanling\n121601\n145795\nPOLYGON ((110.9652 28.99895…\n\n\nZhijiang\n92069\n112587\nPOLYGON ((109.8818 27.60661…\n\n\nLengshuijiang\n43258\n107515\nPOLYGON ((111.5307 27.81472…\n\n\nShuangfeng\n144567\n162322\nPOLYGON ((112.263 27.70421,…\n\n\nXinhua\n132119\n145517\nPOLYGON ((111.3345 28.19642…\n\n\nChengbu\n51694\n61826\nPOLYGON ((110.4455 26.69317…\n\n\nDongan\n59024\n79925\nPOLYGON ((111.4531 26.86812…\n\n\nDongkou\n69349\n82589\nPOLYGON ((110.6622 27.37305…\n\n\nLonghui\n73780\n83352\nPOLYGON ((110.985 27.65983,…\n\n\nShaodong\n94651\n119897\nPOLYGON ((111.9054 27.40254…\n\n\nSuining\n100680\n116749\nPOLYGON ((110.389 27.10006,…\n\n\nWugang\n69398\n81510\nPOLYGON ((110.9878 27.03345…\n\n\nXinning\n52798\n63530\nPOLYGON ((111.0736 26.84627…\n\n\nXinshao\n140472\n151986\nPOLYGON ((111.6013 27.58275…\n\n\nShaoshan\n118623\n174193\nPOLYGON ((112.5391 27.97742…\n\n\nXiangxiang\n180933\n210294\nPOLYGON ((112.4549 28.05783…\n\n\nBaojing\n82798\n97361\nPOLYGON ((109.7015 28.82844…\n\n\nFenghuang\n83090\n96472\nPOLYGON ((109.5239 28.19206…\n\n\nGuzhang\n97356\n108936\nPOLYGON ((109.8968 28.74034…\n\n\nHuayuan\n59482\n79819\nPOLYGON ((109.5647 28.61712…\n\n\nJishou\n77334\n108871\nPOLYGON ((109.8375 28.4696,…\n\n\nLongshan\n38777\n48531\nPOLYGON ((109.6337 29.62521…\n\n\nLuxi\n111463\n128935\nPOLYGON ((110.1067 28.41835…\n\n\nYongshun\n74715\n84305\nPOLYGON ((110.0003 29.29499…\n\n\nAnhua\n174391\n188958\nPOLYGON ((111.6034 28.63716…\n\n\nNan\n150558\n171869\nPOLYGON ((112.3232 29.46074…\n\n\nYuanjiang\n122144\n148402\nPOLYGON ((112.4391 29.1791,…\n\n\nJianghua\n68012\n83813\nPOLYGON ((111.6461 25.29661…\n\n\nLanshan\n84575\n104663\nPOLYGON ((112.2286 25.61123…\n\n\nNingyuan\n143045\n155742\nPOLYGON ((112.0715 26.09892…\n\n\nShuangpai\n51394\n73336\nPOLYGON ((111.8864 26.11957…\n\n\nXintian\n98279\n112705\nPOLYGON ((112.2578 26.0796,…\n\n\nHuarong\n47671\n78084\nPOLYGON ((112.9242 29.69134…\n\n\nLinxiang\n26360\n58257\nPOLYGON ((113.5502 29.67418…\n\n\nMiluo\n236917\n279414\nPOLYGON ((112.9902 29.02139…\n\n\nPingjiang\n220631\n237883\nPOLYGON ((113.8436 29.06152…\n\n\nXiangyin\n185290\n219273\nPOLYGON ((112.9173 28.98264…\n\n\nCili\n64640\n83354\nPOLYGON ((110.8822 29.69017…\n\n\nChaling\n70046\n90124\nPOLYGON ((113.7666 27.10573…\n\n\nLiling\n126971\n168462\nPOLYGON ((113.5673 27.94346…\n\n\nYanling\n144693\n165714\nPOLYGON ((113.9292 26.6154,…\n\n\nYou\n129404\n165668\nPOLYGON ((113.5879 27.41324…\n\n\nZhuzhou\n284074\n311663\nPOLYGON ((113.2493 28.02411…\n\n\nSangzhi\n112268\n126892\nPOLYGON ((110.556 29.40543,…\n\n\nYueyang\n203611\n229971\nPOLYGON ((113.343 29.61064,…\n\n\nQiyang\n145238\n165876\nPOLYGON ((111.5563 26.81318…\n\n\nTaojiang\n251536\n271045\nPOLYGON ((112.0508 28.67265…\n\n\nShaoyang\n108078\n117731\nPOLYGON ((111.5013 27.30207…\n\n\nLianyuan\n238300\n256646\nPOLYGON ((111.6789 28.02946…\n\n\nHongjiang\n108870\n126603\nPOLYGON ((110.1441 27.47513…\n\n\nHengyang\n108085\n127467\nPOLYGON ((112.7144 26.98613…\n\n\nGuiyang\n262835\n295688\nPOLYGON ((113.0811 26.04963…\n\n\nChangsha\n248182\n336838\nPOLYGON ((112.9421 28.03722…\n\n\nTaoyuan\n244850\n267729\nPOLYGON ((112.0612 29.32855…\n\n\nXiangtan\n404456\n431516\nPOLYGON ((113.0426 27.8942,…\n\n\nDao\n67608\n85667\nPOLYGON ((111.498 25.81679,…\n\n\nJiangyong\n33860\n51028\nPOLYGON ((111.3659 25.39472…\n\n\n\n\n\nLastly, qtm() of tmap package is used to plot the lag_sum GDPPC and w_sum_gdppc maps next to each other for quick comparison.\n\nw_sum_gdppc &lt;- qtm(hunan, \"w_sum GDPPC\")\ntmap_arrange(lag_sum_gdppc, w_sum_gdppc, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03a.html",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03a.html",
    "title": "3A: Network Constrained Spatial Point Patterns Analysis",
    "section": "",
    "text": "R for Geospatial Data Science and Analytics - 7  Network Constrained Spatial Point Patterns Analysis"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03a.html#exercise-3a-reference",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03a.html#exercise-3a-reference",
    "title": "3A: Network Constrained Spatial Point Patterns Analysis",
    "section": "",
    "text": "R for Geospatial Data Science and Analytics - 7  Network Constrained Spatial Point Patterns Analysis"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03a.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03a.html#overview",
    "title": "3A: Network Constrained Spatial Point Patterns Analysis",
    "section": "2 Overview",
    "text": "2 Overview\n\n\n\n\n\n\nNote\n\n\n\nWhat is NetKDE and Why is it important? Network Constrained Kernel Density Estimation (NetKDE) is an advanced spatial analysis technique used to estimate the density of spatial events (such as crime incidents, traffic accidents, wildlife sightings, etc.) while accounting for the underlying network structure, such as roads, railways, or rivers. Unlike traditional Kernel Density Estimation (KDE), which assumes that events are distributed freely in a 2D plane, NetKDE restricts the analysis to a network, providing a more accurate representation when events are constrained to specific pathways or routes.\nNetKDE provides a more realistic density estimation for data constrained to a network, avoiding misleading results that might arise from traditional KDE. For example, traffic accidents or crime hotspots along a road network are better analyzed using NetKDE since it restricts the analysis to the roads themselves.\n\n\nNetwork constrained Spatial Point Patterns Analysis (NetSPAA) is a collection of spatial point patterns analysis methods special developed for analysing spatial point event occurs on or alongside network. The spatial point event can be locations of traffic accident or childcare centre for example. The network, on the other hand can be a road network or river network.\nIn this hands-on exercise, you are going to gain hands-on experience on using appropriate functions of spNetwork package:\n\nto derive network kernel density estimation (NKDE), and\nto perform network G-function and k-function analysis"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03a.html#learning-outcome",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03a.html#learning-outcome",
    "title": "3A: Network Constrained Spatial Point Patterns Analysis",
    "section": "3 Learning Outcome",
    "text": "3 Learning Outcome\n\nUnderstand and perform Network Constrained Spatial Point Patterns Analysis (NetSPAA) for events on networks (e.g., traffic accidents, childcare centers).\nUse spNetwork to derive network kernel density estimation (NKDE) for spatial analysis.\nConduct network G- and K-function analyses to test for complete spatial randomness (CSR).\nVisualize geospatial data using tmap for interactive and high-quality mapping.\nPrepare data by importing geospatial datasets using the sf package and managing CRS information.\nUse lixelize_lines() to cut lines into lixels for NKDE analysis.\nApply NKDE methods (simple, discontinuous, continuous) to analyze point patterns on networks.\nVisualize NKDE results by rescaling densities for effective mapping.\nPerform CSR tests using the kfunctions() from spNetwork to analyze spatial interactions among events."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03a.html#the-data",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03a.html#the-data",
    "title": "3A: Network Constrained Spatial Point Patterns Analysis",
    "section": "4 The Data",
    "text": "4 The Data\nThis study will analyze the spatial distribution of childcare centers in the Punggol planning area using the following geospatial datasets:\n\n\n\n\n\n\n\n\nDataset\nDescription\nFormat\n\n\n\n\nPunggol_St\nLine feature data representing the road network within Punggol Planning Area.\nESRI Shapefile\n\n\nPunggol_CC\nPoint feature data representing the location of childcare centers within Punggol Planning Area.\nESRI Shapefile"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03a.html#installing-and-launching-the-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03a.html#installing-and-launching-the-r-packages",
    "title": "3A: Network Constrained Spatial Point Patterns Analysis",
    "section": "5 Installing and Launching the R Packages",
    "text": "5 Installing and Launching the R Packages\nThe following R packages will be used in this exercise:\n\n\n\n\n\n\n\n\nPackage\nPurpose\nUse Case in Exercise\n\n\n\n\nspNetwork\nProvides functions for Spatial Point Patterns Analysis (e.g., KDE, K-function) on networks and spatial matrix building for spatial analysis.\nConducting spatial point pattern analysis and building spatial weights based on network distances.\n\n\nsf\nOffers functions to manage, process, and manipulate Simple Features for geospatial data handling.\nHandling and processing geospatial data in Simple Features format.\n\n\ntmap\nCreates cartographic quality static and interactive maps using the Leaflet API.\nPlotting high-quality static and interactive maps for spatial analysis.\n\n\n\nTo install and load these packages, use the following code:\n\npacman::p_load(sf, spNetwork, tmap, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03a.html#import-data-and-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03a.html#import-data-and-preparation",
    "title": "3A: Network Constrained Spatial Point Patterns Analysis",
    "section": "6 Import Data and Preparation",
    "text": "6 Import Data and Preparation\nThe code block below uses st_read() of sf package to important Punggol_St and Punggol_CC geospatial data sets into RStudio as sf data frames.\n\nnetwork &lt;- st_read(dsn=\"data/geospatial\", \n                   layer=\"Punggol_St\")\n\nReading layer `Punggol_St' from data source \n  `/Users/walter/code/isss626/isss626-gaa/Hands-on_Ex/Hands-on_Ex03/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 2642 features and 2 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 34038.56 ymin: 40941.11 xmax: 38882.85 ymax: 44801.27\nProjected CRS: SVY21 / Singapore TM\n\n\n\nchildcare &lt;- st_read(dsn=\"data/geospatial\",\n                     layer=\"Punggol_CC\")\n\nReading layer `Punggol_CC' from data source \n  `/Users/walter/code/isss626/isss626-gaa/Hands-on_Ex/Hands-on_Ex03/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 61 features and 1 field\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 34423.98 ymin: 41503.6 xmax: 37619.47 ymax: 44685.77\nz_range:       zmin: 0 zmax: 0\nProjected CRS: SVY21 / Singapore TM\n\n\n\n6.1 Examine Data Content\nWe can examine the structure of the output simple features data tables in RStudio. Alternatively, we can print basic information on the data as shown below.\n\nNetworkChildcare\n\n\n\nnetwork\n\nSimple feature collection with 2642 features and 2 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 34038.56 ymin: 40941.11 xmax: 38882.85 ymax: 44801.27\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n     LINK_ID                   ST_NAME                       geometry\n1  116130894                PUNGGOL RD LINESTRING (36546.89 44574....\n2  116130897 PONGGOL TWENTY-FOURTH AVE LINESTRING (36546.89 44574....\n3  116130901   PONGGOL SEVENTEENTH AVE LINESTRING (36012.73 44154....\n4  116130902   PONGGOL SEVENTEENTH AVE LINESTRING (36062.81 44197....\n5  116130907           PUNGGOL CENTRAL LINESTRING (36131.85 42755....\n6  116130908                PUNGGOL RD LINESTRING (36112.93 42752....\n7  116130909           PUNGGOL CENTRAL LINESTRING (36127.4 42744.5...\n8  116130910               PUNGGOL FLD LINESTRING (35994.98 42428....\n9  116130911               PUNGGOL FLD LINESTRING (35984.97 42407....\n10 116130912            EDGEFIELD PLNS LINESTRING (36200.87 42219....\n\n\n\ndim(network)\n\n[1] 2642    3\n\n\n\nstr(network)\n\nClasses 'sf' and 'data.frame':  2642 obs. of  3 variables:\n $ LINK_ID : num  1.16e+08 1.16e+08 1.16e+08 1.16e+08 1.16e+08 ...\n $ ST_NAME : chr  \"PUNGGOL RD\" \"PONGGOL TWENTY-FOURTH AVE\" \"PONGGOL SEVENTEENTH AVE\" \"PONGGOL SEVENTEENTH AVE\" ...\n $ geometry:sfc_LINESTRING of length 2642; first list element:  'XY' num [1:2, 1:2] 36547 36559 44575 44614\n - attr(*, \"sf_column\")= chr \"geometry\"\n - attr(*, \"agr\")= Factor w/ 3 levels \"constant\",\"aggregate\",..: NA NA\n  ..- attr(*, \"names\")= chr [1:2] \"LINK_ID\" \"ST_NAME\"\n\n\n\nst_crs(network)\n\nCoordinate Reference System:\n  User input: SVY21 / Singapore TM \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\n\n\n\nchildcare\n\nSimple feature collection with 61 features and 1 field\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 34423.98 ymin: 41503.6 xmax: 37619.47 ymax: 44685.77\nz_range:       zmin: 0 zmax: 0\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n      Name                      geometry\n1   kml_10 POINT Z (36173.81 42550.33 0)\n2   kml_99 POINT Z (36479.56 42405.21 0)\n3  kml_100 POINT Z (36618.72 41989.13 0)\n4  kml_101 POINT Z (36285.37 42261.42 0)\n5  kml_122  POINT Z (35414.54 42625.1 0)\n6  kml_161 POINT Z (36545.16 42580.09 0)\n7  kml_172 POINT Z (35289.44 44083.57 0)\n8  kml_188 POINT Z (36520.56 42844.74 0)\n9  kml_205  POINT Z (36924.01 41503.6 0)\n10 kml_222 POINT Z (37141.76 42326.36 0)\n\n\n\ndim(childcare)\n\n[1] 61  2\n\n\n\nstr(childcare)\n\nClasses 'sf' and 'data.frame':  61 obs. of  2 variables:\n $ Name    : chr  \"kml_10\" \"kml_99\" \"kml_100\" \"kml_101\" ...\n $ geometry:sfc_POINT of length 61; first list element:  'XYZ' num  36174 42550 0\n - attr(*, \"sf_column\")= chr \"geometry\"\n - attr(*, \"agr\")= Factor w/ 3 levels \"constant\",\"aggregate\",..: NA\n  ..- attr(*, \"names\")= chr \"Name\"\n\n\n\nst_crs(childcare)\n\nCoordinate Reference System:\n  User input: SVY21 / Singapore TM \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03a.html#visualising-geospatial-data",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03a.html#visualising-geospatial-data",
    "title": "3A: Network Constrained Spatial Point Patterns Analysis",
    "section": "7 Visualising Geospatial Data",
    "text": "7 Visualising Geospatial Data\nTo visualise geospatial data, we can use plot() from base R. Alternatively, we can visualise the geospatial data with high cartographic quality and interactive manner using the tmap package.\n\nUsing PlotUsing Tmap\n\n\n\nplot(st_geometry(network))\n\nplot(childcare,\n     add=T,\n     col='red',\n     pch = 19)\n\n\n\n\n\n\n\n\n\n\n\ntmap_mode('view')\n\ntm_shape(childcare) + \n  tm_dots(col = 'red') + \n  tm_shape(network) +\n  tm_lines()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03a.html#network-constrained-kde-nkde-analysis-using-spnetwork",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03a.html#network-constrained-kde-nkde-analysis-using-spnetwork",
    "title": "3A: Network Constrained Spatial Point Patterns Analysis",
    "section": "8 Network Constrained KDE (NKDE) Analysis using spNetwork",
    "text": "8 Network Constrained KDE (NKDE) Analysis using spNetwork\nIn this section, we will perform NKDE analysis by using appropriate functions provided in spNetwork package.\n\n8.1 Preparing the Lixels Objects\nBefore computing NKDE, the SpatialLines object need to be cut into lixels with a specified minimal distance. This task can be performed by using with lixelize_lines() of spNetwork as shown in the code block below.\n\nlixels &lt;- lixelize_lines(lines=network, \n                         lx_length=700, \n                         mindist = 375)\n\n\n\n\n\n\n\nNote\n\n\n\nArguments\n\nlines: The sf object with linestring geometry type to modify\nlx_length: The length of a lixel\nmindist: The minimum length of a lixel. After cut, if the length of the final lixel is shorter than the minimum distance, then it is added to the previous lixel. if NULL, then mindist = maxdist/10. Note that the segments that are already shorter than the minimum distance are not modified\n\nThere is another function called lixelize_lines.mc() which provide multicore support.\n\n\n\n\n8.2 Generating Line Centre Points\nNext, we will use lines_center() of spNetwork to generate a SpatialPointsDataFrame (i.e. samples) with line centre points.\n\nsamples &lt;- lines_center(lixels)\n\nThe points are located at center of the line based on the length of the line.\n\n\n8.3 Performing NKDE\nTo compute the NKDE:\n\ndensities &lt;- nkde(network, \n                  events = childcare,\n                  w = rep(1, nrow(childcare)),\n                  samples = samples,\n                  kernel_name = \"quartic\",\n                  bw = 300, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(1,1), \n                  max_depth = 8,\n                  # agg = 5, \n                  sparse = TRUE,\n                  verbose = FALSE)\n\n\n\n\n\n\n\nNote\n\n\n\n\nspNetwork supports various kernel methods, including quartic, triangle, gaussian, scaled gaussian, tricube, cosine, triweight, epanechnikov, or uniform. In this case, quartic kernel is used.\nmethod argument indicates that simple method is used to calculate the NKDE. Currently, spNetwork support three popular methods, they are:\n\nmethod=“simple”. This first method was presented by Xie et al. (2008) and proposes an intuitive solution. The distances between events and sampling points are replaced by network distances, and the formula of the kernel is adapted to calculate the density over a linear unit instead of an areal unit.\nmethod=“discontinuous”. The method is proposed by Okabe et al (2008), which equally “divides” the mass density of an event at intersections of lixels.\nmethod=“continuous”. If the discontinuous method is unbiased, it leads to a discontinuous kernel function which is a bit counter-intuitive. Okabe et al (2008) proposed another version of the kernel, that divide the mass of the density at intersection but adjusts the density before the intersection to make the function continuous.\n\n\nFor more info, refer to the user guide of spNetwork package.\n\n\n\n\n8.4 Visualising NKDE\nTo visualise the NKDE values, we have to perform a few preparation steps.\n\nInsert the computed density values (i.e. densities) into samples and lixels objects as density field.\n\n\nsamples$density &lt;- densities\nlixels$density &lt;- densities\n\n\nrescale the density values if required.\n\nSince svy21 projection system is in meter, the computed density values are very small i.e. 0.0000005. We should rescale the density values from number of events per meter to number of events per kilometer.\n\nsamples$density &lt;- samples$density*1000\nlixels$density &lt;- lixels$density*1000\n\n\nUse tmap to plot interactive map\n\n\ntmap_mode('view')\ntm_shape(lixels)+\n  tm_lines(col=\"density\")+\ntm_shape(childcare)+\n  tm_dots()\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nRoad segments with relatively higher density of childcare centres is shown in darker color (refer to legend). Road segments with relatively lower density of childcare centre is shown in lighter color."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03a.html#network-constrained-g--and-k-function-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03a.html#network-constrained-g--and-k-function-analysis",
    "title": "3A: Network Constrained Spatial Point Patterns Analysis",
    "section": "9 Network Constrained G- and K-Function Analysis",
    "text": "9 Network Constrained G- and K-Function Analysis\nIn this section, we are going to perform Complete Spatial Randomness (CSR) test by using kfunctions() of spNetwork package.\nThe CSR test is based on the assumption of the binomial point process which implies the hypothesis that the childcare centres are randomly and independently distributed over the street network.\nNull Hypothesis (\\(H_0\\)): The observed spatial point events (i.e., distribution of childcare centres) exhibit a uniform distribution over a street network in Punggol Planning Area.\nIf this hypothesis is rejected, we may infer that the distribution of childcare centres are spatially interacting and dependent on each other; as a result, they may form non-random patterns.\n\n# set seed for reproducibility\nset.seed(1234)\nkfun_childcare &lt;- kfunctions(network, \n                             childcare,\n                             start = 0, \n                             end = 1000, \n                             step = 50, \n                             width = 50, \n                             nsim = 39, \n                             resolution = 50,\n                             verbose = FALSE, \n                             conf_int = 0.05)\n\n\n\n\n\n\n\nNote\n\n\n\nExplanation on Arguments used\n\nlines: A SpatialLinesDataFrame with sampling points.\npoints: A SpatialPointsDataFrame representing points on the network.\nstart: Start value for evaluating the k and g functions.\nend: Last value for evaluating the k and g functions.\nstep: Jump between two evaluations of the k and g functions.\nwidth: Width of each donut for the g-function.\nnsim: Number of Monte Carlo simulations (39 simulations in this example, more simulation may be required for inference).\nresolution: Resolution for simulating random points on the network.\nconf_int: Width of the confidence interval (default = 0.05).\nFor additional arguments, refer to the user guide of the spNetwork package.\n\n\n\nWe can visualise the ggplot2 object of k-function by using the code chunk below.\n\nkfun_childcare$plotk + \n  labs(title =\"K-Function\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIntepretation of kfunctions() outputs\n\nplotk: A ggplot2 object representing the values of the k-function\nplotg: A ggplot2 object representing the values of the g-function\nvalues: A DataFrame with the values used to build the plots\n\nsee kfunctions function - RDocumentation\nIntepretation of the graph output\nThe blue line is the empirical network K-function of the childcare centres in Punggol planning area. The gray envelope represents the results of the 39 simulations in the interval 2.5% - 97.5%.\nSince the blue line between the distance of 250m-400m are below the gray area, we can infer that the childcare centres in Punggol planning area resemble regular pattern at the distance of 250m-400m."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02b.html",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02b.html",
    "title": "2B: 2nd Order Spatial Point Patterns Analysis",
    "section": "",
    "text": "R for Geospatial Data Science and Analytics - 5  2nd Order Spatial Point Patterns Analysis Methods"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02b.html#exercise-2b-reference",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02b.html#exercise-2b-reference",
    "title": "2B: 2nd Order Spatial Point Patterns Analysis",
    "section": "",
    "text": "R for Geospatial Data Science and Analytics - 5  2nd Order Spatial Point Patterns Analysis Methods"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02b.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02b.html#overview",
    "title": "2B: 2nd Order Spatial Point Patterns Analysis",
    "section": "2 Overview",
    "text": "2 Overview\nSpatial Point Pattern Analysis is the evaluation of the pattern or distribution, of a set of points on a surface. The point can be location of:\n\nevents such as crime, traffic accident and disease onset, or\nbusiness services (coffee and fastfood outlets) or facilities such as childcare and eldercare.\n\nUsing appropriate functions of spatstat, this hands-on exercise aims to discover the spatial point processes of childecare centres in Singapore.\nThe specific questions we would like to answer are as follows:\n\nare the childcare centres in Singapore randomly distributed throughout the country?\n\nif the answer is not, then the next logical question is where are the locations with higher concentration of childcare centres?\n\nThis hands-on exercise continues from Hands-on Exercise 2A"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02b.html#learning-outcome",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02b.html#learning-outcome",
    "title": "2B: 2nd Order Spatial Point Patterns Analysis",
    "section": "3 Learning Outcome",
    "text": "3 Learning Outcome\n\nImporting and managing geospatial data using sf and spatstat packages.\nConverting spatial data formats from sf to spatstat’s ppp format.\nHandling and correcting duplicate spatial points in datasets.\nDefining and applying spatial windows (owin objects) for focused analysis.\nConducting 2nd-order spatial point pattern analyses using G, F, K, and L functions.\nPerforming Monte Carlo simulations and hypothesis testing to assess spatial randomness.\nVisualizing spatial patterns and statistical results using appropriate plotting functions."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02b.html#the-data",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02b.html#the-data",
    "title": "2B: 2nd Order Spatial Point Patterns Analysis",
    "section": "4 The Data",
    "text": "4 The Data\n\n\n\n\n\n\n\n\n\nDataset\nDescription\nSource\nFormat\n\n\n\n\nCHILDCARE\nPoint data containing location and attributes of childcare centers.\nData.gov.sg\nGeoJSON\n\n\nMP14_SUBZONE_WEB_PL\nPolygon data with URA 2014 Master Plan Planning Subzone boundaries.\nData.gov.sg\nESRI Shapefile\n\n\nCoastalOutline\nPolygon data representing Singapore’s national boundary.\nSingapore Land Authority (SLA)\nESRI Shapefile"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02b.html#installing-and-loading-the-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02b.html#installing-and-loading-the-r-packages",
    "title": "2B: 2nd Order Spatial Point Patterns Analysis",
    "section": "5 Installing and Loading the R Packages",
    "text": "5 Installing and Loading the R Packages\nThe following R packages will be used in this exercise:\n\n\n\n\n\n\n\n\nPackage\nPurpose\nUse Case in Exercise\n\n\n\n\nsf\nImports, manages, and processes vector-based geospatial data.\nHandling vector geospatial data in R.\n\n\nspatstat\nProvides tools for point pattern analysis.\nPerforming 1st- and 2nd-order spatial point pattern analysis and deriving kernel density estimation (KDE).\n\n\nraster\nReads, writes, and manipulates gridded spatial data (raster).\nConverting image outputs from spatstat into raster format.\n\n\nmaptools\nOffers tools for manipulating geographic data.\nConverting spatial objects into ppp format for use with spatstat.\n\n\ntmap\nCreates static and interactive thematic maps using cartographic quality elements.\nPlotting static and interactive point pattern maps.\n\n\n\nTo install and load these packages in R, use the following code:\n\npacman::p_load(sf, raster, spatstat, tmap, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02b.html#reproducibility",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02b.html#reproducibility",
    "title": "2B: 2nd Order Spatial Point Patterns Analysis",
    "section": "6 Reproducibility",
    "text": "6 Reproducibility\nAs this document involves Monte Carlo simulations, we will set the seed to ensure reproducibility\n\nset.seed(1234)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02b.html#spatial-data-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02b.html#spatial-data-wrangling",
    "title": "2B: 2nd Order Spatial Point Patterns Analysis",
    "section": "7 Spatial Data Wrangling",
    "text": "7 Spatial Data Wrangling\n\n7.1 Importing the Spatial Data\nTo import the three geographical datasets, we will use st_read() from sf.\n\nchildcare_sf &lt;- st_read(\"data/child-care-services-geojson.geojson\")\n\nReading layer `child-care-services-geojson' from data source \n  `/Users/walter/code/isss626/isss626-gaa/Hands-on_Ex/Hands-on_Ex02/data/child-care-services-geojson.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1545 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6824 ymin: 1.248403 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\nsg_sf &lt;- st_read(dsn = \"data\", layer=\"CostalOutline\")\n\nReading layer `CostalOutline' from data source \n  `/Users/walter/code/isss626/isss626-gaa/Hands-on_Ex/Hands-on_Ex02/data' \n  using driver `ESRI Shapefile'\nSimple feature collection with 60 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2663.926 ymin: 16357.98 xmax: 56047.79 ymax: 50244.03\nProjected CRS: SVY21\n\n\n\nmpsz_sf &lt;- st_read(dsn = \"data\",\n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/walter/code/isss626/isss626-gaa/Hands-on_Ex/Hands-on_Ex02/data' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\n\n7.2 Inspect and Reproject to Same Projection System\nBefore we can use these data for analysis, it is important for us to ensure that they are projected in same projection system.\nFirst, we check the childcare dataset.\n\nst_crs(childcare_sf)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n\nThis dataset is using the WGS84 crs. We will reproject all the dataset to SVY21 crs for standardization and analysis.\n\nchildcare_sf &lt;- st_transform(childcare_sf , crs = 3414)\nst_crs(childcare_sf)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nThe childcare dataset has been reprojected to SVY21 successfully.\nNext, we inspect the Coastal Outline dataset.\n\nst_crs(sg_sf)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\nNotice that this dataset is using SVY21 crs, however the ID provided is EPSG:9001 does not match the intended ID, EPSG:3414 of SVY21. In this case, we will set the crs to the correct ID using the code block below.\n\nsg_sf &lt;- st_set_crs(sg_sf, 3414)\nst_crs(sg_sf)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nSimilarly, we will inspect the Master Plan Subzone Dataset.\n\nst_crs(mpsz_sf)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\nSince the ID is also EPSG:9001, we will set the crs to EPSG:3414 too.\n\nmpsz_sf &lt;- st_set_crs(mpsz_sf, 3414)\nst_crs(mpsz_sf)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02b.html#mapping-the-geospatial-datasets",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02b.html#mapping-the-geospatial-datasets",
    "title": "2B: 2nd Order Spatial Point Patterns Analysis",
    "section": "8 Mapping the Geospatial Datasets",
    "text": "8 Mapping the Geospatial Datasets\nAfter checking the referencing system of each geospatial data data frame, it is also useful for us to plot a map to show their spatial patterns.\n\n8.1 Static Map\n\n# add polygon layer of the coastal outline of sg island\ntm_shape(sg_sf)+ tm_polygons() +\n# add polygon layer of the subzone based on sg masterplan\ntm_shape(mpsz_sf) + tm_polygons() +\n# add dot layer to show the locations of childcare centres\ntm_shape(childcare_sf) + tm_dots() +\ntm_layout()\n\n\n\n\n\n\n\n\nWhen all the 3 datasets are overlayed together, it shows the locations of childcare centres on the Singapore island. Since all the geospatial layers are within the same map context, it means their referencing system and coordinate values are referred to similar spatial context. This consistency is crucial for accurate geospatial analysis.\n\n\n8.2 Interactive Map\nAlternatively, we can also prepare a pin map by using the code block below.\n\ntmap_mode('view')\n\n# tm_basemap(\"Esri.WorldGrayCanvas\") +\n# tm_basemap(\"OpenStreetMap\") +\ntm_basemap(\"Esri.WorldTopoMap\") +\ntm_shape(childcare_sf) +\n  tm_dots(alpha = 0.5)\n\n\n\n\n\n\ntmap_mode('plot')\n\nIn interactive mode, tmap uses the Leaflet for R API, allowing you to freely navigate, zoom, and click on features for detailed information. You can also change the map’s background using layers like ESRI.WorldGrayCanvas, OpenStreetMap, and ESRI.WorldTopoMap, with ESRI.WorldGrayCanvas as the default.\n\n\n\n\n\n\nTip\n\n\n\nRemember to switch back to plot mode after interacting to avoid connection issues and limit interactive maps to fewer than 10 in RMarkdown documents for Netlify publishing."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02b.html#geospatial-data-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02b.html#geospatial-data-wrangling",
    "title": "2B: 2nd Order Spatial Point Patterns Analysis",
    "section": "9 Geospatial Data Wrangling",
    "text": "9 Geospatial Data Wrangling\nWhile simple feature data frames are becoming more popular compared to sp’s Spatial* classes, many geospatial analysis packages still require data in the Spatial* format. This section will show you how to convert a simple feature data frame to sp’s Spatial* class.\n\n9.1 Converting sf data frames to sp’s Spatial* class\nThe code block below uses as_Spatial() of sf package to convert the three geospatial data from simple feature data frame to sp’s Spatial* class.\n\nchildcare &lt;- as_Spatial(childcare_sf)\nmpsz &lt;- as_Spatial(mpsz_sf)\nsg &lt;- as_Spatial(sg_sf)\n\nAfter the sf dataframe to sp Spatial* conversion, let’s inspect the Spatial* classes.\n\nchildcare\n\nclass       : SpatialPointsDataFrame \nfeatures    : 1545 \nextent      : 11203.01, 45404.24, 25667.6, 49300.88  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 2\nnames       :    Name,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Description \nmin values  :   kml_1, &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;018989&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;1, MARINA BOULEVARD, #B1 - 01, ONE MARINA BOULEVARD, SINGAPORE 018989&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;THE LITTLE SKOOL-HOUSE INTERNATIONAL PTE. LTD.&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;08F73931F4A691F4&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20200826094036&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt; \nmax values  : kml_999,                  &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;829646&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;200, PONGGOL SEVENTEENTH AVENUE, SINGAPORE 829646&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Child Care Services&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;RAFFLES KIDZ @ PUNGGOL PTE LTD&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;379D017BF244B0FA&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20200826094036&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt; \n\n\n\nmpsz\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 323 \nextent      : 2667.538, 56396.44, 15748.72, 50256.33  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 15\nnames       : OBJECTID, SUBZONE_NO, SUBZONE_N, SUBZONE_C, CA_IND, PLN_AREA_N, PLN_AREA_C,       REGION_N, REGION_C,          INC_CRC, FMEL_UPD_D,     X_ADDR,     Y_ADDR,    SHAPE_Leng,    SHAPE_Area \nmin values  :        1,          1, ADMIRALTY,    AMSZ01,      N, ANG MO KIO,         AM, CENTRAL REGION,       CR, 00F5E30B5C9B7AD8,      16409,  5092.8949,  19579.069, 871.554887798, 39437.9352703 \nmax values  :      323,         17,    YUNNAN,    YSSZ09,      Y,     YISHUN,         YS,    WEST REGION,       WR, FFCCF172717C2EAF,      16409, 50424.7923, 49552.7904, 68083.9364708,  69748298.792 \n\n\n\nsg\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 60 \nextent      : 2663.926, 56047.79, 16357.98, 50244.03  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 4\nnames       : GDO_GID, MSLINK, MAPID,              COSTAL_NAM \nmin values  :       1,      1,     0,             ISLAND LINK \nmax values  :      60,     67,     0, SINGAPORE - MAIN ISLAND \n\n\nThe geospatial data have been converted into their respective sp’s Spatial* classes.\n\n\n9.2 Converting the Spatial* Class into Generic sp Format\nspatstat requires the analytical data in ppp object form. There is no straightforward way to convert a Spatial* classes into ppp object. We need to convert the Spatial classes* into Spatial object first.\n\n\n\n\n\n\nTip\n\n\n\nppp refers to planar point pattern. It is used to represent spatial point patterns in spatstat; it contains the event locations with possibly associated marks, and the observation window where the events occur.\nsee Chapter 18 The spatstat package | Spatial Statistics for Data Science: Theory and Practice with R\n\n\nThe codes block below converts the Spatial* classes into generic sp objects.\n\nchildcare_sp &lt;- as(childcare, \"SpatialPoints\")\nsg_sp &lt;- as(sg, \"SpatialPolygons\")\n\nNext, we can display the sp objects properties.\n\nchildcare_sp\n\nclass       : SpatialPoints \nfeatures    : 1545 \nextent      : 11203.01, 45404.24, 25667.6, 49300.88  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \n\nsg_sp\n\nclass       : SpatialPolygons \nfeatures    : 60 \nextent      : 2663.926, 56047.79, 16357.98, 50244.03  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \n\n\nLet’s further inspect the differences between Spatial* classes and generic sp object with the example of childcare and childcare_sp object.\n\nhead(childcare)\n\n   Name\n1 kml_1\n2 kml_2\n3 kml_3\n4 kml_4\n5 kml_5\n6 kml_6\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Description\n1 &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;760742&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;742, YISHUN AVENUE 5, #01 - 470, SINGAPORE 760742&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Child Care Services&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;AVERBEL CHILD DEVELOPMENT CENTRE PTE LTD&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;AEA27114446235CE&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20200826094036&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;\n2                                    &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;159053&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;20, LENGKOK BAHRU, #02 - 05, SINGAPORE 159053&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Child Care Services&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;AWWA LTD.&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;86B24416FB1663C6&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20200826094036&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;\n3        &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;556912&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;22, LI HWAN VIEW, GOLDEN HILL ESTATE, SINGAPORE 556912&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Child Care Services&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;BABIES BY-THE-PARK PTE. LTD.&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;F971CBBA973E1AE5&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20200826094036&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;\n4 &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;569139&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;3, ANG MO KIO STREET 62, #01 - 36, LINK@AMK, SINGAPORE 569139&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Child Care Services&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;Baby Elk Infant Care Pte Ltd&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;86A4F25D1C7C9D85&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20200826094036&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;\n5                           &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;467961&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;22A, KEW DRIVE, SINGAPORE 467961&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Child Care Services&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;BABYPLANET MONTESSORI PTE. LTD.&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;CFE3F056F8171C7B&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20200826094036&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;\n6                       &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;598523&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;3 Jalan Kakatua, JURONG PARK, SINGAPORE 598523&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Child Care Services&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;BAMBINI CHILDCARE LLP&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;2B4F0B285ED28C4A&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20200826094036&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;\n\n\n\nhead(childcare_sp)\n\nclass       : SpatialPoints \nfeatures    : 1 \nextent      : 27976.73, 27976.73, 45716.7, 45716.7  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \n\n\nNote that the Spatial* classes contain more attribute data as compared its generic sp object counterpart.\n\n\n\n\n\n\nTip\n\n\n\nDifferences between Spatial* classes and generic sp object\n\nData Storage: SpatialPoints stores only the coordinates, while SpatialPointsDataFrame stores both coordinates and additional attribute data\nFunctionality: SpatialPointsDataFrame allows for more complex operations and analyses due to the additional data it holds\n\nsee Introduction to spatial points in R - Michael T. Hallworth, Ph.D.\n\n\n\n\n9.3 Converting the Generic sp Format into spatstat’s ppp Format\nNow, we will use as.ppp() function of spatstat to convert the spatial data into spatstat’s ppp object format.\n\nchildcare_ppp &lt;- as.ppp(childcare_sf)\nchildcare_ppp\n\nMarked planar point pattern: 1545 points\nmarks are of storage type  'character'\nwindow: rectangle = [11203.01, 45404.24] x [25667.6, 49300.88] units\n\n\nLet’s examine the difference by plotting childcare_ppp:\n\nplot(childcare_ppp)\n\n\n\n\n\n\n\n\nWe can also view the summary statistics of the newly created ppp object by using the code block below.\n\nsummary(childcare_ppp)\n\nMarked planar point pattern:  1545 points\nAverage intensity 1.91145e-06 points per square unit\n\nCoordinates are given to 11 decimal places\n\nmarks are of type 'character'\nSummary:\n   Length     Class      Mode \n     1545 character character \n\nWindow: rectangle = [11203.01, 45404.24] x [25667.6, 49300.88] units\n                    (34200 x 23630 units)\nWindow area = 808287000 square units\n\n\n\n\n\n\n\n\nTip\n\n\n\nBe aware of the warning message regarding duplicates. In spatial point pattern analysis, duplicates can be a significant issue. The statistical methods used for analyzing spatial point patterns often assume that the points are distinct and non-coincident.\n\n\n\n\n9.4 Handling Duplicated Points\nWe can check the duplication in a ppp object by using the duplicated function with different configurations.\n\n\n\n\n\n\nTip\n\n\n\nThe duplicated function has an argument rule:\n\nDefault Behavior (rule = \"spatstat\"):\n\nPoints are considered identical if both their coordinates (like x and y positions) and their marks (additional data or labels attached to the points) are exactly the same.\nThis is the strictest check, requiring everything about the points to match.\n\nOnly Checking Coordinates (rule = \"unmark\"):\n\nPoints are considered duplicates if their coordinates are the same, regardless of their marks.\nMarks are ignored, so only the positions are compared.\n\nUsing deldir Package (rule = \"deldir\"):\n\nPoints are considered duplicates based on their coordinates, but the comparison is done using a specific method (duplicatedxy) from the deldir package.\nThis approach ensures the check is consistent with other functions in the deldir package, which is often used for spatial data analysis (like creating Delaunay triangulations).\n\n\nIn other words,\n\nrule = \"spatstat\": Strict check (coordinates and marks).\nrule = \"unmark\": Less strict (coordinates only).\nrule = \"deldir\": Coordinate check, consistent with the deldir package methods.\n\nsee R: Determine Duplicated Points in a Spatial Point Pattern\n\n\n\n# duplicated(childcare_ppp)\n# any(duplicated(childcare_ppp))\nrules &lt;- c(\"spatstat\", \"deldir\", \"unmark\")\n\nduplicate_counts &lt;- list()\nfor (rule in rules) {\n  duplicates &lt;- duplicated(childcare_ppp, rule = rule)\n  num_duplicates &lt;- sum(duplicates)\n  duplicate_counts[[rule]] &lt;- num_duplicates\n}\n\nprint(duplicate_counts)\n\n$spatstat\n[1] 0\n\n$deldir\n[1] 0\n\n$unmark\n[1] 74\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that this behavior happens because the data contains marked points with the same coordinates but different properties.\nUpon manual inspection, a set of example is “39, WOODLANDS CLOSE, #01 - 62, MEGA@WOODLANDS, SINGAPORE 737856” and “39, WOODLANDS CLOSE, #01 - 59, MEGA@WOODLANDS, SINGAPORE 737856”.\nThese are 2 childcare centres that resides in the same building. Thus, it can only be picked up using the “unmark” rule which only examine for exact match of the point coordinate only.\n\n\nTo count the number of coincident points, we will use the multiplicity() function as shown in the code block below. see R: Multiplicity for more info.\n\nmultiplicity(childcare_ppp)\n\nIf we want to know how many locations have more than one point event:\n\nsum(multiplicity(childcare_ppp) &gt; 1)\n\n[1] 0\n\n\n\n# double check\ncoincident_points &lt;- duplicated(childcare_ppp,  rule=\"unmark\")\ncoincident_coordinates &lt;- childcare_ppp[coincident_points]\nprint(coincident_coordinates)\n\nMarked planar point pattern: 74 points\nmarks are of storage type  'character'\nwindow: rectangle = [11203.01, 45404.24] x [25667.6, 49300.88] units\n\n\nThe output shows that there are 74 duplicated point events.\n\n\n9.5 How to Spot Duplicate Points on the Map\nThere are three ways to overcome this problem. The easiest way is to delete the duplicates. But, that will also mean that some useful point events will be lost.\nThe second solution is use jittering, which will add a small perturbation to the duplicate points so that they do not occupy the exact same space.\nThe third solution is to make each point “unique” and then attach the duplicates of the points to the patterns as marks, as attributes of the points. Then you would need analytical techniques that take into account these marks.\n\n9.5.1 Jittering\nThe code block below implements the jittering approach.\n\nchildcare_ppp_jit &lt;- rjitter(childcare_ppp,\n                             retry=TRUE,\n                             nsim=1,\n                             drop=TRUE)\n\nplot(childcare_ppp_jit, pch = 16, cex = 0.5, main = \"Jittered Points\")\n\n\n\n\n\n\n\n\n\nany(duplicated(childcare_ppp_jit))\n\n[1] FALSE\n\n\n\n\n\n9.6 Creating owin Object\nWhen analysing spatial point patterns, it is a good practice to confine the analysis with a geographical area like Singapore boundary. In spatstat, an object called owin is specially designed to represent this polygonal region.\nThe code block below is used to covert sg SpatialPolygon object into owin object of spatstat.\n\nsg_owin &lt;- as.owin(sg_sf)\n\nThe output object can be displayed by using plot() function:\n\nplot(sg_owin)\n\n\n\n\n\n\n\n\nAnd using summary() function of Base R:\n\nsummary(sg_owin)\n\nWindow: polygonal boundary\n50 separate polygons (1 hole)\n                 vertices         area relative.area\npolygon 1 (hole)       30     -7081.18     -9.76e-06\npolygon 2              55     82537.90      1.14e-04\npolygon 3              90    415092.00      5.72e-04\npolygon 4              49     16698.60      2.30e-05\npolygon 5              38     24249.20      3.34e-05\npolygon 6             976  23344700.00      3.22e-02\npolygon 7             721   1927950.00      2.66e-03\npolygon 8            1992   9992170.00      1.38e-02\npolygon 9             330   1118960.00      1.54e-03\npolygon 10            175    925904.00      1.28e-03\npolygon 11            115    928394.00      1.28e-03\npolygon 12             24      6352.39      8.76e-06\npolygon 13            190    202489.00      2.79e-04\npolygon 14             37     10170.50      1.40e-05\npolygon 15             25     16622.70      2.29e-05\npolygon 16             10      2145.07      2.96e-06\npolygon 17             66     16184.10      2.23e-05\npolygon 18           5195 636837000.00      8.78e-01\npolygon 19             76    312332.00      4.31e-04\npolygon 20            627  31891300.00      4.40e-02\npolygon 21             20     32842.00      4.53e-05\npolygon 22             42     55831.70      7.70e-05\npolygon 23             67   1313540.00      1.81e-03\npolygon 24            734   4690930.00      6.47e-03\npolygon 25             16      3194.60      4.40e-06\npolygon 26             15      4872.96      6.72e-06\npolygon 27             15      4464.20      6.15e-06\npolygon 28             14      5466.74      7.54e-06\npolygon 29             37      5261.94      7.25e-06\npolygon 30            111    662927.00      9.14e-04\npolygon 31             69     56313.40      7.76e-05\npolygon 32            143    145139.00      2.00e-04\npolygon 33            397   2488210.00      3.43e-03\npolygon 34             90    115991.00      1.60e-04\npolygon 35             98     62682.90      8.64e-05\npolygon 36            165    338736.00      4.67e-04\npolygon 37            130     94046.50      1.30e-04\npolygon 38             93    430642.00      5.94e-04\npolygon 39             16      2010.46      2.77e-06\npolygon 40            415   3253840.00      4.49e-03\npolygon 41             30     10838.20      1.49e-05\npolygon 42             53     34400.30      4.74e-05\npolygon 43             26      8347.58      1.15e-05\npolygon 44             74     58223.40      8.03e-05\npolygon 45            327   2169210.00      2.99e-03\npolygon 46            177    467446.00      6.44e-04\npolygon 47             46    699702.00      9.65e-04\npolygon 48              6     16841.00      2.32e-05\npolygon 49             13     70087.30      9.66e-05\npolygon 50              4      9459.63      1.30e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 725376000 square units\nFraction of frame area: 0.401"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02b.html#combining-point-events-object-and-owin-object",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02b.html#combining-point-events-object-and-owin-object",
    "title": "2B: 2nd Order Spatial Point Patterns Analysis",
    "section": "10 Combining Point Events Object and Owin Object",
    "text": "10 Combining Point Events Object and Owin Object\nFor the last step of geospatial data wrangling, we will extract childcare events that are located within Singapore by using the code block below.\n\n\n\n\n\n\nImportant\n\n\n\nSince the dataset contains duplicated points, we will use the jittered ppp object for downstream analysis.\n\n\n\nchildcareSG_ppp &lt;- childcare_ppp_jit[sg_owin]\n\nThe output object combined both the point and polygon feature in one ppp object class as shown below.\n\nsummary(childcareSG_ppp)\n\nMarked planar point pattern:  1545 points\nAverage intensity 2.129929e-06 points per square unit\n\nCoordinates are given to 11 decimal places\n\nmarks are of type 'character'\nSummary:\n   Length     Class      Mode \n     1545 character character \n\nWindow: polygonal boundary\n50 separate polygons (1 hole)\n                 vertices         area relative.area\npolygon 1 (hole)       30     -7081.18     -9.76e-06\npolygon 2              55     82537.90      1.14e-04\npolygon 3              90    415092.00      5.72e-04\npolygon 4              49     16698.60      2.30e-05\npolygon 5              38     24249.20      3.34e-05\npolygon 6             976  23344700.00      3.22e-02\npolygon 7             721   1927950.00      2.66e-03\npolygon 8            1992   9992170.00      1.38e-02\npolygon 9             330   1118960.00      1.54e-03\npolygon 10            175    925904.00      1.28e-03\npolygon 11            115    928394.00      1.28e-03\npolygon 12             24      6352.39      8.76e-06\npolygon 13            190    202489.00      2.79e-04\npolygon 14             37     10170.50      1.40e-05\npolygon 15             25     16622.70      2.29e-05\npolygon 16             10      2145.07      2.96e-06\npolygon 17             66     16184.10      2.23e-05\npolygon 18           5195 636837000.00      8.78e-01\npolygon 19             76    312332.00      4.31e-04\npolygon 20            627  31891300.00      4.40e-02\npolygon 21             20     32842.00      4.53e-05\npolygon 22             42     55831.70      7.70e-05\npolygon 23             67   1313540.00      1.81e-03\npolygon 24            734   4690930.00      6.47e-03\npolygon 25             16      3194.60      4.40e-06\npolygon 26             15      4872.96      6.72e-06\npolygon 27             15      4464.20      6.15e-06\npolygon 28             14      5466.74      7.54e-06\npolygon 29             37      5261.94      7.25e-06\npolygon 30            111    662927.00      9.14e-04\npolygon 31             69     56313.40      7.76e-05\npolygon 32            143    145139.00      2.00e-04\npolygon 33            397   2488210.00      3.43e-03\npolygon 34             90    115991.00      1.60e-04\npolygon 35             98     62682.90      8.64e-05\npolygon 36            165    338736.00      4.67e-04\npolygon 37            130     94046.50      1.30e-04\npolygon 38             93    430642.00      5.94e-04\npolygon 39             16      2010.46      2.77e-06\npolygon 40            415   3253840.00      4.49e-03\npolygon 41             30     10838.20      1.49e-05\npolygon 42             53     34400.30      4.74e-05\npolygon 43             26      8347.58      1.15e-05\npolygon 44             74     58223.40      8.03e-05\npolygon 45            327   2169210.00      2.99e-03\npolygon 46            177    467446.00      6.44e-04\npolygon 47             46    699702.00      9.65e-04\npolygon 48              6     16841.00      2.32e-05\npolygon 49             13     70087.30      9.66e-05\npolygon 50              4      9459.63      1.30e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 725376000 square units\nFraction of frame area: 0.401\n\n\n\nplot(childcareSG_ppp)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02b.html#first-order-spatial-point-patterns-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02b.html#first-order-spatial-point-patterns-analysis",
    "title": "2B: 2nd Order Spatial Point Patterns Analysis",
    "section": "11 First-order Spatial Point Patterns Analysis",
    "text": "11 First-order Spatial Point Patterns Analysis\nIn this section, you will learn how to perform first-order SPPA by using spatstat package. The hands-on exercise will focus on:\n\nderiving kernel density estimation (KDE) layer for visualising and exploring the intensity of point processes,\nperforming Confirmatory Spatial Point Patterns Analysis by using Nearest Neighbour statistics.\n\n\n11.1 Kernel Density Estimation\nIn this section, you will learn how to compute the kernel density estimation (KDE) of childcare services in Singapore.\n\n11.1.1 Computing Kernel Density Estimation Using Automatic Bandwidth Selection Method\n\nbw.diggle() automatic bandwidth selection method. Other recommended methods are bw.CvL(), bw.scott() or bw.ppl().\n\nThe smoothing kernel used is gaussian, which is the default. Other smoothing methods are: “epanechnikov”, “quartic” or “disc”.\n\nThe intensity estimate is corrected for edge effect bias by using method described by Jones (1993) and Diggle (2010, equation 18.9). The default is FALSE.\n\n\nkde_childcareSG_bw &lt;- density(childcareSG_ppp,\n                              sigma=bw.diggle,\n                              edge=TRUE,\n                            kernel=\"gaussian\")\n\nThe plot() function of Base R is then used to display the kernel density derived.\n\nplot(kde_childcareSG_bw)\n\n\n\n\n\n\n\n\nThe density values of the output range from 0 to 0.000035 which is way too small to comprehend. This is because the default unit of measurement of SVY21 is in meter. As a result, the density values computed is in “number of points per square meter”.\nBefore we move on to next section, it is good to know that you can retrieve the bandwidth used to compute the kde layer by using the code block below.\n\nbw &lt;- bw.diggle(childcareSG_ppp)\nbw\n\n   sigma \n414.4576 \n\n\n\n\n11.1.2 Rescaling KDE values\nrescale.ppp() is used below to convert the unit of measurement from meter to kilometer:\n\nchildcareSG_ppp.km &lt;- rescale.ppp(childcareSG_ppp, 1000, \"km\")\n\nNow, we can re-run density() using the resale data set and plot the output kde map.\n\nkde_childcareSG.bw &lt;- density(childcareSG_ppp.km, sigma=bw.diggle, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG.bw)\n\n\n\n\n\n\n\n\nSince we just did a rescaling operation, the output image looks identical to the earlier version with the only changes in terms of data values.\n\n\n\n11.2 Working with Different Automatic Bandwidth Methods\nBeside bw.diggle(), there are 3 other spatstat functions can be used to determine the bandwidth, they are: bw.CvL(), bw.scott(), and bw.ppl().\nLet us take a look at the bandwidth return by these automatic bandwidth calculation methods by using:\n\n bw.CvL(childcareSG_ppp.km)\n\n   sigma \n5.009037 \n\n\n\nbw.scott(childcareSG_ppp.km)\n\n sigma.x  sigma.y \n2.224958 1.451103 \n\n\n\nbw.ppl(childcareSG_ppp.km)\n\n   sigma \n0.273598 \n\n\n\nbw.diggle(childcareSG_ppp.km)\n\n    sigma \n0.4144576 \n\n\nBaddeley et al. (2016) suggest using the bw.ppl() algorithm, as it tends to produce more appropriate values when the pattern consists predominantly of tight clusters. However, they also note that if the aim of a study is to detect a single tight cluster amidst random noise, the bw.diggle() method is likely to be more effective.\nTo compare the output of using bw.diggle and bw.ppl methods:\n\nkde_childcareSG.ppl &lt;- density(childcareSG_ppp.km,\n                               sigma=bw.ppl,\n                               edge=TRUE,\n                               kernel=\"gaussian\")\npar(mfrow=c(1,2))\n\nplot(kde_childcareSG.bw, main = \"bw.diggle\")\nplot(kde_childcareSG.ppl, main = \"bw.ppl\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02b.html#working-with-different-kernel-methods",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02b.html#working-with-different-kernel-methods",
    "title": "2B: 2nd Order Spatial Point Patterns Analysis",
    "section": "12 6.3 Working with Different Kernel Methods",
    "text": "12 6.3 Working with Different Kernel Methods\nBy default, the kernel method used in density.ppp() is gaussian. But there are three other options, namely: Epanechnikov, Quartic and Dics. Let us take a look at what they look like:\n\npar(mfrow=c(2,2))\nplot(density(childcareSG_ppp.km,\n             sigma=bw.ppl,\n             edge=TRUE,\n             kernel=\"gaussian\"),\n     main=\"Gaussian\")\n\nplot(density(childcareSG_ppp.km,\n             sigma=bw.ppl,\n             edge=TRUE,\n             kernel=\"epanechnikov\"),\n     main=\"Epanechnikov\")\n\nplot(density(childcareSG_ppp.km,\n             sigma=bw.ppl,\n             edge=TRUE,\n             kernel=\"quartic\"),\n     main=\"Quartic\")\n\nplot(density(childcareSG_ppp.km,\n             sigma=bw.ppl,\n             edge=TRUE,\n             kernel=\"disc\"),\n     main=\"Disc\")\n\n\n\n\n\n\n\n\nObservations: In this dataset, the choice of kernel function has only a minor impact on the overall density plots. The Gaussian, Epanechnikov, and Quartic kernels produce smoother transitions and distribute the density over a broader area. In contrast, the Disc kernel provides a more localized density estimation with sharper boundaries and less smoothness."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02b.html#fixed-and-adaptive-kde",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02b.html#fixed-and-adaptive-kde",
    "title": "2B: 2nd Order Spatial Point Patterns Analysis",
    "section": "13 Fixed and Adaptive KDE",
    "text": "13 Fixed and Adaptive KDE\n\n13.1 Computing KDE by using Fixed Bandwidth\nNext, we will compute a KDE layer by defining a bandwidth of 600 meter. Notice that in the code block below, the sigma value used is 0.6. This is because the unit of measurement of childcareSG_ppp.km object is in kilometer, hence the 600m is 0.6km.\nIn this section, we will learn how to derive adaptive kernel density estimation by using density.adaptive() of spatstat.\n\nkde_childcareSG_adaptive &lt;- adaptive.density(childcareSG_ppp.km, method=\"kernel\")\nplot(kde_childcareSG_adaptive)\n\n\n\n\n\n\n\n\nWe can compare the fixed and adaptive kernel density estimation outputs by using:\n\npar(mfrow=c(1,2))\n\nplot(kde_childcareSG.bw, main = \"Fixed bandwidth\")\nplot(kde_childcareSG_adaptive, main = \"Adaptive bandwidth\")\n\n\n\n\n\n\n\n\n\n\n13.2 Converting KDE Output into Grid Object\nTo achieve the same result, we convert the object to a format suitable for mapping:\n\ngridded_kde_childcareSG_bw &lt;- as.SpatialGridDataFrame.im(kde_childcareSG.bw)\nspplot(gridded_kde_childcareSG_bw)\n\n\n13.2.1 Converting Grided Output into Raster\nNext, we will convert the gridded kernel density objects into RasterLayer object by using raster() of raster package.\n\nkde_childcareSG_bw_raster &lt;- raster(kde_childcareSG.bw)\n\nLet us take a look at the properties of kde_childcareSG_bw_raster RasterLayer.\n\nkde_childcareSG_bw_raster\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 0.4170614, 0.2647348  (x, y)\nextent     : 2.663926, 56.04779, 16.35798, 50.24403  (xmin, xmax, ymin, ymax)\ncrs        : NA \nsource     : memory\nnames      : layer \nvalues     : -1.006362e-14, 21.11878  (min, max)\n\n\nNote that the crs property is NA.\n\n\n13.2.2 Assigning Projection Systems\nTo include the CRS information on kde_childcareSG_bw_raster RasterLayer, we will do the following:\n\nprojection(kde_childcareSG_bw_raster) &lt;- CRS(\"+init=EPSG:3414\")\nkde_childcareSG_bw_raster\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 0.4170614, 0.2647348  (x, y)\nextent     : 2.663926, 56.04779, 16.35798, 50.24403  (xmin, xmax, ymin, ymax)\ncrs        : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +units=m +no_defs \nsource     : memory\nnames      : layer \nvalues     : -1.006362e-14, 21.11878  (min, max)\n\n\nNow, the crs property is completed.\n\n\n\n13.3 Visualising the output in tmap\nFinally, we will display the raster in cartographic quality map using tmap package.\n\ntm_shape(kde_childcareSG_bw_raster) +\n  tm_raster(\"layer\", palette = \"viridis\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\"), frame = FALSE)\n\n\n\n\n\n\n\n\n\nvalues(kde_childcareSG_bw_raster)\n\nNote that the raster values are encoded explicitly onto the raster pixel using the values in “v” field.\n\n\n13.4 Comparing Spatial Point Patterns using KDE\nIn this section, we will learn how to compare KDE of childcare at Punggol, Tampines, Chua Chu Kang and Jurong West planning areas.\n\n13.4.1 Extracting Study Area\nThe code block below will be used to extract the target planning areas.\n\npg &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"PUNGGOL\")\ntm &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"TAMPINES\")\nck &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"CHOA CHU KANG\")\njw &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"JURONG WEST\")\n\nPlotting the target planning areas:\n\npar(mfrow=c(2,2))\nplot(pg, main = \"Punggol\")\n\n\n\n\n\n\n\nplot(tm, main = \"Tampines\")\n\n\n\n\n\n\n\nplot(ck, main = \"Choa Chu Kang\")\n\n\n\n\n\n\n\nplot(jw, main = \"Jurong West\")\n\n\n\n\n\n\n\n\n\n\n13.4.2 Creating owin object\nNow, we will convert these sf objects into owin objects that is required by spatstat.\n\npg_owin = as.owin(pg)\ntm_owin = as.owin(tm)\nck_owin = as.owin(ck)\njw_owin = as.owin(jw)\n\n\n\n13.4.3 Combining Childcare Points and the Study Area\nTo extract childcare that is within the specific region for analysis, we can use:\n\nchildcare_pg_ppp = childcare_ppp_jit[pg_owin]\nchildcare_tm_ppp = childcare_ppp_jit[tm_owin]\nchildcare_ck_ppp = childcare_ppp_jit[ck_owin]\nchildcare_jw_ppp = childcare_ppp_jit[jw_owin]\n\nNext, rescale.ppp() function is used to transform the unit of measurement from meter to kilometer.\n\nchildcare_pg_ppp.km = rescale.ppp(childcare_pg_ppp, 1000, \"km\")\nchildcare_tm_ppp.km = rescale.ppp(childcare_tm_ppp, 1000, \"km\")\nchildcare_ck_ppp.km = rescale.ppp(childcare_ck_ppp, 1000, \"km\")\nchildcare_jw_ppp.km = rescale.ppp(childcare_jw_ppp, 1000, \"km\")\n\nThe code block below is used to plot the four study areas and the locations of the childcare centres.\n\npar(mfrow=c(2,2))\n\nplot(childcare_pg_ppp.km, main=\"Punggol\")\nplot(childcare_tm_ppp.km, main=\"Tampines\")\nplot(childcare_ck_ppp.km, main=\"Choa Chu Kang\")\nplot(childcare_jw_ppp.km, main=\"Jurong West\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02b.html#second-order-spatial-point-pattern-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02b.html#second-order-spatial-point-pattern-analysis",
    "title": "2B: 2nd Order Spatial Point Patterns Analysis",
    "section": "14 Second-Order Spatial Point Pattern Analysis",
    "text": "14 Second-Order Spatial Point Pattern Analysis\nIn this section, we will analyze spatial point patterns using various functions: G-Function, F-Function, K-Function, and L-Function.\n\n14.1 Analysing Spatial Point Process Using G-Function\nThe G function measures the distribution of the distances from an arbitrary event to its nearest event. In this section, we will learn how to compute G-function estimation by using Gest() of spatstat package. We will also learn how to perform monte carlo simulation test using envelope() of spatstat package.\n\n14.1.1 Choa Chu Kang Planning Area\n\n14.1.1.1 Computing G-function Estimation\nTo compute G-function using Gest() of spatstat package:\n\n\n\n\n\n\nNote\n\n\n\ncorrection is an optional argument in Gest()\nOptional. The edge correction(s) to be used to estimate . A vector of character strings selected from “none”, “rs”, “km”, “Hanisch” and “best”. Alternatively correction=“all” selects all options.\nsee Gest function - RDocumentation\n\n\n\n# rs and border has the same effect\nG_CK = Gest(childcare_ck_ppp, correction = \"rs\")\nG_CK = Gest(childcare_ck_ppp, correction = \"border\")\nplot(G_CK, xlim=c(0,500))\n\n\n\n\n\n\n\n\nWe can also use the “all” option in correction to display all forms of edge corrections from “none”, “rs”, “km”, “Hanisch” and “best”.\n\nG_CK_all = Gest(childcare_ck_ppp, correction = \"all\")\nplot(G_CK_all, xlim=c(0,500))\n\n\n\n\n\n\n\n\n\n\n14.1.1.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\\(H_0\\) = The distribution of childcare services at Choa Chu Kang are randomly distributed.\n\\(H_1\\)= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nMonte Carlo test with G-function:\n\n\n\n\n\n\nNote\n\n\n\nThe envelope function calculates overall and pointwise confidence envelopes for a curve based on bootstrap replicates of the curve evaluated at a number of fixed points.\nIn other words, It helps you determine if your observed spatial pattern (e.g., locations of points in a study area) is significantly different from what you would expect under a random or theoretical distribution.\nHow It Works: 1. Simulate Data: It generates multiple simulated datasets (often by randomizing the locations of points) based on the null hypothesis (e.g., complete spatial randomness). 2. Compute Statistics: For each simulated dataset, it computes a spatial statistic (e.g., G-function, F-function) and creates a distribution of these statistics. 3. Compare: It compares the observed statistic from your actual data to the distribution of statistics from the simulated datasets. 4. Envelope Plot: It plots the range (envelope) of the simulated statistics along with the observed statistic, allowing you to see if your observed statistic falls outside the range of what is expected under the null hypothesis.\nWhen to Use It? Use the envelope function when you want to:\n\nTest if the observed spatial pattern deviates significantly from a random pattern or other theoretical patterns.\nAssess the statistical significance of spatial features or clustering in your data.\n\n\nChoice of nsim: The choice of 39 simulations (nsim = 39) in Monte Carlo techniques for spatial analysis is often a practical compromise between computational efficiency and statistical robustness.\n\nComputational Efficiency: Running a large number of simulations can be computationally expensive, especially for complex spatial analyses. We can strike a balance between obtaining reliable results and keeping computational costs manageable by selecting absolute minimum sample size.\n\nThe minimum number of simulations \\(m\\) required for a Monte Carlo test at a particular significance level can be determined by:\n\\(\\alpha = \\frac{1}{m+1}\\)\nfor a one-tailed test and\n\\(\\alpha = \\frac{2}{m+1}\\)\nThus, a one-tailed test at a significance level of 5% would require a minimum of 19 simulations, a two-tailed test at a significance level of 5% would require a minimum of 39 simulations.\nsee Going beyond the required number of simulations required for a particular significance level when conducting a Monte Carlo test - Cross Validated, spatial statistics - Simulation envelopes and significance levels - Geographic Information Systems Stack Exchange\n\n\n\nG_CK.csr &lt;- envelope(childcare_ck_ppp, Gest, nsim=39)\n\nGenerating 39 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, \n39.\n\nDone.\n\n\n\nplot(G_CK.csr)\n\n\n\n\n\n\n\n\n\n\n\n14.1.2 Tampines Planning Area\n\n14.1.2.1 Computing G-function Estimation\nWe will use the best edge correction for this example.\n\nG_tm = Gest(childcare_tm_ppp, correction = \"best\")\nplot(G_tm)\n\n\n\n\n\n\n\n\n\nG_tm_all = Gest(childcare_tm_ppp, correction = \"all\")\nplot(G_tm_all)\n\n\n\n\n\n\n\n\n\n\n\n\n14.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\\(H_0\\) = The distribution of childcare services at Tampines are randomly distributed.\n\\(H_1\\) = The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected is p-value is smaller than alpha value of 0.001.\nThe code block below is used to perform the hypothesis testing.\n\nG_tm.csr &lt;- envelope(childcare_tm_ppp, Gest, correction = \"all\", nsim=39)\n\nGenerating 39 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, \n39.\n\nDone.\n\n\n\nplot(G_tm.csr)\n\n\n\n\n\n\n\n\n\n\n14.3 Analysing Spatial Point Process Using F-Function\nThe F function estimates the empty space function F(r) or its hazard rate h(r) from a point pattern in a window of arbitrary shape. In this section, we will learn how to compute F-function estimation by using Fest() of spatstat package. We will also learn how to perform monte carlo simulation test using envelope() of spatstat package.\n\n14.3.1 Choa Chu Kang Planning Area\n\n14.3.1.1 Computing F-function estimation\n\n\n\n\n\n\nNote\n\n\n\nFest() has the same correction option as Gest().\n\n\n\nF_CK_all = Fest(childcare_ck_ppp, correction = \"all\")\nplot(F_CK_all)\n\n\n\n\n\n\n\n\n\n\n14.3.1.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\\(H_0\\) = The distribution of childcare services at Choa Chu Kang are randomly distributed.\n\\(H_1\\) = The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nMonte Carlo test with F-function:\n\nF_CK.csr &lt;- envelope(childcare_ck_ppp, Fest, nsim=39)\n\nGenerating 39 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, \n39.\n\nDone.\n\n\n\nplot(F_CK.csr)\n\n\n\n\n\n\n\n\n\n\n\n14.3.2 Tampines Planning Area\n\n14.3.2.1 Computing F-function estimation\nMonte Carlo test with F-function:\n\nF_tm = Fest(childcare_tm_ppp, correction = \"best\")\nplot(F_tm)\n\n\n\n\n\n\n\n\n\n\n14.3.2.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\\(H_0\\) = The distribution of childcare services at Tampines are randomly distributed.\n\\(H_1\\) = The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected is p-value is smaller than alpha value of 0.001.\nThe code block below is used to perform the hypothesis testing.\n\nF_tm.csr &lt;- envelope(childcare_tm_ppp, Fest, correction = \"all\", nsim=39)\n\nGenerating 39 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, \n39.\n\nDone.\n\n\n\nplot(F_tm.csr)\n\n\n\n\n\n\n\n\n\n\n\n\n14.4 Analysing Spatial Point Process Using K-Function\nK-function measures the number of events found up to a given distance of any particular event. In this section, you will learn how to compute K-function estimates by using Kest() of spatstat package. We will also learn how to perform monte carlo simulation test using envelope() of spatstat package.\n\n\n\n\n\n\nNote\n\n\n\nKest()’s correction option is different fromGest()andFest()`.\ncorrection: Optional. A character vector containing any selection of the options “none”, “border”, “bord.modif”, “isotropic”, “Ripley”, “translate”, “translation”, “rigid”, “none”, “good” or “best”. It specifies the edge correction(s) to be applied. Alternatively correction=“all” selects all options.\nsee Kest function - RDocumentation\n\n\n\n14.4.1 Choa Chu Kang Planning Area\n\n14.4.1.1 Computing K-Function Estimate\n\nK_ck = Kest(childcare_ck_ppp, correction = \"Ripley\")\nplot(K_ck, . -r ~ r, ylab= \"K(d)-r\", xlab = \"d(m)\")\n\n\n\n\n\n\n\n\n\n\n14.4.1.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\\(H_0\\) = The distribution of childcare services at Choa Chu Kang are randomly distributed.\n\\(H_1\\) = The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nThe code block below is used to perform the hypothesis testing.\n\nK_ck.csr &lt;- envelope(childcare_ck_ppp, Kest, nsim=39, rank = 1, glocal=TRUE)\n\nGenerating 39 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, \n39.\n\nDone.\n\n\n\nplot(K_ck.csr, . - r ~ r, xlab=\"d\", ylab=\"K(d)-r\")\n\n\n\n\n\n\n\n\n\n\n\n14.4.2 Tampines Planning Area\n\n14.4.2.1 Computing K-function Estimation\n\nK_tm = Kest(childcare_tm_ppp, correction = \"Ripley\")\nplot(K_tm, . -r ~ r,\n     ylab= \"K(d)-r\", xlab = \"d(m)\",\n     xlim=c(0,1000))\n\n\n\n\n\n\n\n\n\n\n\n\n14.5 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\\(H_0\\) = The distribution of childcare services at Tampines are randomly distributed.\n\\(H_1\\) = The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nThe code block below is used to perform the hypothesis testing.\n\nK_tm.csr &lt;- envelope(childcare_tm_ppp, Kest, nsim=39, rank = 1, glocal=TRUE)\n\nGenerating 39 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, \n39.\n\nDone.\n\n\n\nplot(K_tm.csr, . - r ~ r,\n     xlab=\"d\", ylab=\"K(d)-r\", xlim=c(0,500))\n\n\n\n\n\n\n\n\n\n\n14.6 Analysing Spatial Point Process Using L-Function\nIn this section, you will learn how to compute L-function estimation by using Lest() of spatstat package. We will also learn how to perform monte carlo simulation test using envelope() of spatstat package.\n\n14.6.1 Choa Chu Kang Planning Area\n\n14.6.1.1 Computing L-function Estimation\n\n\n\n\n\n\nNote\n\n\n\nLest() has the same correction options as Kest().\n\n\n\nL_ck = Lest(childcare_ck_ppp, correction = \"Ripley\")\nplot(L_ck, . -r ~ r,\n     ylab= \"L(d)-r\", xlab = \"d(m)\")\n\n\n\n\n\n\n\n\n\n\n14.6.1.2 Performing Complete Spatial Randomness Test\n\\(H_0\\) = The distribution of childcare services at Choa Chu Kang are randomly distributed.\n\\(H_1\\) = The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value if smaller than alpha value of 0.001.\nThe code block below is used to perform the hypothesis testing.\n\nL_ck.csr &lt;- envelope(childcare_ck_ppp, Lest, nsim=39, rank = 1, glocal=TRUE)\n\nGenerating 39 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, \n39.\n\nDone.\n\n\n\nplot(L_ck.csr, . - r ~ r, xlab=\"d\", ylab=\"L(d)-r\")\n\n\n\n\n\n\n\n\n\n\n\n14.6.2 Tampines Planning Area\n\n14.6.2.1 Computing L-function Estimate\n\nL_tm = Lest(childcare_tm_ppp, correction = \"Ripley\")\nplot(L_tm, . -r ~ r,\n     ylab= \"L(d)-r\", xlab = \"d(m)\",\n     xlim=c(0,1000))\n\n\n\n\n\n\n\n\n\n\n\n\n14.7 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\\(H_0\\) = The distribution of childcare services at Tampines are randomly distributed.\n\\(H_1\\) = The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nThe code chunk below will be used to perform the hypothesis testing\n\nL_tm.csr &lt;- envelope(childcare_tm_ppp, Lest, nsim=39, rank = 1, glocal=TRUE)\n\nGenerating 39 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, \n39.\n\nDone.\n\n\n\nplot(L_tm.csr, . - r ~ r,\n     xlab=\"d\", ylab=\"L(d)-r\", xlim=c(0,500))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html",
    "title": "5B: Local Measures of Spatial Autocorrelation",
    "section": "",
    "text": "R for Geospatial Data Science and Analytics - 10  Local Measures of Spatial Autocorrelation"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#exercise-5b-reference",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#exercise-5b-reference",
    "title": "5B: Local Measures of Spatial Autocorrelation",
    "section": "",
    "text": "R for Geospatial Data Science and Analytics - 10  Local Measures of Spatial Autocorrelation"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#overview",
    "title": "5B: Local Measures of Spatial Autocorrelation",
    "section": "2 Overview",
    "text": "2 Overview\nLocal Measures of Spatial Autocorrelation (LMSA) analyze the relationships between each observation and its surroundings, rather than summarizing these relationships across an entire map. They provide scores that reveal the spatial structure of the data, similar in concept to global measures, and are often mathematically connected, as global measures can be decomposed into local ones.\nIn this exercise, we will learn to compute Local Measures of Spatial Autocorrelation (LMSA) using the spdep package, including Local Moran’s I, Getis-Ord’s Gi-statistics, and their visualizations."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#learning-outcome",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#learning-outcome",
    "title": "5B: Local Measures of Spatial Autocorrelation",
    "section": "3 Learning Outcome",
    "text": "3 Learning Outcome\n\nImport geospatial data using the sf package\nImport CSV data using the readr package\nPerform relational joins using the dplyr package\nCompute Local Indicator of Spatial Association (LISA) statistics using spdep\n\nDetect clusters and outliers with Local Moran’s I\nIdentify hot and cold spots with Getis-Ord’s Gi-statistics\n\nVisualize analysis outputs using the tmap package"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#the-analytical-question",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#the-analytical-question",
    "title": "5B: Local Measures of Spatial Autocorrelation",
    "section": "4 The Analytical Question",
    "text": "4 The Analytical Question\nIn spatial policy planning, one of the main development objective of the local government and planners is to ensure equal distribution of development in the province. In this study, we will apply spatial statistical methods to examine the distribution of development in Hunan Province, China, using a selected indicator (e.g., GDP per capita).\n\nOur key questions are:\n\nIs development evenly distributed geographically?\nIf not, is there evidence of spatial clustering?\nIf clustering exists, where are these clusters located?"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#the-data",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#the-data",
    "title": "5B: Local Measures of Spatial Autocorrelation",
    "section": "5 The Data",
    "text": "5 The Data\nThe following 2 datasets will be used in this exercise.\n\n\n\n\n\n\n\n\nData Set\nDescription\nFormat\n\n\n\n\nHunan county boundary layer\nGeospatial data set representing the county boundaries of Hunan\nESRI Shapefile\n\n\nHunan_2012.csv\nContains selected local development indicators for Hunan in 2012\nCSV"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#installing-and-launching-the-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#installing-and-launching-the-r-packages",
    "title": "5B: Local Measures of Spatial Autocorrelation",
    "section": "6 Installing and Launching the R Packages",
    "text": "6 Installing and Launching the R Packages\nThe following R packages will be used in this exercise:\n\n\n\n\n\n\n\n\nPackage\nPurpose\nUse Case in Exercise\n\n\n\n\nsf\nHandles spatial data, particularly vector-based geospatial data.\nImporting and managing township boundary data for Myanmar.\n\n\nrgdal\nProvides bindings to the Geospatial Data Abstraction Library (GDAL) for reading and writing spatial data.\nReading and writing geospatial data in various formats, including shapefiles.\n\n\nspdep\nAnalyzes spatial dependence and provides tools for spatial econometrics.\nPerforming spatially constrained clustering and other spatial dependence analyses.\n\n\ntidyverse\nA collection of packages for data science tasks like data manipulation and visualization.\nHandling attribute data, reading CSV files, and data wrangling with readr, dplyr, and ggplot2.\n\n\ntmap\nCreates static and interactive thematic maps.\nVisualizing data using choropleth maps to display spatial patterns and relationships.\n\n\ncorrplot\nVisualizes correlation matrices.\nCreating correlation plots to explore relationships between different ICT measures.\n\n\nggpubr\nProvides functions to create and customize ‘ggplot2’-based publication-ready plots.\nEnhancing multivariate data visualizations for clearer presentation of results.\n\n\nheatmaply\nGenerates interactive heatmaps.\nVisualizing multivariate data through interactive heatmaps for deeper insights.\n\n\ncluster\nPerforms cluster analysis.\nConducting hierarchical clustering to group similar regions based on ICT measures.\n\n\nClustGeo\nPerforms spatially constrained hierarchical clustering.\nApplying spatial constraints to hierarchical clustering for identifying homogeneous regions.\n\n\n\nTo install and load these packages, use the following code:\n\npacman::p_load(sf, spdep, tmap, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#import-data-and-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#import-data-and-preparation",
    "title": "5B: Local Measures of Spatial Autocorrelation",
    "section": "7 Import Data and Preparation",
    "text": "7 Import Data and Preparation\nIn this section, we will perform 3 necessary steps to prepare the data for analysis.\n\n\n\n\n\n\nNote\n\n\n\nThe data preparation is the same as previous exercise such as Exercise 4A.\n\n\n\n7.1 Import Geospatial Shapefile\nFirstly, we will use st_read() of sf package to import Hunan shapefile into R. The imported shapefile will be simple features Object of sf.\n\nhunan &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `/Users/walter/code/isss626/isss626-gaa/Hands-on_Ex/Hands-on_Ex05/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n7.2 Import Aspatial csv File\nNext, we will import Hunan_2012.csv into R by using read_csv() of readr package. The output is R dataframe class.\n\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\n\n\n7.3 Perform Relational Join\nThen, we will perform a left_join() to update the attribute table of hunan’s SpatialPolygonsDataFrame with the attribute fields of hunan2012 dataframe.\n\nhunan &lt;- left_join(hunan,hunan2012) %&gt;%\n  select(1:4, 7, 15)\n\n\n\n7.4 Visualizing Regional Development Indicator\nTo visualize the regional development indicator, we can prepare a base map and a choropleth map to show the distribution of GDPPC 2012 (GDP per capita) by using qtm() of tmap package.\n\nequal &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal interval classification\")\n\nquantile &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal quantile classification\")\n\ntmap_arrange(equal, \n             quantile, \n             asp=1, \n             ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#local-indicators-of-spatial-associationlisa",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#local-indicators-of-spatial-associationlisa",
    "title": "5B: Local Measures of Spatial Autocorrelation",
    "section": "8 Local Indicators of Spatial Association(LISA)",
    "text": "8 Local Indicators of Spatial Association(LISA)\nLocal Indicators of Spatial Association (LISA) are statistics used to identify clusters and outliers in the spatial distribution of a variable. For example, if we are analyzing the GDP per capita in Hunan Province, China, LISA can help detect areas (counties) where GDP values are significantly higher or lower than expected by chance. This means that these values deviate from what would be seen in a random distribution across space.\nIn this section, we will apply appropriate Local Indicators for Spatial Association (LISA), particularly the Local Moran’s I statistic, to identify clusters and outliers in the 2012 GDP per capita data for Hunan Province.\n\n8.1 Computing Contiguity Spatial Weights\nBefore we can compute the global spatial autocorrelation statistics, we need to construct a spatial weights of the study area. The spatial weights is used to define the neighbourhood relationships between the geographical units (i.e. county) in the study area.\nIn the code block below, the poly2nb() function from the spdep package calculates contiguity weight matrices for the study area by identifying regions that share boundaries.\nBy default, poly2nb() uses the “Queen” criteria, which considers any shared boundary or corner as a neighbor (equivalent to setting queen = TRUE). If we want to restrict the criteria to shared boundaries only (excluding corners), set queen = FALSE.\n\nwm_q &lt;- poly2nb(hunan, \n                queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours. There are two area units with only one neighbours.\n\n\n8.2 Row-standardised Weights Matrix\nNext, we need to assign weights to each neighboring polygon. In this case, we’ll use equal weights (style=“W”), where each neighboring polygon gets a weight of 1/(number of neighbors). This means we take the value for each neighbor and divide it by the total number of neighbors, then sum these weighted values to calculate a summary measure, such as weighted income.\nWhile this equal weighting approach is straightforward and easy to understand, it has a limitation: polygons on the edges of the study area have fewer neighbors, which can lead to over- or underestimation of the actual spatial relationships (spatial autocorrelation) in the data.\n\n\n\n\n\n\nTip\n\n\n\nFor simplicity, we use the style=“W” option in this example, but keep in mind that other, potentially more accurate methods are available, such as style=“B”.\n\n\n\nrswm_q &lt;- nb2listw(wm_q, style=\"W\", zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nThe input of nb2listw() must be an object of class nb. The syntax of the function has two major arguments, namely style and zero.poly.\nstyle can take values “W”, “B”, “C”, “U”, “minmax” and “S”. B is the basic binary coding, W is row standardised (sums over all links to n), C is globally standardised (sums over all links to n), U is equal to C divided by the number of neighbours (sums over all links to unity), while S is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al. 1999, p. 167-168 (sums over all links to n).\nThe zero.policy=TRUE option allows for lists of non-neighbors. This should be used with caution since the user may not be aware of missing neighbors in their dataset however, a zero.policy of FALSE would return an error.\n\n\n\n\n\n8.3 Computing Local Moran’s I\nTo compute local Moran’s I, the localmoran() function of spdep will be used. It computes Ii values, given a set of zi values and a listw object providing neighbour weighting information for the polygon associated with the zi values.\nFirst, we compute a vector fips that contains the indexes to sort the County column of the hunan dataset in ascending alphabetical order.\n\nfips &lt;- order(hunan$County)\nglimpse(hunan$County[fips])\n\n chr [1:88] \"Anhua\" \"Anren\" \"Anxiang\" \"Baojing\" \"Chaling\" \"Changning\" ...\n\n\nThen, we compute local Moran’s I of GDPPC2012 at the county level.\n\nlocalMI &lt;- localmoran(hunan$GDPPC, rswm_q)\n\nhead(localMI)\n\n            Ii          E.Ii       Var.Ii        Z.Ii Pr(z != E(Ii))\n1 -0.001468468 -2.815006e-05 4.723841e-04 -0.06626904      0.9471636\n2  0.025878173 -6.061953e-04 1.016664e-02  0.26266425      0.7928094\n3 -0.011987646 -5.366648e-03 1.133362e-01 -0.01966705      0.9843090\n4  0.001022468 -2.404783e-07 5.105969e-06  0.45259801      0.6508382\n5  0.014814881 -6.829362e-05 1.449949e-03  0.39085814      0.6959021\n6 -0.038793829 -3.860263e-04 6.475559e-03 -0.47728835      0.6331568\n\n\n\n\n\n\n\n\nTip\n\n\n\nlocalmoran() function returns a matrix of values whose columns are:\n\nIi: the local Moran’s I statistics\nE.Ii: the expectation of local moran statistic under the randomisation hypothesis\nVar.Ii: the variance of local moran statistic under the randomisation hypothesis\nZ.Ii:the standard deviate of local moran statistic\nPr(): the p-value of local moran statistic\n\n\n\nNext, we list the content of the local Moran matrix derived by using printCoefmat().\n\nprintCoefmat(data.frame(\n  localMI[fips,], \n  row.names=hunan$County[fips]),\n  check.names=FALSE)\n\n                       Ii        E.Ii      Var.Ii        Z.Ii Pr.z....E.Ii..\nAnhua         -2.2493e-02 -5.0048e-03  5.8235e-02 -7.2467e-02         0.9422\nAnren         -3.9932e-01 -7.0111e-03  7.0348e-02 -1.4791e+00         0.1391\nAnxiang       -1.4685e-03 -2.8150e-05  4.7238e-04 -6.6269e-02         0.9472\nBaojing        3.4737e-01 -5.0089e-03  8.3636e-02  1.2185e+00         0.2230\nChaling        2.0559e-02 -9.6812e-04  2.7711e-02  1.2932e-01         0.8971\nChangning     -2.9868e-05 -9.0010e-09  1.5105e-07 -7.6828e-02         0.9388\nChangsha       4.9022e+00 -2.1348e-01  2.3194e+00  3.3590e+00         0.0008\nChengbu        7.3725e-01 -1.0534e-02  2.2132e-01  1.5895e+00         0.1119\nChenxi         1.4544e-01 -2.8156e-03  4.7116e-02  6.8299e-01         0.4946\nCili           7.3176e-02 -1.6747e-03  4.7902e-02  3.4200e-01         0.7324\nDao            2.1420e-01 -2.0824e-03  4.4123e-02  1.0297e+00         0.3032\nDongan         1.5210e-01 -6.3485e-04  1.3471e-02  1.3159e+00         0.1882\nDongkou        5.2918e-01 -6.4461e-03  1.0748e-01  1.6338e+00         0.1023\nFenghuang      1.8013e-01 -6.2832e-03  1.3257e-01  5.1198e-01         0.6087\nGuidong       -5.9160e-01 -1.3086e-02  3.7003e-01 -9.5104e-01         0.3416\nGuiyang        1.8240e-01 -3.6908e-03  3.2610e-02  1.0305e+00         0.3028\nGuzhang        2.8466e-01 -8.5054e-03  1.4152e-01  7.7931e-01         0.4358\nHanshou        2.5878e-02 -6.0620e-04  1.0167e-02  2.6266e-01         0.7928\nHengdong       9.9964e-03 -4.9063e-04  6.7742e-03  1.2742e-01         0.8986\nHengnan        2.8064e-02 -3.2160e-04  3.7597e-03  4.6294e-01         0.6434\nHengshan      -5.8201e-03 -3.0437e-05  5.1076e-04 -2.5618e-01         0.7978\nHengyang       6.2997e-02 -1.3046e-03  2.1865e-02  4.3486e-01         0.6637\nHongjiang      1.8790e-01 -2.3019e-03  3.1725e-02  1.0678e+00         0.2856\nHuarong       -1.5389e-02 -1.8667e-03  8.1030e-02 -4.7503e-02         0.9621\nHuayuan        8.3772e-02 -8.5569e-04  2.4495e-02  5.4072e-01         0.5887\nHuitong        2.5997e-01 -5.2447e-03  1.1077e-01  7.9685e-01         0.4255\nJiahe         -1.2431e-01 -3.0550e-03  5.1111e-02 -5.3633e-01         0.5917\nJianghua       2.8651e-01 -3.8280e-03  8.0968e-02  1.0204e+00         0.3076\nJiangyong      2.4337e-01 -2.7082e-03  1.1746e-01  7.1800e-01         0.4728\nJingzhou       1.8270e-01 -8.5106e-04  2.4363e-02  1.1759e+00         0.2396\nJinshi        -1.1988e-02 -5.3666e-03  1.1334e-01 -1.9667e-02         0.9843\nJishou        -2.8680e-01 -2.6305e-03  4.4028e-02 -1.3543e+00         0.1756\nLanshan        6.3334e-02 -9.6365e-04  2.0441e-02  4.4972e-01         0.6529\nLeiyang        1.1581e-02 -1.4948e-04  2.5082e-03  2.3422e-01         0.8148\nLengshuijiang -1.7903e+00 -8.2129e-02  2.1598e+00 -1.1623e+00         0.2451\nLi             1.0225e-03 -2.4048e-07  5.1060e-06  4.5260e-01         0.6508\nLianyuan      -1.4672e-01 -1.8983e-03  1.9145e-02 -1.0467e+00         0.2952\nLiling         1.3774e+00 -1.5097e-02  4.2601e-01  2.1335e+00         0.0329\nLinli          1.4815e-02 -6.8294e-05  1.4499e-03  3.9086e-01         0.6959\nLinwu         -2.4621e-03 -9.0703e-06  1.9258e-04 -1.7676e-01         0.8597\nLinxiang       6.5904e-02 -2.9028e-03  2.5470e-01  1.3634e-01         0.8916\nLiuyang        3.3688e+00 -7.7502e-02  1.5180e+00  2.7972e+00         0.0052\nLonghui        8.0801e-01 -1.1377e-02  1.5538e-01  2.0787e+00         0.0376\nLongshan       7.5663e-01 -1.1100e-02  3.1449e-01  1.3690e+00         0.1710\nLuxi           1.8177e-01 -2.4855e-03  3.4249e-02  9.9561e-01         0.3194\nMayang         2.1852e-01 -5.8773e-03  9.8049e-02  7.1663e-01         0.4736\nMiluo          1.8704e+00 -1.6927e-02  2.7925e-01  3.5715e+00         0.0004\nNan           -9.5789e-03 -4.9497e-04  6.8341e-03 -1.0988e-01         0.9125\nNingxiang      1.5607e+00 -7.3878e-02  8.0012e-01  1.8274e+00         0.0676\nNingyuan       2.0910e-01 -7.0884e-03  8.2306e-02  7.5356e-01         0.4511\nPingjiang     -9.8964e-01 -2.6457e-03  5.6027e-02 -4.1698e+00         0.0000\nQidong         1.1806e-01 -2.1207e-03  2.4747e-02  7.6396e-01         0.4449\nQiyang         6.1966e-02 -7.3374e-04  8.5743e-03  6.7712e-01         0.4983\nRucheng       -3.6992e-01 -8.8999e-03  2.5272e-01 -7.1814e-01         0.4727\nSangzhi        2.5053e-01 -4.9470e-03  6.8000e-02  9.7972e-01         0.3272\nShaodong      -3.2659e-02 -3.6592e-05  5.0546e-04 -1.4510e+00         0.1468\nShaoshan       2.1223e+00 -5.0227e-02  1.3668e+00  1.8583e+00         0.0631\nShaoyang       5.9499e-01 -1.1253e-02  1.3012e-01  1.6807e+00         0.0928\nShimen        -3.8794e-02 -3.8603e-04  6.4756e-03 -4.7729e-01         0.6332\nShuangfeng     9.2835e-03 -2.2867e-03  3.1516e-02  6.5174e-02         0.9480\nShuangpai      8.0591e-02 -3.1366e-04  8.9838e-03  8.5358e-01         0.3933\nSuining        3.7585e-01 -3.5933e-03  4.1870e-02  1.8544e+00         0.0637\nTaojiang      -2.5394e-01 -1.2395e-03  1.4477e-02 -2.1002e+00         0.0357\nTaoyuan        1.4729e-02 -1.2039e-04  8.5103e-04  5.0903e-01         0.6107\nTongdao        4.6482e-01 -6.9870e-03  1.9879e-01  1.0582e+00         0.2900\nWangcheng      4.4220e+00 -1.1067e-01  1.3596e+00  3.8873e+00         0.0001\nWugang         7.1003e-01 -7.8144e-03  1.0710e-01  2.1935e+00         0.0283\nXiangtan       2.4530e-01 -3.6457e-04  3.2319e-03  4.3213e+00         0.0000\nXiangxiang     2.6271e-01 -1.2703e-03  2.1290e-02  1.8092e+00         0.0704\nXiangyin       5.4525e-01 -4.7442e-03  7.9236e-02  1.9539e+00         0.0507\nXinhua         1.1810e-01 -6.2649e-03  8.6001e-02  4.2409e-01         0.6715\nXinhuang       1.5725e-01 -4.1820e-03  3.6648e-01  2.6667e-01         0.7897\nXinning        6.8928e-01 -9.6674e-03  2.0328e-01  1.5502e+00         0.1211\nXinshao        5.7578e-02 -8.5932e-03  1.1769e-01  1.9289e-01         0.8470\nXintian       -7.4050e-03 -5.1493e-03  1.0877e-01 -6.8395e-03         0.9945\nXupu           3.2406e-01 -5.7468e-03  5.7735e-02  1.3726e+00         0.1699\nYanling       -6.9021e-02 -5.9211e-04  9.9306e-03 -6.8667e-01         0.4923\nYizhang       -2.6844e-01 -2.2463e-03  4.7588e-02 -1.2202e+00         0.2224\nYongshun       6.3064e-01 -1.1350e-02  1.8830e-01  1.4795e+00         0.1390\nYongxing       4.3411e-01 -9.0735e-03  1.5088e-01  1.1409e+00         0.2539\nYou            7.8750e-02 -7.2728e-03  1.2116e-01  2.4714e-01         0.8048\nYuanjiang      2.0004e-04 -1.7760e-04  2.9798e-03  6.9181e-03         0.9945\nYuanling       8.7298e-03 -2.2981e-06  2.3221e-05  1.8121e+00         0.0700\nYueyang        4.1189e-02 -1.9768e-04  2.3113e-03  8.6085e-01         0.3893\nZhijiang       1.0476e-01 -7.8123e-04  1.3100e-02  9.2214e-01         0.3565\nZhongfang     -2.2685e-01 -2.1455e-03  3.5927e-02 -1.1855e+00         0.2358\nZhuzhou        3.2864e-01 -5.2432e-04  7.2391e-03  3.8688e+00         0.0001\nZixing        -7.6849e-01 -8.8210e-02  9.4057e-01 -7.0144e-01         0.4830\n\n\n\n8.3.1 Mapping the Local Moran’s I\nBefore mapping the local Moran’s I map, we append the local Moran’s I dataframe (i.e. localMI) onto hunan SpatialPolygonDataFrame, hunan.localMI.\n\nhunan.localMI &lt;- cbind(hunan,localMI) %&gt;%\n  rename(Pr.Ii = Pr.z....E.Ii..)\nhead(hunan.localMI)\n\nSimple feature collection with 6 features and 11 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 110.4922 ymin: 28.61762 xmax: 112.3013 ymax: 30.12812\nGeodetic CRS:  WGS 84\n   NAME_2  ID_3  NAME_3   ENGTYPE_3  County GDPPC           Ii          E.Ii\n1 Changde 21098 Anxiang      County Anxiang 23667 -0.001468468 -2.815006e-05\n2 Changde 21100 Hanshou      County Hanshou 20981  0.025878173 -6.061953e-04\n3 Changde 21101  Jinshi County City  Jinshi 34592 -0.011987646 -5.366648e-03\n4 Changde 21102      Li      County      Li 24473  0.001022468 -2.404783e-07\n5 Changde 21103   Linli      County   Linli 25554  0.014814881 -6.829362e-05\n6 Changde 21104  Shimen      County  Shimen 27137 -0.038793829 -3.860263e-04\n        Var.Ii        Z.Ii     Pr.Ii                       geometry\n1 4.723841e-04 -0.06626904 0.9471636 POLYGON ((112.0625 29.75523...\n2 1.016664e-02  0.26266425 0.7928094 POLYGON ((112.2288 29.11684...\n3 1.133362e-01 -0.01966705 0.9843090 POLYGON ((111.8927 29.6013,...\n4 5.105969e-06  0.45259801 0.6508382 POLYGON ((111.3731 29.94649...\n5 1.449949e-03  0.39085814 0.6959021 POLYGON ((111.6324 29.76288...\n6 6.475559e-03 -0.47728835 0.6331568 POLYGON ((110.8825 30.11675...\n\n\n\n\n8.3.2 Mapping local Moran’s I values\nWe can plot the local Moran’s I values using choropleth mapping functions of tmap package.\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\",\n          palette = \"RdBu\",\n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n8.3.3 Mapping local Moran’s I p-values\nThe choropleth shows there is evidence for both positive and negative Ii values. However, it is useful to consider the p-values for each of these values\nThe code block below produce a choropleth map of Moran’s I p-values by using functions of tmap package.\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n8.3.4 Mapping both local Moran’s I values and p-values\nFor effective interpretation, it is better to plot both the local Moran’s I values map and its corresponding p-values map next to each other.\nThe code block below will be used to create such visualisation.\n\nlocalMI.map &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\", \n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)\n\npvalue.map &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(localMI.map, pvalue.map, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n# find county with max Ii and observe its Ii, p-value\nmax_row &lt;- hunan.localMI[which.max(hunan.localMI$Ii), , drop = FALSE]\nmax_row\n\nSimple feature collection with 1 feature and 11 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 112.8907 ymin: 27.91915 xmax: 113.506 ymax: 28.66025\nGeodetic CRS:  WGS 84\n     NAME_2  ID_3   NAME_3 ENGTYPE_3   County GDPPC       Ii       E.Ii\n84 Changsha 21107 Changsha  District Changsha 88656 4.902202 -0.2134796\n     Var.Ii    Z.Ii        Pr.Ii                       geometry\n84 2.319447 3.35901 0.0007822232 POLYGON ((112.9421 28.03722...\n\n\n\n# find county with max Ii and observe its Ii, p-value\nmin_row &lt;- hunan.localMI[which.min(hunan.localMI$Ii), , drop = FALSE]\nmin_row\n\nSimple feature collection with 1 feature and 11 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 111.3138 ymin: 27.53506 xmax: 111.6069 ymax: 27.81732\nGeodetic CRS:  WGS 84\n   NAME_2  ID_3        NAME_3   ENGTYPE_3        County GDPPC        Ii\n34  Loudi 21143 Lengshuijiang County City Lengshuijiang 64257 -1.790335\n          E.Ii   Var.Ii      Z.Ii    Pr.Ii                       geometry\n34 -0.08212937 2.159843 -1.162329 0.245102 POLYGON ((111.5307 27.81472...\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe plot above consists of two maps: one showing the Local Moran’s I statistics (Ii) and the other displaying the corresponding p-values for Local Moran’s I statistics.\n\n8.3.4.1 Left Plot: Local Moran’s I Statistics\nColor Scale: - The color scale ranges from light yellow to dark green, representing different ranges of Local Moran’s I values (Ii).\n\nDark Green Areas: Represent counties with high positive Local Moran’s I values (between 3 and 5). These areas show strong positive spatial autocorrelation, indicating clusters where counties have similar high GDP per capita values compared to their neighbors.\nLight Yellow Areas: Represent counties with lower Local Moran’s I values (around 0 to 1). These areas have weaker spatial autocorrelation, suggesting less significant clustering or similarity with their neighbors.\nOrange Areas: Represent negative Local Moran’s I values (between -2 to 0). These are areas where counties have significantly different GDP per capita values from their neighbors (spatial outliers).\n\n\n\n8.3.4.2 Right Plot: Local Moran’s I p-values\nColor Scale: - The color scale ranges from light blue to dark blue, representing different ranges of p-values for Local Moran’s I statistics.\n\nDark Blue Areas: Represent counties with very low p-values (less than 0.001), indicating that the observed spatial clustering is statistically significant at a very high confidence level.\nLighter Blue Areas: Represent counties with higher p-values (e.g., between 0.01 and 0.10), suggesting that the clustering is less statistically significant.\nVery Light Blue Areas: Represent counties with p-values greater than 0.10, indicating that there is no statistically significant spatial autocorrelation.\n\n\n\n8.3.4.3 Observations:\n\nThe map shows that the central-eastern region (around the Changsha county) has several dark blue counties with very low p-values, indicating strong evidence of significant spatial clustering of similar GDP per capita values."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#creating-a-lisa-cluster-map",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#creating-a-lisa-cluster-map",
    "title": "5B: Local Measures of Spatial Autocorrelation",
    "section": "9 Creating a LISA Cluster Map",
    "text": "9 Creating a LISA Cluster Map\nThe LISA Cluster Map shows the significant locations color coded by type of spatial autocorrelation.\nBefore we can generate the LISA cluster map, we have to plot the Moran scatterplot.\n\n9.1 Plotting Moran scatterplot\nThe Moran scatterplot is an illustration of the relationship between the values of the chosen attribute at each location and the average value of the same attribute at neighboring locations.\nThe code block below plots the Moran scatterplot of GDPPC 2012 by using moran.plot() of spdep.\n\nnci &lt;- moran.plot(hunan$GDPPC, rswm_q,\n                  labels=as.character(hunan$County), \n                  xlab=\"GDPPC 2012\", \n                  ylab=\"Spatially Lag GDPPC 2012\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nObservations The plot is split in 4 quadrants. The top right corner belongs to areas that have high GDPPC and are surrounded by other areas that have the average level of GDPPC. This are the high-high locations in the lesson slide, recall:\n\n\n\n\n\n9.2 Plotting Moran scatterplot with Standardised Variable\nTo plot Moran scatterplot with standardised variable:\n\nUse scale() to center and scale the variable. Centering is done by subtracting the mean (omitting NAs) the corresponding columns, and scaling is done by dividing the (centered) variable by their standard deviations.\nUse as.vector() to ensure that the standardized output is treated as a vector, which is necessary for proper mapping into the output data frame.\nPlot the Moran scatterplot\n\n\nhunan$Z.GDPPC &lt;- scale(hunan$GDPPC) %&gt;% \n  as.vector \n\n\nnci2 &lt;- moran.plot(hunan$Z.GDPPC, rswm_q,\n                   labels=as.character(hunan$County),\n                   xlab=\"z-GDPPC 2012\", \n                   ylab=\"Spatially Lag z-GDPPC 2012\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that the plot is similar to the previous plot. After scaling it, the cut off axis for x and y-axis is at 0.\n\n\n\n\n9.3 Preparing LISA map classes\nThe code block below show the steps to prepare a LISA cluster map.\n\nConvert to Vector\n\n\nquadrant &lt;- vector(mode=\"numeric\",\n                   length=nrow(localMI))\n\n\nderive the spatially lagged variable of interest (i.e. GDPPC) and centers the spatially lagged variable around its mean.\n\n\nhunan$lag_GDPPC &lt;- lag.listw(rswm_q, \n                             hunan$GDPPC)\n\nDV &lt;- hunan$lag_GDPPC - mean(hunan$lag_GDPPC)     \n\n\ncenter the local Moran’s variable around the mean.\n\n\nLM_I &lt;- localMI[,1] - mean(localMI[,1])    \n\n\nset a statistical significance level (alpha value) for the local Moran.\n\n\nsignif &lt;- 0.05       \n\n\ndefine quadrants. The four command lines define the low-low (1), low-high (2), high-low (3) and high-high (4) categories.\n\n\nquadrant[DV &lt;0 & LM_I&gt;0] &lt;- 1\nquadrant[DV &gt;0 & LM_I&lt;0] &lt;- 2\nquadrant[DV &lt;0 & LM_I&lt;0] &lt;- 3  \nquadrant[DV &gt;0 & LM_I&gt;0] &lt;- 4      \n\n\nplace non-significant Moran in the category 0.\n\n\nquadrant[localMI[,5]&gt;signif] &lt;- 0\n\n\n\n9.4 Plotting LISA map\nNow, we can build the LISA map:\n\nhunan.localMI$quadrant &lt;- quadrant\ncolors &lt;- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters &lt;- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)\n\n\n\n\n\n\n\n\nFor effective interpretation, it is better to plot both the local Moran’s I values map and its corresponding p-values map next to each other.\nTo create such visualisation:\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\n\nhunan.localMI$quadrant &lt;- quadrant\ncolors &lt;- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters &lt;- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\nLISAmap &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)\n\ntmap_arrange(gdppc, LISAmap, \n             asp=1, ncol=2)\n\n\n\n\n\n\n\n\nWe can also include the local Moran’s I map and p-value map as shown below for easy comparison.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQuestion: What statistical observations can you draw from the LISA map above?\nFrom the LISA map and GDPPC map, there is a significant “high-high” cluster in the central-eastern part of the province, where counties with high GDP per capita are surrounded by similar counties.\nThis pattern is reinforced by the Local Moran’s I statistics map, which show the same region in a deep green shade, indicating strong positive spatial autocorrelation. The corresponding low p-values further confirm the statistical significance of this economic clustering.\nNotably, the high-high cluster on the LISA map extends over more counties than those highlighted by the Local Moran’s I statistics."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#hot-spot-and-cold-spot-area-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#hot-spot-and-cold-spot-area-analysis",
    "title": "5B: Local Measures of Spatial Autocorrelation",
    "section": "10 Hot Spot and Cold Spot Area Analysis",
    "text": "10 Hot Spot and Cold Spot Area Analysis\nBeside detecting cluster and outliers, localised spatial statistics can be also used to detect hot spot and/or cold spot areas.\nThe term ‘hot spot’ has been used generically across disciplines to describe a region or value that is higher relative to its surroundings (Lepers et al 2005, Aben et al 2012, Isobe et al 2015).\n\n10.1 Getis and Ord’s G-Statistics\nAn alternative spatial statistics to detect spatial anomalies is the Getis and Ord’s G-statistics (Getis and Ord, 1972; Ord and Getis, 1995). It looks at neighbours within a defined proximity to identify where either high or low values clutser spatially. Here, statistically significant hot-spots are recognised as areas of high values where other areas within a neighbourhood range also share high values too.\nThe analysis consists of three steps:\n\nDeriving spatial weight matrix\nComputing Gi statistics\nMapping Gi statistics\n\n\n\n10.2 Deriving Distance-based Weight Matrix\nFirst, we need to define a new set of neighbors. Unlike spatial autocorrelation, which considers units sharing borders, the Getis-Ord method defines neighbors based on distance.\nThere are two types of distance-based proximity matrices:\n\nFixed Distance Weight Matrix: Neighbors are defined within a fixed distance.\nAdaptive Distance Weight Matrix: Neighbors are defined based on a varying distance that adapts to include a specified number of nearest neighbors.\n\n\n10.2.1 Deriving the Centroid\nTo create a connectivity graph, we first need to associate points (centroids) with each polygon in our spatial data. This process involves more than simply running st_centroid() on the us.bound sf object; we need to extract coordinates into a separate data frame.\nWe achieve this using a mapping function, which applies a specific function to each element of a vector and returns a new vector of the same length. Here, the input vector is the geometry column of us.bound, and the function applied is st_centroid(). We’ll use the map_dbl function from the purrr package to do this. For more details, refer to the map documentation.\nTo extract longitude values, we map the st_centroid() function over the geometry column of us.bound and access the longitude using double bracket notation [[ ]] and 1, which retrieves the first value (longitude) from each centroid.\n\nlongitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])\n\nWe do the same for latitude with one key difference. We access the second value per each centroid with [[2]].\n\nlatitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\n\nNow that we have latitude and longitude, we use cbind to put longitude and latitude into the same object.\n\ncoords &lt;- cbind(longitude, latitude)\n\n\n\n10.2.2 Determine the Cut-off Distance\nTo determine the upper limit for the distance band:\n\nUse the knearneigh() function from spdep to create a matrix containing the indices of the k nearest neighbors for each point.\nConvert the knn object from knearneigh() into a neighbor list (nb class) using knn2nb(). This list contains integer vectors representing the neighbor region numbers.\nUse nbdists() from spdep to calculate the lengths of the neighbor relationships (distances). If coordinates are projected, the distances are in the units of the coordinates; otherwise, they are in kilometers.\nFlatten the list structure of the returned distances using unlist().\n\n\n#coords &lt;- coordinates(hunan)\nk1 &lt;- knn2nb(knearneigh(coords))\nk1dists &lt;- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79 \n\n\n\n\n\n\n\n\nNote\n\n\n\nUsing the summary report, we can observe that the largest first nearest neighbour distance is 61.79 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour.\n\n\n\n\n10.2.3 Computing Fixed Distance Weight Matrix\nUse dnearneigh() to compute distance weight matrix:\n\n# get max dist from k1dists rounded up to integer\nmax_dist &lt;- as.integer(ceiling(max(k1dists)))\n\nwm_d62 &lt;- dnearneigh(x=coords, d1=0, d2=max_dist, longlat = TRUE)\nwm_d62\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \n\n\nNext, nb2listw() is used to convert the nb object into spatial weights object.\n\nwm62_lw &lt;- nb2listw(wm_d62, style = 'B')\nsummary(wm62_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \nLink number distribution:\n\n 1  2  3  4  5  6 \n 6 15 14 26 20  7 \n6 least connected regions:\n6 15 30 32 56 65 with 1 link\n7 most connected regions:\n21 28 35 45 50 52 82 with 6 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1   S2\nB 88 7744 324 648 5440\n\n\nThe output spatial weights object is called wm62_lw.\n\n\n\n10.3 Computing adaptive distance weight matrix\nOne of the characteristics of fixed distance weight matrix is that more densely settled areas (usually the urban areas) tend to have more neighbours and the less densely settled areas (usually the rural counties) tend to have lesser neighbours. Having many neighbours smoothes the neighbour relationship across more neighbours.\nIt is possible to control the numbers of neighbours directly using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry as shown in the code block below.\n\n# set nearest neighbour as 8\nknn &lt;- knn2nb(knearneigh(coords, k=8))\nknn\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 704 \nPercentage nonzero weights: 9.090909 \nAverage number of links: 8 \nNon-symmetric neighbours list\n\n\nNext, nb2listw() is used to convert the nb object into spatial weights object.\n\nknn_lw &lt;- nb2listw(knn, style = 'B')\nsummary(knn_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 704 \nPercentage nonzero weights: 9.090909 \nAverage number of links: 8 \nNon-symmetric neighbours list\nLink number distribution:\n\n 8 \n88 \n88 least connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 with 8 links\n88 most connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 with 8 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 88 7744 704 1300 23014"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#computing-gi-statistics",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#computing-gi-statistics",
    "title": "5B: Local Measures of Spatial Autocorrelation",
    "section": "11 Computing Gi statistics",
    "text": "11 Computing Gi statistics\n\n11.1 Gi statistics using fixed distance\n\nfips &lt;- order(hunan$County)\ngi.fixed &lt;- localG(hunan$GDPPC, wm62_lw)\ngi.fixed\n\n [1]  0.436075843 -0.265505650 -0.073033665  0.413017033  0.273070579\n [6] -0.377510776  2.863898821  2.794350420  5.216125401  0.228236603\n[11]  0.951035346 -0.536334231  0.176761556  1.195564020 -0.033020610\n[16]  1.378081093 -0.585756761 -0.419680565  0.258805141  0.012056111\n[21] -0.145716531 -0.027158687 -0.318615290 -0.748946051 -0.961700582\n[26] -0.796851342 -1.033949773 -0.460979158 -0.885240161 -0.266671512\n[31] -0.886168613 -0.855476971 -0.922143185 -1.162328599  0.735582222\n[36] -0.003358489 -0.967459309 -1.259299080 -1.452256513 -1.540671121\n[41] -1.395011407 -1.681505286 -1.314110709 -0.767944457 -0.192889342\n[46]  2.720804542  1.809191360 -1.218469473 -0.511984469 -0.834546363\n[51] -0.908179070 -1.541081516 -1.192199867 -1.075080164 -1.631075961\n[56] -0.743472246  0.418842387  0.832943753 -0.710289083 -0.449718820\n[61] -0.493238743 -1.083386776  0.042979051  0.008596093  0.136337469\n[66]  2.203411744  2.690329952  4.453703219 -0.340842743 -0.129318589\n[71]  0.737806634 -1.246912658  0.666667559  1.088613505 -0.985792573\n[76]  1.233609606 -0.487196415  1.626174042 -1.060416797  0.425361422\n[81] -0.837897118 -0.314565243  0.371456331  4.424392623 -0.109566928\n[86]  1.364597995 -1.029658605 -0.718000620\nattr(,\"internals\")\n               Gi      E(Gi)        V(Gi)        Z(Gi) Pr(z != E(Gi))\n [1,] 0.064192949 0.05747126 2.375922e-04  0.436075843   6.627817e-01\n [2,] 0.042300020 0.04597701 1.917951e-04 -0.265505650   7.906200e-01\n [3,] 0.044961480 0.04597701 1.933486e-04 -0.073033665   9.417793e-01\n [4,] 0.039475779 0.03448276 1.461473e-04  0.413017033   6.795941e-01\n [5,] 0.049767939 0.04597701 1.927263e-04  0.273070579   7.847990e-01\n [6,] 0.008825335 0.01149425 4.998177e-05 -0.377510776   7.057941e-01\n [7,] 0.050807266 0.02298851 9.435398e-05  2.863898821   4.184617e-03\n [8,] 0.083966739 0.04597701 1.848292e-04  2.794350420   5.200409e-03\n [9,] 0.115751554 0.04597701 1.789361e-04  5.216125401   1.827045e-07\n[10,] 0.049115587 0.04597701 1.891013e-04  0.228236603   8.194623e-01\n[11,] 0.045819180 0.03448276 1.420884e-04  0.951035346   3.415864e-01\n[12,] 0.049183846 0.05747126 2.387633e-04 -0.536334231   5.917276e-01\n[13,] 0.048429181 0.04597701 1.924532e-04  0.176761556   8.596957e-01\n[14,] 0.034733752 0.02298851 9.651140e-05  1.195564020   2.318667e-01\n[15,] 0.011262043 0.01149425 4.945294e-05 -0.033020610   9.736582e-01\n[16,] 0.065131196 0.04597701 1.931870e-04  1.378081093   1.681783e-01\n[17,] 0.027587075 0.03448276 1.385862e-04 -0.585756761   5.580390e-01\n[18,] 0.029409313 0.03448276 1.461397e-04 -0.419680565   6.747188e-01\n[19,] 0.061466754 0.05747126 2.383385e-04  0.258805141   7.957856e-01\n[20,] 0.057656917 0.05747126 2.371303e-04  0.012056111   9.903808e-01\n[21,] 0.066518379 0.06896552 2.820326e-04 -0.145716531   8.841452e-01\n[22,] 0.045599896 0.04597701 1.928108e-04 -0.027158687   9.783332e-01\n[23,] 0.030646753 0.03448276 1.449523e-04 -0.318615290   7.500183e-01\n[24,] 0.035635552 0.04597701 1.906613e-04 -0.748946051   4.538897e-01\n[25,] 0.032606647 0.04597701 1.932888e-04 -0.961700582   3.362000e-01\n[26,] 0.035001352 0.04597701 1.897172e-04 -0.796851342   4.255374e-01\n[27,] 0.012746354 0.02298851 9.812587e-05 -1.033949773   3.011596e-01\n[28,] 0.061287917 0.06896552 2.773884e-04 -0.460979158   6.448136e-01\n[29,] 0.014277403 0.02298851 9.683314e-05 -0.885240161   3.760271e-01\n[30,] 0.009622875 0.01149425 4.924586e-05 -0.266671512   7.897221e-01\n[31,] 0.014258398 0.02298851 9.705244e-05 -0.886168613   3.755267e-01\n[32,] 0.005453443 0.01149425 4.986245e-05 -0.855476971   3.922871e-01\n[33,] 0.043283712 0.05747126 2.367109e-04 -0.922143185   3.564539e-01\n[34,] 0.020763514 0.03448276 1.393165e-04 -1.162328599   2.451020e-01\n[35,] 0.081261843 0.06896552 2.794398e-04  0.735582222   4.619850e-01\n[36,] 0.057419907 0.05747126 2.338437e-04 -0.003358489   9.973203e-01\n[37,] 0.013497133 0.02298851 9.624821e-05 -0.967459309   3.333145e-01\n[38,] 0.019289310 0.03448276 1.455643e-04 -1.259299080   2.079223e-01\n[39,] 0.025996272 0.04597701 1.892938e-04 -1.452256513   1.464303e-01\n[40,] 0.016092694 0.03448276 1.424776e-04 -1.540671121   1.233968e-01\n[41,] 0.035952614 0.05747126 2.379439e-04 -1.395011407   1.630124e-01\n[42,] 0.031690963 0.05747126 2.350604e-04 -1.681505286   9.266481e-02\n[43,] 0.018750079 0.03448276 1.433314e-04 -1.314110709   1.888090e-01\n[44,] 0.015449080 0.02298851 9.638666e-05 -0.767944457   4.425202e-01\n[45,] 0.065760689 0.06896552 2.760533e-04 -0.192889342   8.470456e-01\n[46,] 0.098966900 0.05747126 2.326002e-04  2.720804542   6.512325e-03\n[47,] 0.085415780 0.05747126 2.385746e-04  1.809191360   7.042128e-02\n[48,] 0.038816536 0.05747126 2.343951e-04 -1.218469473   2.230456e-01\n[49,] 0.038931873 0.04597701 1.893501e-04 -0.511984469   6.086619e-01\n[50,] 0.055098610 0.06896552 2.760948e-04 -0.834546363   4.039732e-01\n[51,] 0.033405005 0.04597701 1.916312e-04 -0.908179070   3.637836e-01\n[52,] 0.043040784 0.06896552 2.829941e-04 -1.541081516   1.232969e-01\n[53,] 0.011297699 0.02298851 9.615920e-05 -1.192199867   2.331829e-01\n[54,] 0.040968457 0.05747126 2.356318e-04 -1.075080164   2.823388e-01\n[55,] 0.023629663 0.04597701 1.877170e-04 -1.631075961   1.028743e-01\n[56,] 0.006281129 0.01149425 4.916619e-05 -0.743472246   4.571958e-01\n[57,] 0.063918654 0.05747126 2.369553e-04  0.418842387   6.753313e-01\n[58,] 0.070325003 0.05747126 2.381374e-04  0.832943753   4.048765e-01\n[59,] 0.025947288 0.03448276 1.444058e-04 -0.710289083   4.775249e-01\n[60,] 0.039752578 0.04597701 1.915656e-04 -0.449718820   6.529132e-01\n[61,] 0.049934283 0.05747126 2.334965e-04 -0.493238743   6.218439e-01\n[62,] 0.030964195 0.04597701 1.920248e-04 -1.083386776   2.786368e-01\n[63,] 0.058129184 0.05747126 2.343319e-04  0.042979051   9.657182e-01\n[64,] 0.046096514 0.04597701 1.932637e-04  0.008596093   9.931414e-01\n[65,] 0.012459080 0.01149425 5.008051e-05  0.136337469   8.915545e-01\n[66,] 0.091447733 0.05747126 2.377744e-04  2.203411744   2.756574e-02\n[67,] 0.049575872 0.02298851 9.766513e-05  2.690329952   7.138140e-03\n[68,] 0.107907212 0.04597701 1.933581e-04  4.453703219   8.440175e-06\n[69,] 0.019616151 0.02298851 9.789454e-05 -0.340842743   7.332220e-01\n[70,] 0.032923393 0.03448276 1.454032e-04 -0.129318589   8.971056e-01\n[71,] 0.030317663 0.02298851 9.867859e-05  0.737806634   4.606320e-01\n[72,] 0.019437582 0.03448276 1.455870e-04 -1.246912658   2.124295e-01\n[73,] 0.055245460 0.04597701 1.932838e-04  0.666667559   5.049845e-01\n[74,] 0.074278054 0.05747126 2.383538e-04  1.088613505   2.763244e-01\n[75,] 0.013269580 0.02298851 9.719982e-05 -0.985792573   3.242349e-01\n[76,] 0.049407829 0.03448276 1.463785e-04  1.233609606   2.173484e-01\n[77,] 0.028605749 0.03448276 1.455139e-04 -0.487196415   6.261191e-01\n[78,] 0.039087662 0.02298851 9.801040e-05  1.626174042   1.039126e-01\n[79,] 0.031447120 0.04597701 1.877464e-04 -1.060416797   2.889550e-01\n[80,] 0.064005294 0.05747126 2.359641e-04  0.425361422   6.705732e-01\n[81,] 0.044606529 0.05747126 2.357330e-04 -0.837897118   4.020885e-01\n[82,] 0.063700493 0.06896552 2.801427e-04 -0.314565243   7.530918e-01\n[83,] 0.051142205 0.04597701 1.933560e-04  0.371456331   7.102977e-01\n[84,] 0.102121112 0.04597701 1.610278e-04  4.424392623   9.671399e-06\n[85,] 0.021901462 0.02298851 9.843172e-05 -0.109566928   9.127528e-01\n[86,] 0.064931813 0.04597701 1.929430e-04  1.364597995   1.723794e-01\n[87,] 0.031747344 0.04597701 1.909867e-04 -1.029658605   3.031703e-01\n[88,] 0.015893319 0.02298851 9.765131e-05 -0.718000620   4.727569e-01\nattr(,\"cluster\")\n [1] Low  Low  High High High High High High High Low  Low  High Low  Low  Low \n[16] High High High High Low  High High Low  Low  High Low  Low  Low  Low  Low \n[31] Low  Low  Low  High Low  Low  Low  Low  Low  Low  High Low  Low  Low  Low \n[46] High High Low  Low  Low  Low  High Low  Low  Low  Low  Low  High Low  Low \n[61] Low  Low  Low  High High High Low  High Low  Low  High Low  High High Low \n[76] High Low  Low  Low  Low  Low  Low  High High Low  High Low  Low \nLevels: Low High\nattr(,\"gstari\")\n[1] FALSE\nattr(,\"call\")\nlocalG(x = hunan$GDPPC, listw = wm62_lw)\nattr(,\"class\")\n[1] \"localG\"\n\n\nThe output of localG() is a vector of G or Gstar values, with attributes “gstari” set to TRUE or FALSE, “call” set to the function call, and class “localG”.\nThe Gi statistic is expressed as a Z-score, where higher values indicate stronger clustering. The direction (positive or negative) shows whether the clusters are high or low.\nNext, we’ll join the Gi values to the corresponding hunan sf data frame using the following code:\n\nhunan.gi &lt;- cbind(hunan, as.matrix(gi.fixed)) %&gt;%\n  rename(gstat_fixed = as.matrix.gi.fixed.)\n\nThis code performs three tasks: 1. Converts the output vector (gi.fixed) to an R matrix using as.matrix(). 2. Combines the hunan data and the gi.fixed matrix into a new spatial data frame (hunan.gi) using cbind(). 3. Renames the Gi values column to gstat_fixed using rename().\n\n\n11.2 Mapping Gi Values with Fixed Distance Weights\nThe code block below shows the functions used to map the Gi values derived using fixed distance weight matrix.\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\n\nGimap_fd &lt;-tm_shape(hunan.gi) +\n  tm_fill(col = \"gstat_fixed\", \n          style = \"pretty\",\n          palette=\"-RdBu\",\n          title = \"local Gi\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(title = \"Gi Map using Fixed Distance Weight Matrix\")\n\n\ntmap_arrange(gdppc, Gimap_fd, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQuestion: What statistical observation can you draw from the Gi map above?\nsee below with adaptive weight distance matrix viz.\n\n\n\n\n11.3 Gi statistics using adaptive distance\nNext, we use similar steps to compute the Gi values for GDPPC2012 by using an adaptive distance weight matrix (i.e knb_lw) and compare the methodology.\n\nfips &lt;- order(hunan$County)\ngi.adaptive &lt;- localG(hunan$GDPPC, knn_lw)\nhunan.gi &lt;- cbind(hunan, as.matrix(gi.adaptive)) %&gt;%\n  rename(gstat_adaptive = as.matrix.gi.adaptive.)\n\n\n# adaptive distance\ngdppc&lt;- qtm(hunan, \"GDPPC\")\n\nGimap_ad &lt;- tm_shape(hunan.gi) + \n  tm_fill(col = \"gstat_adaptive\", \n          style = \"pretty\", \n          palette=\"-RdBu\", \n          title = \"local Gi\") + \n  tm_borders(alpha = 0.5)+ \n  tm_layout(title = \"Gi Map using Adaptive Distance Weight Matrix\")\n\n\ntmap_arrange(gdppc, \n             Gimap_ad, \n             asp=1, \n             ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQuestion: What statistical observation can you draw from the Gi maps (comparing between Fixed and Adaptive Weight matrix) above?\nBoth methods identify similar clusters in the central-eastern (hot spots), and western regions (cold spots), confirming consistent spatial patterns.\nThe fixed distance approach captures more localized clusters, while the adaptive distance approach reveals broader patterns (smoothing effect discussed above), adjusting dynamically to neighborhood density.\nAlso note that the range of legend is slightly different across the 2 methods."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06a.html",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06a.html",
    "title": "6A: Geographical Segmentation with Spatially Constrained Clustering Techniques",
    "section": "",
    "text": "R for Geospatial Data Science and Analytics - 12  Geographical Segmentation with Spatially Constrained Clustering Techniques"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06a.html#exercise-6a-reference",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06a.html#exercise-6a-reference",
    "title": "6A: Geographical Segmentation with Spatially Constrained Clustering Techniques",
    "section": "",
    "text": "R for Geospatial Data Science and Analytics - 12  Geographical Segmentation with Spatially Constrained Clustering Techniques"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06a.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06a.html#overview",
    "title": "6A: Geographical Segmentation with Spatially Constrained Clustering Techniques",
    "section": "2 Overview",
    "text": "2 Overview\nIn this exercise, we will learn to delineate homogeneous regions using hierarchical and spatially constrained clustering techniques on geographically referenced multivariate data."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06a.html#learning-outcome",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06a.html#learning-outcome",
    "title": "6A: Geographical Segmentation with Spatially Constrained Clustering Techniques",
    "section": "3 Learning Outcome",
    "text": "3 Learning Outcome\n\nConvert GIS polygon data to R’s simple feature data frame.\nConvert simple feature data frame to R’s SpatialPolygonDataFrame object.\nPerform cluster analysis with hclust().\nConduct spatially constrained cluster analysis using skater().\nVisualize analysis outputs using ggplot2 and tmap."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06a.html#the-analytical-question",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06a.html#the-analytical-question",
    "title": "6A: Geographical Segmentation with Spatially Constrained Clustering Techniques",
    "section": "4 The Analytical Question",
    "text": "4 The Analytical Question\nIn geobusiness and spatial policy, delineating market or planning areas into homogeneous regions using multivariate data is a common approach.\n\nIn this exercise, we aim to divide Shan State, Myanmar into homogeneous regions based on various Information and Communication Technology (ICT) indicators, such as radio, television, landline phones, mobile phones, computers, and home internet access."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06a.html#the-data",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06a.html#the-data",
    "title": "6A: Geographical Segmentation with Spatially Constrained Clustering Techniques",
    "section": "5 The Data",
    "text": "5 The Data\nThe following 2 datasets will be used in this study.\n\n\n\n\n\n\n\n\nData Set\nDescription\nFormat\n\n\n\n\nmyanmar_township_boundaries\nGIS data in ESRI shapefile format containing township boundary information of Myanmar, represented as polygon features.\nESRI Shapefile\n\n\nShan-ICT.csv\nExtract of the 2014 Myanmar Population and Housing Census at the township level.\nCSV\n\n\n\nBoth datasets are downloaded from the Myanmar Information Management Unit (MIMU)."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06a.html#installing-and-launching-the-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06a.html#installing-and-launching-the-r-packages",
    "title": "6A: Geographical Segmentation with Spatially Constrained Clustering Techniques",
    "section": "6 Installing and Launching the R Packages",
    "text": "6 Installing and Launching the R Packages\nTo install and load these packages, use the following code:\n\npacman::p_load(spdep, tmap, sf, ClustGeo, \n               ggpubr, cluster, factoextra, NbClust,\n               heatmaply, corrplot, psych, tidyverse, GGally, plotly, knitr)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06a.html#import-data-and-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06a.html#import-data-and-preparation",
    "title": "6A: Geographical Segmentation with Spatially Constrained Clustering Techniques",
    "section": "7 Import Data and Preparation",
    "text": "7 Import Data and Preparation\n\n7.1 Import Geospatial Shapefile\nFirstly, we will use st_read() of sf package to import Myanmar Township Boundary shapefile into R. The imported shapefile will be simple features Object of sf.\n\nshan_sf &lt;- st_read(dsn = \"data/geospatial\", layer = \"myanmar_township_boundaries\")\n\nReading layer `myanmar_township_boundaries' from data source \n  `/Users/walter/code/isss626/isss626-gaa/Hands-on_Ex/Hands-on_Ex06/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 330 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.17275 ymin: 9.671252 xmax: 101.1699 ymax: 28.54554\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOBJECTID\nST\nST_PCODE\nDT\nDT_PCODE\nTS\nTS_PCODE\nST_2\nLABEL2\nSELF_ADMIN\nST_RG\nT_NAME_WIN\nT_NAME_M3\nAREA\ngeometry\n\n\n\n\n250\nKachin\nMMR001\nMohnyin\nMMR001D002\nHpakant\nMMR001009\nKachin State\nHpakant\n\n\n\n\n\n\n\n\n169795\nNA\nState\nzm;uefU\nဖားကန့်\n5761.2964\nMULTIPOLYGON (((96.15953 26…\n\n\n\n\n\n\n\n\n\n\n163\nShan (North)\nMMR015\nMongmit\nMMR015D008\nMongmit\nMMR015017\nShan State (North)\nMongmit\n\n\n\n\n\n\n\n\n61072\nNA\nState\nrdk;rdwf\nမိုးမိတ်\n2703.6114\nMULTIPOLYGON (((96.96001 23…\n\n\n\n\n\n\n\n\n\n\n96\nBago (East)\nMMR007\nBago\nMMR007D001\nWaw\nMMR007004\nBago Region (East)\nWaw\n\n\n\n\n\n\n\n\n199032\nNA\nRegion\na0g\nဝေါ\n952.4398\nMULTIPOLYGON (((96.61505 17…\n\n\n\n\n\n\n\n\n\n\n147\nBago (West)\nMMR008\nPyay\nMMR008D001\nPaukkhaung\nMMR008002\nBago Region (West)\nPaukkhaung\n\n\n\n\n\n\n\n\n117164\nNA\nRegion\naygufacgif;\nပေါက်ခေါင်း\n1918.6734\nMULTIPOLYGON (((95.82708 19…\n\n\n\n\n\n\n\n\n\n\n263\nMandalay\nMMR010\nPyinoolwin\nMMR010D002\nMogoke\nMMR010011\nMandalay Region\nMogoke\n\n\n\n\n\n\n\n\n191775\nNA\nRegion\nrdk;ukwf\nမိုးကုတ်\n1178.5076\nMULTIPOLYGON (((96.22371 23…\n\n\n\n\n\n\n\n\n\n\n167\nKachin\nMMR001\nBhamo\nMMR001D003\nShwegu\nMMR001011\nKachin State\nShwegu\n\n\n\n\n\n\n\n\n84750\nNA\nState\na&Tul\nရွှေကူ\n3088.8291\nMULTIPOLYGON (((96.68573 24…\n\n\n\n\n\n\n\n\n\n\n\n\n\nSince our study area is the Shan state, we will examine the state list in the dataframe for the relevant state names (keys) for filtering.\n\n# Display unique values in the \"ST\" column sorted in ascending order\nsort(unique(shan_sf$ST))\n\n [1] \"Ayeyarwady\"   \"Bago (East)\"  \"Bago (West)\"  \"Chin\"         \"Kachin\"      \n [6] \"Kayah\"        \"Kayin\"        \"Magway\"       \"Mandalay\"     \"Mon\"         \n[11] \"Nay Pyi Taw\"  \"Rakhine\"      \"Sagaing\"      \"Shan (East)\"  \"Shan (North)\"\n[16] \"Shan (South)\" \"Tanintharyi\"  \"Yangon\"      \n\n\n\nshan_sf &lt;- \n  shan_sf %&gt;%\n  filter(ST %in% c(\"Shan (East)\", \"Shan (North)\", \"Shan (South)\")) %&gt;% \n  select(c(2:7))\n\nshan_sf\n\nSimple feature collection with 55 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 96.15107 ymin: 19.29932 xmax: 101.1699 ymax: 24.15907\nGeodetic CRS:  WGS 84\nFirst 10 features:\n             ST ST_PCODE       DT   DT_PCODE        TS  TS_PCODE\n1  Shan (North)   MMR015  Mongmit MMR015D008   Mongmit MMR015017\n2  Shan (South)   MMR014 Taunggyi MMR014D001   Pindaya MMR014006\n3  Shan (South)   MMR014 Taunggyi MMR014D001   Ywangan MMR014007\n4  Shan (South)   MMR014 Taunggyi MMR014D001  Pinlaung MMR014009\n5  Shan (North)   MMR015  Mongmit MMR015D008    Mabein MMR015018\n6  Shan (South)   MMR014 Taunggyi MMR014D001     Kalaw MMR014005\n7  Shan (South)   MMR014 Taunggyi MMR014D001     Pekon MMR014010\n8  Shan (South)   MMR014 Taunggyi MMR014D001  Lawksawk MMR014008\n9  Shan (North)   MMR015  Kyaukme MMR015D003 Nawnghkio MMR015013\n10 Shan (North)   MMR015  Kyaukme MMR015D003   Kyaukme MMR015012\n                         geometry\n1  MULTIPOLYGON (((96.96001 23...\n2  MULTIPOLYGON (((96.7731 21....\n3  MULTIPOLYGON (((96.78483 21...\n4  MULTIPOLYGON (((96.49518 20...\n5  MULTIPOLYGON (((96.66306 24...\n6  MULTIPOLYGON (((96.49518 20...\n7  MULTIPOLYGON (((97.14738 19...\n8  MULTIPOLYGON (((96.94981 22...\n9  MULTIPOLYGON (((96.75648 22...\n10 MULTIPOLYGON (((96.95498 22...\n\n\nThen, we filter the dataframe to contain only the Shan states.\n\n\n7.2 Import Aspatial csv File\nNext, we will import Shan-ICT.csv into R by using read_csv() of readr package. The output is R dataframe class.\n\nict &lt;- read_csv (\"data/aspatial/Shan-ICT.csv\")\n\n\nsummary(ict)\n\n District Pcode     District Name      Township Pcode     Township Name     \n Length:55          Length:55          Length:55          Length:55         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n Total households     Radio         Television    Land line phone \n Min.   : 3318    Min.   :  115   Min.   :  728   Min.   :  20.0  \n 1st Qu.: 8711    1st Qu.: 1260   1st Qu.: 3744   1st Qu.: 266.5  \n Median :13685    Median : 2497   Median : 6117   Median : 695.0  \n Mean   :18369    Mean   : 4487   Mean   :10183   Mean   : 929.9  \n 3rd Qu.:23471    3rd Qu.: 6192   3rd Qu.:13906   3rd Qu.:1082.5  \n Max.   :82604    Max.   :30176   Max.   :62388   Max.   :6736.0  \n  Mobile phone      Computer      Internet at home\n Min.   :  150   Min.   :  20.0   Min.   :   8.0  \n 1st Qu.: 2037   1st Qu.: 121.0   1st Qu.:  88.0  \n Median : 3559   Median : 244.0   Median : 316.0  \n Mean   : 6470   Mean   : 575.5   Mean   : 760.2  \n 3rd Qu.: 7177   3rd Qu.: 507.0   3rd Qu.: 630.5  \n Max.   :48461   Max.   :6705.0   Max.   :9746.0  \n\n\nThere are a total of 11 fields and 55 observation in the dataframe.\n\n\n7.3 Deriving New Variables Using the dplyr Package\nThe values in the dataset represent the number of households, which can be biased by the total number of households in each township. Townships with more households will naturally have higher numbers of households owning items like radios or TVs.\nTo address this bias, we will calculate the penetration rate for each ICT variable by dividing the number of households owning each item by the total number of households, then multiplying by 1000. This will normalize the data, allowing for a fair comparison between townships. Here is the code to perform this calculation:\n\nict_derived &lt;- ict %&gt;%\n  # feature engineering: normalize data\n  mutate(\n    RADIO_PR = Radio / `Total households` * 1000,\n    TV_PR = Television / `Total households` * 1000,\n    LLPHONE_PR = `Land line phone` / `Total households` * 1000,\n    MPHONE_PR = `Mobile phone` / `Total households` * 1000,\n    COMPUTER_PR = Computer / `Total households` * 1000,\n    INTERNET_PR = `Internet at home` / `Total households` * 1000\n  ) %&gt;%\n  # improve readability of col names\n  rename(\n    DT_PCODE = `District Pcode`, DT = `District Name`,\n    TS_PCODE = `Township Pcode`, TS = `Township Name`,\n    TT_HOUSEHOLDS = `Total households`,\n    RADIO = Radio, TV = Television,\n    LLPHONE = `Land line phone`, MPHONE = `Mobile phone`,\n    COMPUTER = Computer, INTERNET = `Internet at home`\n  )\n\nWe can then review the summary statistics of the newly derived penetration rates using:\n\nsummary(ict_derived)\n\n   DT_PCODE              DT              TS_PCODE              TS           \n Length:55          Length:55          Length:55          Length:55         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n TT_HOUSEHOLDS       RADIO             TV           LLPHONE      \n Min.   : 3318   Min.   :  115   Min.   :  728   Min.   :  20.0  \n 1st Qu.: 8711   1st Qu.: 1260   1st Qu.: 3744   1st Qu.: 266.5  \n Median :13685   Median : 2497   Median : 6117   Median : 695.0  \n Mean   :18369   Mean   : 4487   Mean   :10183   Mean   : 929.9  \n 3rd Qu.:23471   3rd Qu.: 6192   3rd Qu.:13906   3rd Qu.:1082.5  \n Max.   :82604   Max.   :30176   Max.   :62388   Max.   :6736.0  \n     MPHONE         COMPUTER         INTERNET         RADIO_PR     \n Min.   :  150   Min.   :  20.0   Min.   :   8.0   Min.   : 21.05  \n 1st Qu.: 2037   1st Qu.: 121.0   1st Qu.:  88.0   1st Qu.:138.95  \n Median : 3559   Median : 244.0   Median : 316.0   Median :210.95  \n Mean   : 6470   Mean   : 575.5   Mean   : 760.2   Mean   :215.68  \n 3rd Qu.: 7177   3rd Qu.: 507.0   3rd Qu.: 630.5   3rd Qu.:268.07  \n Max.   :48461   Max.   :6705.0   Max.   :9746.0   Max.   :484.52  \n     TV_PR         LLPHONE_PR       MPHONE_PR       COMPUTER_PR    \n Min.   :116.0   Min.   :  2.78   Min.   : 36.42   Min.   : 3.278  \n 1st Qu.:450.2   1st Qu.: 22.84   1st Qu.:190.14   1st Qu.:11.832  \n Median :517.2   Median : 37.59   Median :305.27   Median :18.970  \n Mean   :509.5   Mean   : 51.09   Mean   :314.05   Mean   :24.393  \n 3rd Qu.:606.4   3rd Qu.: 69.72   3rd Qu.:428.43   3rd Qu.:29.897  \n Max.   :842.5   Max.   :181.49   Max.   :735.43   Max.   :92.402  \n  INTERNET_PR     \n Min.   :  1.041  \n 1st Qu.:  8.617  \n Median : 22.829  \n Mean   : 30.644  \n 3rd Qu.: 41.281  \n Max.   :117.985  \n\n\nThis process adds six new fields to the data frame: RADIO_PR, TV_PR, LLPHONE_PR, MPHONE_PR, COMPUTER_PR, and INTERNET_PR, representing the penetration rates for each ICT variable."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06a.html#exploratory-data-analysis-eda",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06a.html#exploratory-data-analysis-eda",
    "title": "6A: Geographical Segmentation with Spatially Constrained Clustering Techniques",
    "section": "8 Exploratory Data Analysis (EDA)",
    "text": "8 Exploratory Data Analysis (EDA)\n\n8.1 Statistical Graphics\nHistogram is useful to identify the overall distribution of the data values (i.e. left skew, right skew or normal distribution)\n\np &lt;- ggplot(data = ict_derived, aes(x = RADIO)) +\n  geom_histogram(bins = 20, color = \"black\", fill = \"lightblue\") +\n  labs(\n    title = \"Distribution of Radio Ownership\",\n    x = \"Number of Households with Radio\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal()\n\nggplotly(p)\n\n\n\n\n\nBoxplot is useful to detect if there are outliers.\n\nplot_ly(data = ict_derived, \n        x = ~RADIO,\n        type = 'box', \n        fillcolor = 'lightblue', \n        marker = list(color = 'lightblue'),\n        notched = TRUE\n       ) %&gt;%\n  layout(\n    title = \"Distribution of Radio Ownership\",\n    xaxis = list(title = \"Number of Households with Radio\")\n  )\n\n\n\n\n\nWe will also plotting the distribution of the newly derived variables (i.e. Radio penetration rate).\n\n\nShow the code\nmean_pr &lt;- round(mean(ict_derived$`RADIO_PR`, na.rm = TRUE), 1)\n\n# Define common axis properties\naxis_settings &lt;- list(\n  title = \"\",\n  zeroline = FALSE,\n  showline = FALSE,\n  showgrid = FALSE\n)\n\naax_b &lt;- list(\n  title = \"\",\n  zeroline = FALSE,\n  showline = FALSE,\n  showticklabels = FALSE,\n  showgrid = FALSE\n)\n\n# Plot Histogram\nhistogram_plot &lt;- plot_ly(\n  ict_derived,\n  x = ~`RADIO_PR`,\n  type = 'histogram',\n  histnorm = \"count\",\n  marker = list(color = 'lightblue'), \n  hovertemplate = 'Radio Penetration Rate: %{x}&lt;br&gt;Frequency: %{y}&lt;extra&gt;&lt;/extra&gt;'\n) %&gt;%\n  add_lines(\n    x = mean_pr, y = c(0, 15),\n    line = list(width = 3, color = \"black\"),  \n    showlegend = FALSE\n  ) %&gt;%\n  add_annotations(\n    text = paste(\"Mean: \", mean_pr),\n    x = mean_pr, y = 15,\n    showarrow = FALSE,\n    font = list(size = 12)\n  ) %&gt;%\n  layout(\n    xaxis = list(title = \"Radio Penetration Rate\"),\n    yaxis = axis_settings,\n    bargap = 0.1\n  )\n\n# Plot Boxplot\nboxplot_plot &lt;- plot_ly(\n  ict_derived,\n  x = ~`RADIO_PR`,\n  type = \"box\",\n  boxpoints = \"all\",\n  jitter = 0.3,\n  pointpos = -1.8,\n  notched = TRUE,\n  fillcolor = 'lightblue', \n  marker = list(color = 'lightblue'),\n  showlegend = FALSE\n) %&gt;%\n  layout(\n    xaxis = axis_settings,\n    yaxis = aax_b\n  )\n\n# Combine Histogram and Boxplot\nsubplot(\n  boxplot_plot, histogram_plot,\n  nrows = 2, heights = c(0.2, 0.8), shareX = TRUE\n) %&gt;%\n  layout(\n    showlegend = FALSE,\n    title = \"Distribution of Radio Penetration Rate\",\n    xaxis = list(range = c(0, 500))\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhen we compare the distribution of radio ownership vs radio penetration rate, the distribution of radio penetration rate is less skewed.\n\n\nWe can also plot multiple histograms to get a sense of the feature engineered fields.\n\n\nShow the code\nradio &lt;- ggplot(data=ict_derived, \n             aes(x= `RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\ntv &lt;- ggplot(data=ict_derived, \n             aes(x= `TV_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\nllphone &lt;- ggplot(data=ict_derived, \n             aes(x= `LLPHONE_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\nmphone &lt;- ggplot(data=ict_derived, \n             aes(x= `MPHONE_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\ncomputer &lt;- ggplot(data=ict_derived, \n             aes(x= `COMPUTER_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\ninternet &lt;- ggplot(data=ict_derived, \n             aes(x= `INTERNET_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\nggarrange(radio, tv, llphone, mphone, computer, internet, ncol = 3, nrow = 2)\n\n\n\n\n\n\n\n\n\n\n\n8.2 EDA using Choropleth Map\n\n8.2.1 Joining Geospatial Data with Aspatial Data\nBefore we can prepare the choropleth map, we need to combine both the geospatial data object (i.e. shan_sf) and aspatial data.frame object (i.e. ict_derived) into one. This will be performed by using the left_join function of dplyr package. The shan_sf simple feature data.frame will be used as the base data object and the ict_derived data.frame will be used as the join table.\nTo join the data objects, we use the unique identifier, TS_PCODE.\n\nfile_path &lt;- \"data/rds/shan_sf.rds\"\n\nif (!file.exists(file_path)) {\n  shan_sf &lt;- left_join(shan_sf, ict_derived, by = c(\"TS_PCODE\" = \"TS_PCODE\"))\n  write_rds(shan_sf, file_path)\n  \n} else {\n  shan_sf &lt;- read_rds(file_path)\n}\n\nkable(head(shan_sf))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nST\nST_PCODE\nDT.x\nDT_PCODE.x\nTS.x\nTS_PCODE\nDT_PCODE.y\nDT.y\nTS.y\nTT_HOUSEHOLDS.x\nRADIO.x\nTV.x\nLLPHONE.x\nMPHONE.x\nCOMPUTER.x\nINTERNET.x\nRADIO_PR.x\nTV_PR.x\nLLPHONE_PR.x\nMPHONE_PR.x\nCOMPUTER_PR.x\nINTERNET_PR.x\nDT_PCODE.x.x\nDT.x.x\nTS.x.x\nTT_HOUSEHOLDS.y\nRADIO.y\nTV.y\nLLPHONE.y\nMPHONE.y\nCOMPUTER.y\nINTERNET.y\nRADIO_PR.y\nTV_PR.y\nLLPHONE_PR.y\nMPHONE_PR.y\nCOMPUTER_PR.y\nINTERNET_PR.y\nDT_PCODE.y.y\nDT.y.y\nTS.y.y\nTT_HOUSEHOLDS\nRADIO\nTV\nLLPHONE\nMPHONE\nCOMPUTER\nINTERNET\nRADIO_PR\nTV_PR\nLLPHONE_PR\nMPHONE_PR\nCOMPUTER_PR\nINTERNET_PR\ngeometry\n\n\n\n\nShan (North)\nMMR015\nMongmit\nMMR015D008\nMongmit\nMMR015017\nMMR015D003\nKyaukme\nMongmit\n13652\n3907\n7565\n482\n3559\n166\n321\n286.1852\n554.1313\n35.30618\n260.6944\n12.15939\n23.513038\nMMR015D003\nKyaukme\nMongmit\n13652\n3907\n7565\n482\n3559\n166\n321\n286.1852\n554.1313\n35.30618\n260.6944\n12.15939\n23.513038\nMMR015D003\nKyaukme\nMongmit\n13652\n3907\n7565\n482\n3559\n166\n321\n286.1852\n554.1313\n35.30618\n260.6944\n12.15939\n23.513038\nMULTIPOLYGON (((96.96001 23…\n\n\nShan (South)\nMMR014\nTaunggyi\nMMR014D001\nPindaya\nMMR014006\nMMR014D001\nTaunggyi\nPindaya\n17544\n7324\n8862\n348\n2849\n226\n136\n417.4647\n505.1300\n19.83584\n162.3917\n12.88190\n7.751938\nMMR014D001\nTaunggyi\nPindaya\n17544\n7324\n8862\n348\n2849\n226\n136\n417.4647\n505.1300\n19.83584\n162.3917\n12.88190\n7.751938\nMMR014D001\nTaunggyi\nPindaya\n17544\n7324\n8862\n348\n2849\n226\n136\n417.4647\n505.1300\n19.83584\n162.3917\n12.88190\n7.751938\nMULTIPOLYGON (((96.7731 21….\n\n\nShan (South)\nMMR014\nTaunggyi\nMMR014D001\nYwangan\nMMR014007\nMMR014D001\nTaunggyi\nYwangan\n18348\n8890\n4781\n219\n2207\n81\n152\n484.5215\n260.5734\n11.93591\n120.2856\n4.41465\n8.284282\nMMR014D001\nTaunggyi\nYwangan\n18348\n8890\n4781\n219\n2207\n81\n152\n484.5215\n260.5734\n11.93591\n120.2856\n4.41465\n8.284282\nMMR014D001\nTaunggyi\nYwangan\n18348\n8890\n4781\n219\n2207\n81\n152\n484.5215\n260.5734\n11.93591\n120.2856\n4.41465\n8.284282\nMULTIPOLYGON (((96.78483 21…\n\n\nShan (South)\nMMR014\nTaunggyi\nMMR014D001\nPinlaung\nMMR014009\nMMR014D001\nTaunggyi\nPinlaung\n25504\n5908\n13816\n728\n6363\n351\n737\n231.6499\n541.7189\n28.54454\n249.4903\n13.76255\n28.897428\nMMR014D001\nTaunggyi\nPinlaung\n25504\n5908\n13816\n728\n6363\n351\n737\n231.6499\n541.7189\n28.54454\n249.4903\n13.76255\n28.897428\nMMR014D001\nTaunggyi\nPinlaung\n25504\n5908\n13816\n728\n6363\n351\n737\n231.6499\n541.7189\n28.54454\n249.4903\n13.76255\n28.897428\nMULTIPOLYGON (((96.49518 20…\n\n\nShan (North)\nMMR015\nMongmit\nMMR015D008\nMabein\nMMR015018\nMMR015D003\nKyaukme\nMabein\n8632\n3880\n6117\n628\n3389\n142\n165\n449.4903\n708.6423\n72.75255\n392.6089\n16.45042\n19.114921\nMMR015D003\nKyaukme\nMabein\n8632\n3880\n6117\n628\n3389\n142\n165\n449.4903\n708.6423\n72.75255\n392.6089\n16.45042\n19.114921\nMMR015D003\nKyaukme\nMabein\n8632\n3880\n6117\n628\n3389\n142\n165\n449.4903\n708.6423\n72.75255\n392.6089\n16.45042\n19.114921\nMULTIPOLYGON (((96.66306 24…\n\n\nShan (South)\nMMR014\nTaunggyi\nMMR014D001\nKalaw\nMMR014005\nMMR014D001\nTaunggyi\nKalaw\n41341\n11607\n25285\n1739\n16900\n1225\n1741\n280.7624\n611.6204\n42.06478\n408.7951\n29.63160\n42.113156\nMMR014D001\nTaunggyi\nKalaw\n41341\n11607\n25285\n1739\n16900\n1225\n1741\n280.7624\n611.6204\n42.06478\n408.7951\n29.63160\n42.113156\nMMR014D001\nTaunggyi\nKalaw\n41341\n11607\n25285\n1739\n16900\n1225\n1741\n280.7624\n611.6204\n42.06478\n408.7951\n29.63160\n42.113156\nMULTIPOLYGON (((96.49518 20…\n\n\n\n\n\n\n\n8.2.2 Preparing a Choropleth Map\nTo have a quick look at the distribution of Radio penetration rate of Shan State at township level, a choropleth map will be prepared.\nThe code below is used to prepare the choropleth map by using the qtm() function of tmap package.\n\nqtm(shan_sf, \"RADIO_PR\")\n\n\n\n\n\n\n\n\nIn order to reveal the distribution shown in the choropleth map above are bias to the underlying total number of households at the townships, we will create two choropleth maps, one for the total number of households (i.e. TT_HOUSEHOLDS.map) and one for the total number of household with Radio (RADIO.map) by using the code below.\n\nTT_HOUSEHOLDS.map &lt;- tm_shape(shan_sf) + \n  tm_fill(col = \"TT_HOUSEHOLDS\",\n          n = 5,\n          style = \"jenks\", \n          title = \"Total households\") + \n  tm_borders(alpha = 0.5) \n\nRADIO.map &lt;- tm_shape(shan_sf) + \n  tm_fill(col = \"RADIO\",\n          n = 5,\n          style = \"jenks\",\n          title = \"Number Radio \") + \n  tm_borders(alpha = 0.5) \n\ntmap_arrange(TT_HOUSEHOLDS.map, RADIO.map,\n             asp=NA, ncol=2)\n\n\n\n\n\n\n\n\nNotice that the choropleth maps above clearly show that townships with relatively larger number ot households are also showing relatively higher number of radio ownership.\nNow let us plot the choropleth maps showing the distribution of total number of households and Radio penetration rate by using the code below.\n\ntm_shape(shan_sf) +\n    tm_polygons(c(\"TT_HOUSEHOLDS\", \"RADIO_PR\"),\n                style=\"jenks\") +\n    tm_facets(sync = TRUE, ncol = 2) +\n  tm_legend(legend.position = c(\"right\", \"bottom\"))+\n  tm_layout(outer.margins=0, asp=0)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAt first glance, for the Distribution of total number of households and radios plot, it reveals that township with relative larger number of households also exhibit relatively higher number of radio ownership.\nThe penetration rate plot provides more nuanced insight into radio ownership, highlighting regions where radios are more common among households, regardless of the total number of households in the region. This may suggest radio ownership rate is no sole dependent on the total population but also on socio-economic factors, or policies etc."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06a.html#correlation-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06a.html#correlation-analysis",
    "title": "6A: Geographical Segmentation with Spatially Constrained Clustering Techniques",
    "section": "9 Correlation Analysis",
    "text": "9 Correlation Analysis\nBefore we perform cluster analysis, it is important for us to ensure that the cluster variables are not highly correlated.\nIn this section, we will use corrplot.mixed() function of corrplot package to visualise and analyse the correlation of the input variables.\n\ncluster_vars.cor = cor(ict_derived[,12:17])\ncorrplot.mixed(cluster_vars.cor,\n         lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\nThe correlation plot above shows that COMPUTER_PR and INTERNET_PR are highly correlated. This suggest that only one of them should be used in the cluster analysis instead of both."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06a.html#hierarchy-cluster-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06a.html#hierarchy-cluster-analysis",
    "title": "6A: Geographical Segmentation with Spatially Constrained Clustering Techniques",
    "section": "10 Hierarchy Cluster Analysis",
    "text": "10 Hierarchy Cluster Analysis\nIn this section, we will perform hierarchical cluster analysis.\n\n10.1 Extracting Clustering Variables\nThe code below will be used to extract the clustering variables from the shan_sf simple feature object into data.frame.\n\ncluster_vars &lt;- shan_sf %&gt;%\n  st_set_geometry(NULL) %&gt;%\n  # we dont select INTERNET_PR\n  select(\"TS.x\", \"RADIO_PR\", \"TV_PR\", \"LLPHONE_PR\", \"MPHONE_PR\", \"COMPUTER_PR\")\nhead(cluster_vars,10)\n\n        TS.x RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\n1    Mongmit 286.1852 554.1313   35.30618  260.6944    12.15939\n2    Pindaya 417.4647 505.1300   19.83584  162.3917    12.88190\n3    Ywangan 484.5215 260.5734   11.93591  120.2856     4.41465\n4   Pinlaung 231.6499 541.7189   28.54454  249.4903    13.76255\n5     Mabein 449.4903 708.6423   72.75255  392.6089    16.45042\n6      Kalaw 280.7624 611.6204   42.06478  408.7951    29.63160\n7      Pekon 318.6118 535.8494   39.83270  214.8476    18.97032\n8   Lawksawk 387.1017 630.0035   31.51366  320.5686    21.76677\n9  Nawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\n10   Kyaukme 210.9548 601.1773   39.58267  372.4930    30.94709\n\n\nNotice that the final clustering variables list does not include variable INTERNET_PR because it is highly correlated with variable COMPUTER_PR.\nNext, we need to change the rows by township name instead of row number by using the code below.\n\nrow.names(cluster_vars) &lt;- cluster_vars$\"TS.x\"\nhead(cluster_vars,10)\n\n               TS.x RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\nMongmit     Mongmit 286.1852 554.1313   35.30618  260.6944    12.15939\nPindaya     Pindaya 417.4647 505.1300   19.83584  162.3917    12.88190\nYwangan     Ywangan 484.5215 260.5734   11.93591  120.2856     4.41465\nPinlaung   Pinlaung 231.6499 541.7189   28.54454  249.4903    13.76255\nMabein       Mabein 449.4903 708.6423   72.75255  392.6089    16.45042\nKalaw         Kalaw 280.7624 611.6204   42.06478  408.7951    29.63160\nPekon         Pekon 318.6118 535.8494   39.83270  214.8476    18.97032\nLawksawk   Lawksawk 387.1017 630.0035   31.51366  320.5686    21.76677\nNawnghkio Nawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\nKyaukme     Kyaukme 210.9548 601.1773   39.58267  372.4930    30.94709\n\n\nNotice that the row number has been replaced into the township name.\nThen, we will delete the TS.x field.\n\nshan_ict &lt;- select(cluster_vars, c(2:6))\nhead(shan_ict, 10)\n\n          RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\nMongmit   286.1852 554.1313   35.30618  260.6944    12.15939\nPindaya   417.4647 505.1300   19.83584  162.3917    12.88190\nYwangan   484.5215 260.5734   11.93591  120.2856     4.41465\nPinlaung  231.6499 541.7189   28.54454  249.4903    13.76255\nMabein    449.4903 708.6423   72.75255  392.6089    16.45042\nKalaw     280.7624 611.6204   42.06478  408.7951    29.63160\nPekon     318.6118 535.8494   39.83270  214.8476    18.97032\nLawksawk  387.1017 630.0035   31.51366  320.5686    21.76677\nNawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\nKyaukme   210.9548 601.1773   39.58267  372.4930    30.94709\n\n\n\n\n10.2 Data Standardisation\nIn general, multiple variables will be used in cluster analysis. It is not unusual their values range are different. In order to avoid cluster analysis result to be biased due to the clustering variables with large values, it is useful to standardise the input variables before performing cluster analysis.\n\n\n10.3 Min-Max standardisation\nIn the code below, we use:\n\nnormalize() of heatmaply package to standardise the clustering variables by using Min-Max method.\nsummary() is then used to display the summary statistics of the standardised clustering variables.\n\n\nshan_ict.std &lt;- normalize(shan_ict)\nsummary(shan_ict.std)\n\n    RADIO_PR          TV_PR          LLPHONE_PR       MPHONE_PR     \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.2544   1st Qu.:0.4600   1st Qu.:0.1123   1st Qu.:0.2199  \n Median :0.4097   Median :0.5523   Median :0.1948   Median :0.3846  \n Mean   :0.4199   Mean   :0.5416   Mean   :0.2703   Mean   :0.3972  \n 3rd Qu.:0.5330   3rd Qu.:0.6750   3rd Qu.:0.3746   3rd Qu.:0.5608  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n  COMPUTER_PR     \n Min.   :0.00000  \n 1st Qu.:0.09598  \n Median :0.17607  \n Mean   :0.23692  \n 3rd Qu.:0.29868  \n Max.   :1.00000  \n\n\nThe value range of the Min-max standardised clustering variables are now 0 - 1.\n\n\n10.4 Z-score standardisation\nZ-score standardisation can be performed easily by using scale() of Base R. The code below will be used to stadardisation the clustering variables by using Z-score method.\n\nshan_ict.z &lt;- scale(shan_ict)\ndescribe(shan_ict.z)\n\n            vars  n mean sd median trimmed  mad   min  max range  skew kurtosis\nRADIO_PR       1 55    0  1  -0.04   -0.06 0.94 -1.85 2.55  4.40  0.48    -0.27\nTV_PR          2 55    0  1   0.05    0.04 0.78 -2.47 2.09  4.56 -0.38    -0.23\nLLPHONE_PR     3 55    0  1  -0.33   -0.15 0.68 -1.19 3.20  4.39  1.37     1.49\nMPHONE_PR      4 55    0  1  -0.05   -0.06 1.01 -1.58 2.40  3.98  0.48    -0.34\nCOMPUTER_PR    5 55    0  1  -0.26   -0.18 0.64 -1.03 3.31  4.34  1.80     2.96\n              se\nRADIO_PR    0.13\nTV_PR       0.13\nLLPHONE_PR  0.13\nMPHONE_PR   0.13\nCOMPUTER_PR 0.13\n\n\nThe mean and standard deviation of the Z-score standardised clustering variables are 0 and 1 respectively.\n\n\n\n\n\n\nTip\n\n\n\nNote: describe() of psych package is used here instead of summary() of Base R because the earlier provides standard deviation.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWarning: Z-score standardisation method should only be used if we would assume all variables come from some normal distribution.\n\n\n\n\n10.5 Visualising the Standardised Clustering Variables\n\n\n\n\n\n\nTip\n\n\n\nIt is a good practice to visualize the standardised clustering variables.\n\n\nTo plot the scaled Radio_PR field:\n\n\nShow the code\nr &lt;- ggplot(data=ict_derived, \n             aes(x= `RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  ggtitle(\"Raw values without standardisation\")\n\nshan_ict_s_df &lt;- as.data.frame(shan_ict.std)\ns &lt;- ggplot(data=shan_ict_s_df, \n       aes(x=`RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  ggtitle(\"Min-Max Standardisation\")\n\nshan_ict_z_df &lt;- as.data.frame(shan_ict.z)\nz &lt;- ggplot(data=shan_ict_z_df, \n       aes(x=`RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  ggtitle(\"Z-score Standardisation\")\n\nggarrange(r, s, z,\n          ncol = 3,\n          nrow = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhat statistical conclusion can you draw from the histograms above?\nThe overall distribution of the clustering variables changes after the data standardisation.\n\n\n\n\nShow the code\nr &lt;- ggplot(data=ict_derived, \n             aes(x= `RADIO_PR`)) +\n  geom_density(color=\"black\",\n               fill=\"light blue\") +\n  ggtitle(\"Raw values without standardisation\")\n\nshan_ict_s_df &lt;- as.data.frame(shan_ict.std)\ns &lt;- ggplot(data=shan_ict_s_df, \n       aes(x=`RADIO_PR`)) +\n  geom_density(color=\"black\",\n               fill=\"light blue\") +\n  ggtitle(\"Min-Max Standardisation\")\n\nshan_ict_z_df &lt;- as.data.frame(shan_ict.z)\nz &lt;- ggplot(data=shan_ict_z_df, \n       aes(x=`RADIO_PR`)) +\n  geom_density(color=\"black\",\n               fill=\"light blue\") +\n  ggtitle(\"Z-score Standardisation\")\n\nggarrange(r, s, z,\n          ncol = 3,\n          nrow = 1)\n\n\n\n\n\n\n\n\n\n\n\n10.6 Calculating the Proximity Matrix\nR offers several packages to compute distance matrices, and we will use the dist() function from the base package for this purpose.\nThe dist() function supports 6 types of distance calculations: - Euclidean, - Maximum, - Manhattan, - Canberra, - Binary, and - Minkowsk\nBy default, it uses the Euclidean distance.\nBelow is the code to compute the proximity matrix using the Euclidean method:\n\nproxmat &lt;- dist(shan_ict, method = 'euclidean')\n\nTo view the contents of the computed proximity matrix (proxmat), you can use the following code:\n\nproxmat\n\n\n\n10.7 Performing Hierarchical Clustering\nIn R, several packages offer hierarchical clustering functions. We will use the hclust() function from the stats package.\n\nMethod: hclust() uses an agglomerative approach to compute clusters.\n8 Supported Clustering Algorithms:\n\nward.D\nward.D2\nsingle\ncomplete\naverage (UPGMA)\nmcquitty (WPGMA)\nmedian (WPGMC)\ncentroid (UPGMC)\n\n\nThe code chunk below performs hierarchical cluster analysis using ward.D method. The hierarchical clustering output is stored in an object of class hclust which describes the tree produced by the clustering process.\n\nhclust_ward &lt;- hclust(proxmat, method = 'ward.D')\n\nWe can then plot the tree by using plot() of R Graphics.\n\nplot(hclust_ward, cex = 0.6)\n\n\n\n\n\n\n\n\n\n\n10.8 Choosing the Optimal Clustering Algorithm\nOne of the challenges in hierarchical clustering is identifying the method that provides the strongest clustering structure. This can be addressed by using the agnes() function from the cluster package. Unlike hclust(), the agnes() function also calculates the agglomerative coefficient—a measure of clustering strength (values closer to 1 indicate a stronger clustering structure).\nThe code chunk below will be used to compute the agglomerative coefficients of all hierarchical clustering algorithms.\n\nm &lt;- c( \"average\", \"single\", \"complete\", \"ward\")\nnames(m) &lt;- c( \"average\", \"single\", \"complete\", \"ward\")\n\nac &lt;- function(x) {\n  agnes(shan_ict, method = x)$ac\n}\n\nmap_dbl(m, ac)\n\n  average    single  complete      ward \n0.8131144 0.6628705 0.8950702 0.9427730 \n\n\n\n\n\n\n\n\nNote\n\n\n\nThe output shows that Ward’s method has the highest agglomerative coefficient, indicating the strongest clustering structure among the methods evaluated. Therefore, Ward’s method will be used for the subsequent analysis.\n\n\n\n\n10.9 Determining Optimal Clusters\nAnother technical challenge faced by data analysts in performing clustering analysis is to determine the optimal clusters to retain.\nThere are 3 commonly used methods to determine the optimal clusters, they are:\n\nElbow Method\nAverage Silhouette Method\nGap Statistic Method\n\n\n\n10.10 Gap Statistic Method\nThe gap statistic helps determine the optimal number of clusters by comparing the total within-cluster variation for different values of k against their expected values under a random reference distribution. The optimal number of clusters is identified by the value of k that maximizes the gap statistic, indicating that the clustering structure is far from random.\nTo compute the gap statistic, use the clusGap() function from the cluster package:\n\nset.seed(12345)\ngap_stat &lt;- clusGap(shan_ict, \n                    FUN = hcut, \n                    nstart = 25, \n                    K.max = 10, \n                    B = 50)\n# use firstmax to get the location of the first local maximum.\nprint(gap_stat, method = \"firstmax\")\n\nClustering Gap statistic [\"clusGap\"] from call:\nclusGap(x = shan_ict, FUNcluster = hcut, K.max = 10, B = 50, nstart = 25)\nB=50 simulated reference sets, k = 1..10; spaceH0=\"scaledPCA\"\n --&gt; Number of clusters (method 'firstmax'): 1\n          logW   E.logW       gap     SE.sim\n [1,] 8.407129 8.680794 0.2736651 0.04460994\n [2,] 8.130029 8.350712 0.2206824 0.03880130\n [3,] 7.992265 8.202550 0.2102844 0.03362652\n [4,] 7.862224 8.080655 0.2184311 0.03784781\n [5,] 7.756461 7.978022 0.2215615 0.03897071\n [6,] 7.665594 7.887777 0.2221833 0.03973087\n [7,] 7.590919 7.806333 0.2154145 0.04054939\n [8,] 7.526680 7.731619 0.2049390 0.04198644\n [9,] 7.458024 7.660795 0.2027705 0.04421874\n[10,] 7.377412 7.593858 0.2164465 0.04540947\n\n\nThe hcut function is from the factoextra package.\nTo visualize the gap statistic, use the fviz_gap_stat() function from the factoextra package:\n\nfviz_gap_stat(gap_stat)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFrom the gap statistic plot, the suggested number of clusters is 1. However, retaining only one cluster is not meaningful in most contexts. Examining the plot further, the 6-cluster solution has the largest gap statistic after 1, making it the next best choice.\n\n\n\n\n\n\n\n\nTip\n\n\n\nAdditional Note: The NbClust package offers 30 indices for determining the appropriate number of clusters. It helps users select the best clustering scheme by considering different combinations of cluster numbers, distance measures, and clustering methods (Charrad et al., 2014).\n\n\n\n\n10.11 Interpreting the Dendrograms\nA dendrogram visually represents the clustering of observations in hierarchical clustering:\n\nLeaves (Bottom of the Dendrogram): Each leaf represents a single observation.\nBranches and Fusions (Moving Up the Tree): Similar observations are grouped into branches. As we move up the tree, these branches merge at different heights.\nHeight of Fusion (Vertical Axis): Indicates the level of (dis)similarity between merged observations or clusters. A greater height means the observations or clusters are less similar.\n\n\nThe similarity between two observations can only be assessed by the vertical height where their branches first merge. The horizontal distance between observations does not indicate their similarity.\n\nTo highlight specific clusters, use the rect.hclust() function to draw borders around clusters. The border argument specifies the colors for the rectangles.\nThe following code plots the dendrogram and draws borders around the 6 clusters:\n\nplot(hclust_ward, cex = 0.6)\nrect.hclust(hclust_ward, \n            k = 6, \n            border = 2:5)\n\n\n\n\n\n\n\n\nThis visualization helps to easily identify and interpret the selected clusters in the dendrogram.\n\n\n10.12 Visually-driven Hierarchical Clustering Analysis\nWith heatmaply, we are able to build both highly interactive cluster heatmap or static cluster heatmap.\n\n10.12.1 Transforming the Data Frame into a Matrix\nThe data was loaded into a data frame, but it has to be a data matrix to make the heatmap.\nThe code below will be used to transform shan_ict data frame into a data matrix.\n\nshan_ict_mat &lt;- data.matrix(shan_ict)\n\nclass(shan_ict)\n\n[1] \"data.frame\"\n\nclass(shan_ict_mat)\n\n[1] \"matrix\" \"array\" \n\n\n\n\n10.12.2 Plotting Interactive Cluster Heatmap using heatmaply()\nIn the code below, the heatmaply() of heatmaply package is used to build an interactive cluster heatmap.\n\nheatmaply(normalize(shan_ict_mat),\n          Colv=NA,\n          dist_method = \"euclidean\",\n          hclust_method = \"ward.D\",\n          seriate = \"OLO\",\n          colors = Blues,\n          k_row = 6,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 4,\n          fontsize_col = 5,\n          main=\"Geographic Segmentation of Shan State by ICT indicators\",\n          xlab = \"ICT Indicators\",\n          ylab = \"Townships of Shan State\"\n          )\n\n\n\n\n\n\n\n\n10.13 Mapping the Clusters Formed\nUpon close examination of the dendrogram above, we have decided to retain 6 clusters.\n\nuse cutree() of R Base to derive a 6-cluster model\n\n\ngroups &lt;- as.factor(cutree(hclust_ward, k=6))\nclass(groups)\n\n[1] \"factor\"\n\nlength(groups)\n\n[1] 55\n\n\nThe output is called groups. It is a factor object.\nIn order to visualise the clusters, the groups object need to be appended onto shan_sf simple feature object.\nThe code below form the join in three steps:\n\nthe groups list object will be converted into a matrix;\ncbind() is used to append groups matrix onto shan_sf to produce an output simple feature object called shan_sf_cluster; and\nrename of dplyr package is used to rename as.matrix.groups field as CLUSTER.\n\n\nshan_sf_cluster &lt;- cbind(shan_sf, as.matrix(groups)) %&gt;%\n  rename(`CLUSTER`=`as.matrix.groups.`)\n\nNext, qtm() of tmap package is used to plot the choropleth map showing the cluster formed.\n\nqtm(shan_sf_cluster, \"CLUSTER\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe choropleth map above reveals the clusters are very fragmented. The is one of the major limitation when non-spatial clustering algorithm such as hierarchical cluster analysis method is used."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06a.html#spatially-constrained-clustering-skater-approach",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06a.html#spatially-constrained-clustering-skater-approach",
    "title": "6A: Geographical Segmentation with Spatially Constrained Clustering Techniques",
    "section": "11 Spatially Constrained Clustering: SKATER approach",
    "text": "11 Spatially Constrained Clustering: SKATER approach\nIn this section, we will derive spatially constrained cluster by using skater() method of spdep package.\n\n11.1 Converting into SpatialPolygonsDataFrame\nFirst, we need to convert shan_sf into SpatialPolygonsDataFrame. This is because SKATER function only support sp objects such as SpatialPolygonDataFrame.\nThe code below uses as_Spatial() of sf package to convert shan_sf into a SpatialPolygonDataFrame called shan_sp.\n\nshan_sp &lt;- as_Spatial(shan_sf)\n\n\n\n11.2 Computing Neighbour List\nTo compute the list of neighboring polygons, use the poly2nb() function from the spdep package.\n\nshan.nb &lt;- poly2nb(shan_sp)\nsummary(shan.nb)\n\nNeighbour list object:\nNumber of regions: 55 \nNumber of nonzero links: 264 \nPercentage nonzero weights: 8.727273 \nAverage number of links: 4.8 \nLink number distribution:\n\n 2  3  4  5  6  7  8  9 \n 5  9  7 21  4  3  5  1 \n5 least connected regions:\n3 5 7 9 47 with 2 links\n1 most connected region:\n8 with 9 links\n\n\nTo visualize the neighbors on the map, you can plot the community area boundaries and the neighbor network together. Here’s how:\n\nPlot the Boundaries: Use the plot() function to draw the boundaries of the spatial polygons (shan_sf).\nCompute Centroids: Extract the centroids of the polygons using the st_centroid() function, which will serve as the nodes for the neighbor network.\n\n\ncoords &lt;- st_coordinates(\n  st_centroid(st_geometry(shan_sf)))\n\n\nplot(st_geometry(shan_sf),\n     border=grey(.5))\nplot(shan.nb,\n     coords, \n     col=\"blue\", \n     add=TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nPlot Order Matters: Always plot the boundaries first, followed by the network. If the network is plotted first, some areas may be clipped because the plotting area is determined by the first plot’s extent. Plotting the boundaries first ensures the entire map area is visible.\n\n\n\n\n11.3 Calculating Edge Costs\nNext, nbcosts() of spdep package is used to compute the cost of each edge. It is the distance between it nodes. This function compute this distance using a data.frame with observations vector in each node.\nThe code below is used to compute the cost of each edge.\n\nlcosts &lt;- nbcosts(shan.nb, shan_ict)\n\nFor each observation, this gives the pairwise dissimilarity between its values on the five variables and the values for the neighbouring observation (from the neighbour list). Basically, this is the notion of a generalised weight for a spatial weights matrix.\nNext, we will incorporate these costs into a weights object in the same way as we did in the calculation of inverse of distance weights. In other words, we convert the neighbour list to a list weights object by specifying the just computed lcosts as the weights.\nIn order to achieve this, nb2listw() of spdep package is used as shown in the code chunk below.\nNote that we specify the style as B to make sure the cost values are not row-standardised.\n\nshan.w &lt;- nb2listw(shan.nb, \n                   lcosts, \n                   style=\"B\")\nsummary(shan.w)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 55 \nNumber of nonzero links: 264 \nPercentage nonzero weights: 8.727273 \nAverage number of links: 4.8 \nLink number distribution:\n\n 2  3  4  5  6  7  8  9 \n 5  9  7 21  4  3  5  1 \n5 least connected regions:\n3 5 7 9 47 with 2 links\n1 most connected region:\n8 with 9 links\n\nWeights style: B \nWeights constants summary:\n   n   nn       S0       S1        S2\nB 55 3025 76267.65 58260785 522016004\n\n\n\n\n11.4 Computing Minimum Spanning Tree\nThe minimum spanning tree is computed by mean of the mstree() of spdep package as shown in the code chunk below.\n\nshan.mst &lt;- mstree(shan.w)\n\nAfter computing the MST, we can check its class and dimension by using the code below.\n\nclass(shan.mst)\n\n[1] \"mst\"    \"matrix\"\n\ndim(shan.mst)\n\n[1] 54  3\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe dimension is 54 and not 55.\nThis is because the minimum spanning tree consists on n-1 edges (links) in order to traverse all the nodes.\n\n\nWe can display the content of shan.mst by using head() as shown in the code below.\n\nhead(shan.mst)\n\n     [,1] [,2]      [,3]\n[1,]   54   48  47.79331\n[2,]   54   17 109.08506\n[3,]   54   45 127.42203\n[4,]   45   52 146.78891\n[5,]   52   13 110.55197\n[6,]   13   28  92.79567\n\n\nThe plot method for the MST include a way to show the observation numbers of the nodes in addition to the edge. As before, we plot this together with the township boundaries. We can see how the initial neighbour list is simplified to just one edge connecting each of the nodes, while passing through all the nodes.\n\nplot(st_geometry(shan_sf), \n                 border=gray(.5))\nplot.mst(shan.mst, \n         coords, \n         col=\"blue\", \n         cex.lab=0.7, \n         cex.circles=0.005, \n         add=TRUE)\n\n\n\n\n\n\n\n\n\n\n11.5 Computing Spatially Constrained Clusters using SKATER Method\nThe code below compute the spatially constrained cluster using skater() of spdep package.\n\nclust6 &lt;- spdep::skater(edges = shan.mst[,1:2], \n                 data = shan_ict, \n                 method = \"euclidean\", \n                 ncuts = 5)\n\nThe skater() function requires three mandatory arguments:\n\nFirst two columns of the MST matrix: These columns represent the edges of the Minimum Spanning Tree (MST) without the cost.\nData matrix: This is used to update the costs dynamically as units are grouped together.\nNumber of cuts: This is set to one less than the desired number of clusters. Remember, this value represents the number of cuts in the graph, not the total number of clusters.\n\nThe output of skater() is an object of class skater. You can examine its contents using the following code:\n\nstr(clust6)\n\nList of 8\n $ groups      : num [1:55] 3 3 6 3 3 3 3 3 3 3 ...\n $ edges.groups:List of 6\n  ..$ :List of 3\n  .. ..$ node: num [1:22] 13 48 54 55 45 37 34 16 25 52 ...\n  .. ..$ edge: num [1:21, 1:3] 48 55 54 37 34 16 45 25 13 13 ...\n  .. ..$ ssw : num 3423\n  ..$ :List of 3\n  .. ..$ node: num [1:18] 47 27 53 38 42 15 41 51 43 32 ...\n  .. ..$ edge: num [1:17, 1:3] 53 15 42 38 41 51 15 27 15 43 ...\n  .. ..$ ssw : num 3759\n  ..$ :List of 3\n  .. ..$ node: num [1:11] 2 6 8 1 36 4 10 9 46 5 ...\n  .. ..$ edge: num [1:10, 1:3] 6 1 8 36 4 6 8 10 10 9 ...\n  .. ..$ ssw : num 1458\n  ..$ :List of 3\n  .. ..$ node: num [1:2] 44 20\n  .. ..$ edge: num [1, 1:3] 44 20 95\n  .. ..$ ssw : num 95\n  ..$ :List of 3\n  .. ..$ node: num 23\n  .. ..$ edge: num[0 , 1:3] \n  .. ..$ ssw : num 0\n  ..$ :List of 3\n  .. ..$ node: num 3\n  .. ..$ edge: num[0 , 1:3] \n  .. ..$ ssw : num 0\n $ not.prune   : NULL\n $ candidates  : int [1:6] 1 2 3 4 5 6\n $ ssto        : num 12613\n $ ssw         : num [1:6] 12613 10977 9962 9540 9123 ...\n $ crit        : num [1:2] 1 Inf\n $ vec.crit    : num [1:55] 1 1 1 1 1 1 1 1 1 1 ...\n - attr(*, \"class\")= chr \"skater\"\n\n\nThe most interesting component of this list structure is the groups vector containing the labels of the cluster to which each observation belongs (as before, the label itself is arbitary). This is followed by a detailed summary for each of the clusters in the edges.groups list. Sum of squares measures are given as ssto for the total and ssw to show the effect of each of the cuts on the overall criterion.\nWe can check the cluster assignment by using the code:\n\nccs6 &lt;- clust6$groups\nccs6\n\n [1] 3 3 6 3 3 3 3 3 3 3 2 1 1 1 2 1 1 1 2 4 1 2 5 1 1 1 2 1 2 2 1 2 2 1 1 3 1 2\n[39] 2 2 2 2 2 4 1 3 2 1 1 1 2 1 2 1 1\n\n\nWe can find out how many observations are in each cluster by means of the table command.\nParenthetially, we can also find this as the dimension of each vector in the lists contained in edges.groups. For example, the first list has node with dimension 12, which is also the number of observations in the first cluster.\n\ntable(ccs6)\n\nccs6\n 1  2  3  4  5  6 \n22 18 11  2  1  1 \n\n\nLastly, we can also plot the pruned tree that shows the five clusters on top of the townshop area.\n\nplot(st_geometry(shan_sf), \n     border=gray(.5))\nplot(clust6, \n     coords, \n     cex.lab=.7,\n     groups.colors=c(\"red\",\"green\",\"blue\", \"brown\", \"pink\"),\n     cex.circles=0.005, \n     add=TRUE)\n\n\n\n\n\n\n\n\n\n\n11.6 Visualising the Clusters in Choropleth Map\nThe code below is used to plot the newly derived clusters by using SKATER method.\n\ngroups_mat &lt;- as.matrix(clust6$groups)\nshan_sf_spatialcluster &lt;- cbind(shan_sf_cluster, as.factor(groups_mat)) %&gt;%\n  rename(`SP_CLUSTER`=`as.factor.groups_mat.`)\nqtm(shan_sf_spatialcluster, \"SP_CLUSTER\")\n\n\n\n\n\n\n\n\nFor easy comparison, it will be better to place both the hierarchical clustering and spatially constrained hierarchical clustering maps next to each other.\n\nhclust.map &lt;- qtm(shan_sf_cluster,\n                  \"CLUSTER\") + \n  tm_borders(alpha = 0.5) \n\nshclust.map &lt;- qtm(shan_sf_spatialcluster,\n                   \"SP_CLUSTER\") + \n  tm_borders(alpha = 0.5) \n\ntmap_arrange(hclust.map, shclust.map,\n             asp=NA, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06a.html#spatially-constrained-clustering-clustgeo-method",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06a.html#spatially-constrained-clustering-clustgeo-method",
    "title": "6A: Geographical Segmentation with Spatially Constrained Clustering Techniques",
    "section": "12 Spatially Constrained Clustering: ClustGeo Method",
    "text": "12 Spatially Constrained Clustering: ClustGeo Method\nIn this section, we will perform both non-spatially constrained and spatially constrained hierarchical cluster analysis with the ClustGeo package.\n\n12.1 Overview of the ClustGeo Package\nThe ClustGeo package in R is designed specifically for spatially constrained cluster analysis. It offers a Ward-like hierarchical clustering algorithm called hclustgeo(), which incorporates spatial or geographical constraints:\n\nDissimilarity Matrices (D0 and D1):\n\nD0: Represents dissimilarities in the attribute/clustering variable space. It can be non-Euclidean, and the weights of the observations can be non-uniform.\nD1: Represents dissimilarities in the constraint space (e.g., spatial/geographical constraints).\n\nMixing Parameter (alpha):\n\nA value between [0, 1] that balances the importance of the attribute dissimilarity (D0) and the spatial constraint dissimilarity (D1). The objective is to find an alpha value that enhances spatial continuity without significantly compromising the quality of clustering based on the attributes of interest. This can be achieved using the choicealpha() function.\n\n\n\n\n12.2 Ward-like Hierarchical Clustering with ClustGeo\nThe hclustgeo() function from the ClustGeo package performs a Ward-like hierarchical clustering, similar to the hclust() function.\nTo perform non-spatially constrained hierarchical clustering, provide the function with a dissimilarity matrix, as demonstrated in the code example below:\n\nnongeo_cluster &lt;- hclustgeo(proxmat)\nplot(nongeo_cluster, cex = 0.5)\nrect.hclust(nongeo_cluster, \n            k = 6, \n            border = 2:5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe dissimilarity matrix must be an object of class dist, i.e. an object obtained with the function dist().\n\n\n\n12.2.1 Mapping Clusters\nWe can plot the clusters on a categorical area shaded map.\n\ngroups &lt;- as.factor(cutree(nongeo_cluster, k=6))\n\n\nshan_sf_ngeo_cluster &lt;- cbind(shan_sf, as.matrix(groups)) %&gt;%\n  rename(`CLUSTER` = `as.matrix.groups.`)\n\n\nqtm(shan_sf_ngeo_cluster, \"CLUSTER\")\n\n\n\n\n\n\n\n\n\n\n\n12.3 Spatially Constrained Hierarchical Clustering\nBefore we can performed spatially constrained hierarchical clustering, a spatial distance matrix will be derived by using st_distance() of sf package.\n\ndist &lt;- st_distance(shan_sf, shan_sf)\ndistmat &lt;- as.dist(dist)\nclass(distmat)\n\n[1] \"dist\"\n\n\nas.dist() is needed to convert the data frame into matrix.\nNext, choicealpha() will be used to determine a suitable value for the mixing parameter alpha.\n\ncr &lt;- choicealpha(proxmat, distmat, range.alpha = seq(0, 1, 0.1), K=6, graph = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWith reference to the graphs above, alpha = 0.3 will be used since it is the intersection point between D0 and D1.\n\nclustG &lt;- hclustgeo(proxmat, distmat, alpha = 0.3)\n\nNext, cutree() is used to derive the cluster objecct.\n\ngroups &lt;- as.factor(cutree(clustG, k=6))\n\nWe will then join back the group list with shan_sf polygon feature data frame by using the code below.\n\nshan_sf_Gcluster &lt;- cbind(shan_sf, as.matrix(groups)) %&gt;%\n  rename(`CLUSTER` = `as.matrix.groups.`)\n\nWe can now plot the map of the newly delineated spatially constrained clusters.\n\nqtm(shan_sf_Gcluster, \"CLUSTER\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06a.html#visual-interpretation-of-clusters",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06a.html#visual-interpretation-of-clusters",
    "title": "6A: Geographical Segmentation with Spatially Constrained Clustering Techniques",
    "section": "13 Visual Interpretation of Clusters",
    "text": "13 Visual Interpretation of Clusters\n\n13.1 Visualising individual clustering variable\nTo reveal the distribution of a clustering variable (i.e RADIO_PR) by cluster.\n\nggplot(data = shan_sf_ngeo_cluster,\n       aes(x = CLUSTER, y = RADIO_PR)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe boxplot reveals Cluster 3 displays the highest mean Radio Ownership Per Thousand Household. This is followed by Cluster 2, 1, 4, 6 and 5.\n\n\n\n\n13.2 Multivariate Visualisation\nParallel coordinate plot can be used to reveal clustering variables by cluster very effectively.\nggparcoord() of **GGally* package is used in the code below.\n\nggparcoord(data = shan_sf_ngeo_cluster, \n           columns = c(17:21), \n           scale = \"globalminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of ICT Variables by Cluster\") +\n  facet_grid(~ CLUSTER) + \n  theme(axis.text.x = element_text(angle = 30))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nObservation\n\nThe parallel coordinate plot above reveals that households in Cluster 4 townships tend to own the highest number of TV and mobile-phone.\nOn the other hand, households in Cluster 5 tends to own the lowest of all the five ICT.\n\nNote\nThe scale argument of ggparcoor() provide several methods to scale the clustering variables. They are:\n\nstd: univariately, subtract mean and divide by standard deviation.\nrobust: univariately, subtract median and divide by median absolute deviation.\nuniminmax: univariately, scale so the minimum of the variable is zero, and the maximum is one.\nglobalminmax: no scaling is done; the range of the graphs is defined by the global minimum and the global maximum.\ncenter: use uniminmax to standardize vertical height, then center each variable at a value specified by the scaleSummary param.\ncenterObs: use uniminmax to standardize vertical height, then center each variable at the value of the observation specified by the centerObsID param\n\nThe suitability of scale argument is dependent on your use case.\n\n\nTo complement the visual interpretation, use group_by() and summarise() of dplyr are used to derive mean values of the clustering variables:\n\nshan_sf_ngeo_cluster %&gt;% \n  st_set_geometry(NULL) %&gt;%\n  group_by(CLUSTER) %&gt;%\n  summarise(mean_RADIO_PR = mean(RADIO_PR),\n            mean_TV_PR = mean(TV_PR),\n            mean_LLPHONE_PR = mean(LLPHONE_PR),\n            mean_MPHONE_PR = mean(MPHONE_PR),\n            mean_COMPUTER_PR = mean(COMPUTER_PR))\n\n# A tibble: 6 × 6\n  CLUSTER mean_RADIO_PR mean_TV_PR mean_LLPHONE_PR mean_MPHONE_PR\n  &lt;chr&gt;           &lt;dbl&gt;      &lt;dbl&gt;           &lt;dbl&gt;          &lt;dbl&gt;\n1 1               221.        521.            44.2           246.\n2 2               237.        402.            23.9           134.\n3 3               300.        611.            52.2           392.\n4 4               196.        744.            99.0           651.\n5 5               124.        224.            38.0           132.\n6 6                98.6       499.            74.5           468.\n# ℹ 1 more variable: mean_COMPUTER_PR &lt;dbl&gt;"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01a.html",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01a.html",
    "title": "1A: Geospatial Data Wrangling with R",
    "section": "",
    "text": "R for Geospatial Data Science and Analytics - 1  Geospatial Data Science with R"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01a.html#exercise-1a-reference",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01a.html#exercise-1a-reference",
    "title": "1A: Geospatial Data Wrangling with R",
    "section": "",
    "text": "R for Geospatial Data Science and Analytics - 1  Geospatial Data Science with R"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01a.html#learning-outcome",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01a.html#learning-outcome",
    "title": "1A: Geospatial Data Wrangling with R",
    "section": "2 Learning Outcome",
    "text": "2 Learning Outcome\n\ninstalling and loading sf and tidyverse packages into R environment,\nimporting geospatial data by using appropriate functions of sf package,\nimporting aspatial data by using appropriate function of readr package,\nexploring the content of simple feature data frame by using appropriate Base R and sf functions,\nassigning or transforming coordinate systems by using using appropriate sf functions,\nconverting an aspatial data into a sf data frame by using appropriate function of sf package,\nperforming geoprocessing tasks by using appropriate functions of sf package,\nperforming data wrangling tasks by using appropriate functions of dplyr package and\nperforming Exploratory Data Analysis (EDA) by using appropriate functions from ggplot2 package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01a.html#the-data",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01a.html#the-data",
    "title": "1A: Geospatial Data Wrangling with R",
    "section": "3 The Data",
    "text": "3 The Data\n\n\n\nDataset\nSource\nDescription\n\n\n\n\nMaster Plan 2014 Subzone Boundary (Web)\ndata.gov.sg\nGeospatial boundaries for Singapore’s planning subzones.\n\n\nPre-Schools Location\ndata.gov.sg\nLocation data for pre-schools in Singapore.\n\n\nCycling Path\nLTA DataMall\nGeospatial data for cycling paths in Singapore.\n\n\nSingapore Airbnb Listings\nInside Airbnb\nLatest listing data for Airbnb properties in Singapore."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01a.html#installing-and-loading-the-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01a.html#installing-and-loading-the-r-packages",
    "title": "1A: Geospatial Data Wrangling with R",
    "section": "4 Installing and Loading the R Packages",
    "text": "4 Installing and Loading the R Packages\nTwo main R packages will be used in this exercise:\n\n\n\n\n\n\n\n\nPackage\nPurpose\nUse Case in Exercise\n\n\n\n\nsf\nImporting, managing, and processing geospatial data.\nHandling and processing geospatial data in R.\n\n\ntidyverse\nComprehensive set of tools for data science tasks.\nImporting, wrangling, and visualizing data.\n\n\n\n\n4.1 Tidyverse Sub-packages\nThe tidyverse package includes the following sub-packages used in this exercise:\n\n\n\nSub-package\nPurpose\n\n\n\n\nreadr\nImporting CSV data.\n\n\nreadxl\nImporting Excel worksheets.\n\n\ntidyr\nManipulating and tidying data.\n\n\ndplyr\nTransforming and wrangling data.\n\n\nggplot2\nVisualizing data.\n\n\n\nTo install and load these packages, use the following code:\n\npacman::p_load(sf, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01a.html#import-geospatial-data",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01a.html#import-geospatial-data",
    "title": "1A: Geospatial Data Wrangling with R",
    "section": "5 Import Geospatial Data",
    "text": "5 Import Geospatial Data\nThe code block below uses st_read() function of sf package to import the MP14_SUBZONE_WEB_PL shapefile into R as a polygon feature data frame.\n\ndsn refers to data source name\nlayer points to the file name.\nmore details: st_read function - RDocumentation\n\n\n5.1 Import polygon feature data in shapefile format\n\nmpsz = st_read(dsn = \"data/geospatial\",\n                  layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/walter/code/isss626/isss626-gaa/Hands-on_Ex/Hands-on_Ex01/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\n\n\n\n\n\nNote\n\n\n\nOutput Explanation: The geospatial objects are multipolygon features. There are a total of 323 multipolygon features and 15 fields in mpsz simple feature data frame. mpsz is in svy21 projected coordinates systems. The bounding box provides the x extend and y extend of the data.\n\n\n\n\n5.2 Import polyline feature data in shapefile format\nThe code block below uses st_read() function of sf package to import CyclingPath shapefile into R as line feature data frame.\n\ncyclingpath = st_read(dsn = \"data/geospatial\",\n                         layer = \"CyclingPathGazette\")\n\nReading layer `CyclingPathGazette' from data source \n  `/Users/walter/code/isss626/isss626-gaa/Hands-on_Ex/Hands-on_Ex01/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 3138 features and 2 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 11854.32 ymin: 28347.98 xmax: 42644.17 ymax: 48948.15\nProjected CRS: SVY21\n\n\n\n\n\n\n\n\nNote\n\n\n\nOutput Explanation: There are a total of 3138 features and 2 fields in cyclingpath linestring feature data frame and it is in svy21 projected coordinates system too.\n\n\n\n\n5.3 Import GIS data in kml format\nAs compared to st_read for shapefiles, we need to pass the file extension when importing kml files.\n\npreschool = st_read(\"data/geospatial/PreSchoolsLocation.kml\")\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `/Users/walter/code/isss626/isss626-gaa/Hands-on_Ex/Hands-on_Ex01/data/geospatial/PreSchoolsLocation.kml' \n  using driver `KML'\nSimple feature collection with 2290 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\n\n\nNote\n\n\n\nOutput Explanation: The preschool is a point feature data frame. There are a total of 2290 features and 2 fields. Different from the previous two simple feature data frame, preschool is in wgs84 coordinates system."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01a.html#checking-the-content-of-a-simple-feature-data-frame",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01a.html#checking-the-content-of-a-simple-feature-data-frame",
    "title": "1A: Geospatial Data Wrangling with R",
    "section": "6 Checking the Content of A Simple Feature Data Frame",
    "text": "6 Checking the Content of A Simple Feature Data Frame\nThere are different ways to retrieve information related to the content of a simple feature data frame.\n\n6.1 Working with st_geometry()\nThe column in the sf data.frame that contains the geometries is a list, of class sfc.\nThe code block below shows the general way to use st_geometry(). Alternative is mpsz$geom or mpsz[[1]] to retrieve the geometry list-column.\n\nst_geometry(mpsz)\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 5 geometries:\n\n\n\n\n\n\n\n\nNote\n\n\n\nOutput Explanation: The print only displays basic information of the feature class such as type of geometry, the geographic extent of the features and the coordinate system of the data.\n\n\n\n\n6.2 Working with glimpse()\nTo learn more about the associated attribute information in the data frame, we can use glimpse from dplyr.\n\nglimpse(mpsz)\n\nRows: 323\nColumns: 16\n$ OBJECTID   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ SUBZONE_NO &lt;int&gt; 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, 2, 2, …\n$ SUBZONE_N  &lt;chr&gt; \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERSON HIL…\n$ SUBZONE_C  &lt;chr&gt; \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BMSZ07\",…\n$ CA_IND     &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\",…\n$ PLN_AREA_N &lt;chr&gt; \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MERAH\",…\n$ PLN_AREA_C &lt;chr&gt; \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\", \"QT\",…\n$ REGION_N   &lt;chr&gt; \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENT…\n$ REGION_C   &lt;chr&gt; \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",…\n$ INC_CRC    &lt;chr&gt; \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13E0E5\",…\n$ FMEL_UPD_D &lt;date&gt; 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05…\n$ X_ADDR     &lt;dbl&gt; 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 25358.82,…\n$ Y_ADDR     &lt;dbl&gt; 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 29991.38,…\n$ SHAPE_Leng &lt;dbl&gt; 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 4428.913,…\n$ SHAPE_Area &lt;dbl&gt; 1630379.27, 559816.25, 160807.50, 595428.89, 387429.44, 103…\n$ geometry   &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((31495.56 30..., MULTIPOLYGON (…\n\n\n\n\n\n\n\n\nNote\n\n\n\nOutput Explanation: glimpse() report reveals the data type of each fields. For example FMEL-UPD_D field is in date data type and X_ADDR, Y_ADDR, SHAPE_L and SHAPE_AREA fields are all in double-precision values.\n\n\n\n\n6.3 Working with head()\nTo reveal complete information of a feature object, we can use head() from R.\n\nhead(mpsz, n=5)\n\nSimple feature collection with 5 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 25867.68 ymin: 28369.47 xmax: 32362.39 ymax: 30435.54\nProjected CRS: SVY21\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1        1          1   MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2        2          1   PEARL'S HILL    OTSZ01      Y          OUTRAM\n3        3          3      BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4        4          8 HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5        5          3        REDHILL    BMSZ03      N     BUKIT MERAH\n  PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1         MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2         OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3         SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4         BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5         BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n    Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1 29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2 29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3 29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4 29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5 30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n\n\n\n\n\n\n\n\nNote\n\n\n\nOutput Explanation: It shows the top 5 rows from mpsz."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01a.html#plotting-the-geospatial-data",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01a.html#plotting-the-geospatial-data",
    "title": "1A: Geospatial Data Wrangling with R",
    "section": "7 Plotting the Geospatial Data",
    "text": "7 Plotting the Geospatial Data\nThis section covers visualization of geospatial features using plot() of R graphic.\n\nplot(mpsz)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nOutput Explanation: The default plot of an sf object is a multi-plot of all attributes, up to a reasonable maximum as shown above.\n\n\nAlternatively, we can also choose the plot the sf object by using a specific attribute as shown in the code block below.\n\nplot(mpsz[\"PLN_AREA_N\"])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFor high cartographic quality plot, other R package such as tmap should be used."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01a.html#working-with-map-projection",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01a.html#working-with-map-projection",
    "title": "1A: Geospatial Data Wrangling with R",
    "section": "8 Working with Map Projection",
    "text": "8 Working with Map Projection\nMap projection is an important property of a geospatial data. In order to perform geoprocessing using two geospatial data, we need to ensure that both geospatial data are projected using similar coordinate system.\nProjection transformation is a method to project a simple feature data frame from one coordinate system to another coordinate system.\n\n8.1 Assigning EPSG code to a simple feature data frame\nOne of the common issue that can happen during importing geospatial data into R is that the coordinate system of the source data was either missing (such as due to missing .proj for ESRI shapefile) or wrongly assigned during the importing process.\nThis is an example the coordinate system of mpsz simple feature data frame by using st_crs() of sf package as shown in the code block below.\n\nst_crs(mpsz)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\nAlthough mpsz data frame is projected in svy21 but when we read until the end of the print, it indicates that the EPSG is 9001. This is a wrong EPSG code because the correct EPSG code for svy21 should be 3414.\nIn order to assign the correct EPSG code to mpsz data frame, st_set_crs() of sf package is used as shown in the code block below.\n\nmpsz3414 &lt;- st_set_crs(mpsz, 3414)\n\n\nst_crs(mpsz3414)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nOutput Explanation: Note that the EPSG code is 3414 now.\n\n\n8.2 Transforming the projection of preschool from wgs84 to svy21.\nIt is very common in geospatial analytics to transform the original data from geographic coordinate system (gcs) to projected coordinate system (pcs). This is because geographic coordinate system is not appropriate if the analysis need to use distance or/and area measurements.\nLet us take preschool simple feature data frame as an example. The print below reveals that it is in wgs84 coordinate system.\n\n\nGeometry set for 2290 features \nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\nFirst 5 geometries:\n\n\nThis is a scenario that st_set_crs() is not appropriate and st_transform() of sf package should be used. This is because we need to reproject preschool from one coordinate system to another coordinate system mathemetically.\nLet us perform the projection transformation by using the code block below.\n\npreschool3414 &lt;- st_transform(preschool,\n                              crs = 3414)\n\n\nNote: In practice, we need find out the appropriate project coordinate system to use before performing the projection transformation.\n\nNext, let us display the content of preschool3414 sf data frame as shown below.\n\n\nGeometry set for 2290 features \nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 11810.03 ymin: 25596.33 xmax: 45404.24 ymax: 49300.88\nz_range:       zmin: 0 zmax: 0\nProjected CRS: SVY21 / Singapore TM\nFirst 5 geometries:\n\n\n\n\n\n\n\n\nNote\n\n\n\nOutput Explanation: Note that it is in svy21 projected coordinate system now. Furthermore, for the Bounding box:, the values are greater than 0-360 range of decimal degree commonly used by most of the geographic coordinate systems."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01a.html#importing-and-converting-an-aspatial-data",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01a.html#importing-and-converting-an-aspatial-data",
    "title": "1A: Geospatial Data Wrangling with R",
    "section": "9 Importing and Converting An Aspatial Data",
    "text": "9 Importing and Converting An Aspatial Data\nIn this section, we will learn how to process aspatial data such as listing of Inside Airbnb. It is not a geospatial data but among the data fields, there are two fields that capture the x- and y-coordinates of the data points.\nWe will first import the aspatial data and save it as a tibble dataframe and convert it into a simple feature dataframe.\n\n9.1 Import the aspatial data\nSince listings data set is in csv file format, we will use read_csv() of readr package to import listing.csv as shown the code block below.\n\nlistings &lt;- read_csv(\"data/aspatial/listings.csv\")\n\nOutput Explanation: The output R object is called listings and it is a tibble data frame.\n\nlistings &lt;- read_csv(\"data/aspatial/listings.csv\")\n\nAfter importing the data file into R, it is important for us to examine if the data file has been imported correctly.\nThe code block below shows list() of Base R instead of glimpse() is used to do the job.\n\nlist(listings)\n\n[[1]]\n# A tibble: 3,540 × 75\n       id listing_url            scrape_id last_scraped source name  description\n    &lt;dbl&gt; &lt;chr&gt;                      &lt;dbl&gt; &lt;date&gt;       &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;      \n 1  71609 https://www.airbnb.co…   2.02e13 2024-06-29   previ… Ensu… For 3 room…\n 2  71896 https://www.airbnb.co…   2.02e13 2024-06-29   city … B&B … &lt;NA&gt;       \n 3  71903 https://www.airbnb.co…   2.02e13 2024-06-29   city … Room… Like your …\n 4 275343 https://www.airbnb.co…   2.02e13 2024-06-29   city … 10mi… **IMPORTAN…\n 5 275344 https://www.airbnb.co…   2.02e13 2024-06-29   city … 15 m… Lovely hom…\n 6 289234 https://www.airbnb.co…   2.02e13 2024-06-29   previ… Book… This whole…\n 7 294281 https://www.airbnb.co…   2.02e13 2024-06-29   city … 5 mi… I have 3 b…\n 8 324945 https://www.airbnb.co…   2.02e13 2024-06-29   city … Comf… **IMPORTAN…\n 9 330095 https://www.airbnb.co…   2.02e13 2024-06-29   city … Rela… **IMPORTAN…\n10 344803 https://www.airbnb.co…   2.02e13 2024-06-29   city … Budg… Direct bus…\n# ℹ 3,530 more rows\n# ℹ 68 more variables: neighborhood_overview &lt;chr&gt;, picture_url &lt;chr&gt;,\n#   host_id &lt;dbl&gt;, host_url &lt;chr&gt;, host_name &lt;chr&gt;, host_since &lt;date&gt;,\n#   host_location &lt;chr&gt;, host_about &lt;chr&gt;, host_response_time &lt;chr&gt;,\n#   host_response_rate &lt;chr&gt;, host_acceptance_rate &lt;chr&gt;,\n#   host_is_superhost &lt;lgl&gt;, host_thumbnail_url &lt;chr&gt;, host_picture_url &lt;chr&gt;,\n#   host_neighbourhood &lt;chr&gt;, host_listings_count &lt;dbl&gt;, …\n\n\n\n\n\n\n\n\nNote\n\n\n\nOutput Explanation: The. listing tibble data frame consists of 4252 rows and 16 columns. We will use the latitude and longtitude fields. Note that they are in decimal degree format. As a best guess, we will assume that the data is in wgs84 Geographic Coordinate System.\n\n\nThe code block below converts listing data frame into a simple feature data frame by using st_as_sf() of sf packages\n\nlistings_sf &lt;- st_as_sf(listings,\n                       coords = c(\"longitude\", \"latitude\"),\n                       crs=4326) %&gt;%\n  st_transform(crs = 3414)\n\nThings to learn from the arguments above:\n\ncoords: argument requires you to provide the column name of the x-coordinates first then followed by the column name of the y-coordinates.\ncrs: argument requires you to provide the coordinates system in epsg format. EPSG: 4326 is wgs84 Geographic Coordinate System and EPSG: 3414 is Singapore SVY21 Projected Coordinate System. You can search for other country’s epsg code by referring to epsg.io.\n%&gt;%: is used to nest st_transform() to transform the newly created simple feature data frame into svy21 projected coordinates system.\n\nLet us examine the content of this newly created simple feature data frame.\n\nglimpse(listings_sf)\n\nRows: 3,540\nColumns: 74\n$ id                                           &lt;dbl&gt; 71609, 71896, 71903, 2753…\n$ listing_url                                  &lt;chr&gt; \"https://www.airbnb.com/r…\n$ scrape_id                                    &lt;dbl&gt; 2.024063e+13, 2.024063e+1…\n$ last_scraped                                 &lt;date&gt; 2024-06-29, 2024-06-29, …\n$ source                                       &lt;chr&gt; \"previous scrape\", \"city …\n$ name                                         &lt;chr&gt; \"Ensuite Room (Room 1 & 2…\n$ description                                  &lt;chr&gt; \"For 3 rooms.Book room 1 …\n$ neighborhood_overview                        &lt;chr&gt; NA, NA, \"Quiet and view o…\n$ picture_url                                  &lt;chr&gt; \"https://a0.muscache.com/…\n$ host_id                                      &lt;dbl&gt; 367042, 367042, 367042, 1…\n$ host_url                                     &lt;chr&gt; \"https://www.airbnb.com/u…\n$ host_name                                    &lt;chr&gt; \"Belinda\", \"Belinda\", \"Be…\n$ host_since                                   &lt;date&gt; 2011-01-29, 2011-01-29, …\n$ host_location                                &lt;chr&gt; \"Singapore\", \"Singapore\",…\n$ host_about                                   &lt;chr&gt; \"Hi My name is Belinda -H…\n$ host_response_time                           &lt;chr&gt; \"within an hour\", \"within…\n$ host_response_rate                           &lt;chr&gt; \"100%\", \"100%\", \"100%\", \"…\n$ host_acceptance_rate                         &lt;chr&gt; \"N/A\", \"N/A\", \"N/A\", \"99%…\n$ host_is_superhost                            &lt;lgl&gt; FALSE, FALSE, FALSE, FALS…\n$ host_thumbnail_url                           &lt;chr&gt; \"https://a0.muscache.com/…\n$ host_picture_url                             &lt;chr&gt; \"https://a0.muscache.com/…\n$ host_neighbourhood                           &lt;chr&gt; \"Tampines\", \"Tampines\", \"…\n$ host_listings_count                          &lt;dbl&gt; 6, 6, 6, 49, 49, 6, 7, 49…\n$ host_total_listings_count                    &lt;dbl&gt; 11, 11, 11, 73, 73, 11, 8…\n$ host_verifications                           &lt;chr&gt; \"['email', 'phone']\", \"['…\n$ host_has_profile_pic                         &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, T…\n$ host_identity_verified                       &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, T…\n$ neighbourhood                                &lt;chr&gt; NA, NA, \"Singapore, Singa…\n$ neighbourhood_cleansed                       &lt;chr&gt; \"Tampines\", \"Tampines\", \"…\n$ neighbourhood_group_cleansed                 &lt;chr&gt; \"East Region\", \"East Regi…\n$ property_type                                &lt;chr&gt; \"Private room in villa\", …\n$ room_type                                    &lt;chr&gt; \"Private room\", \"Private …\n$ accommodates                                 &lt;dbl&gt; 3, 1, 2, 1, 1, 4, 2, 1, 1…\n$ bathrooms                                    &lt;dbl&gt; NA, 0.5, 0.5, 2.0, 2.5, N…\n$ bathrooms_text                               &lt;chr&gt; \"1 private bath\", \"Shared…\n$ bedrooms                                     &lt;dbl&gt; 2, 1, 1, 1, 1, 3, 2, 1, 1…\n$ beds                                         &lt;dbl&gt; NA, 1, 2, 1, 1, NA, 1, 1,…\n$ amenities                                    &lt;chr&gt; \"[\\\"Free parking on premi…\n$ price                                        &lt;chr&gt; NA, \"$80.00\", \"$80.00\", \"…\n$ minimum_nights                               &lt;dbl&gt; 92, 92, 92, 180, 180, 92,…\n$ maximum_nights                               &lt;dbl&gt; 365, 365, 365, 999, 999, …\n$ minimum_minimum_nights                       &lt;dbl&gt; 92, 92, 92, 180, 180, 92,…\n$ maximum_minimum_nights                       &lt;dbl&gt; 92, 92, 92, 180, 180, 92,…\n$ minimum_maximum_nights                       &lt;dbl&gt; 1125, 1125, 1125, 1125, 1…\n$ maximum_maximum_nights                       &lt;dbl&gt; 1125, 1125, 1125, 1125, 1…\n$ minimum_nights_avg_ntm                       &lt;dbl&gt; 92, 92, 92, 180, 180, 92,…\n$ maximum_nights_avg_ntm                       &lt;dbl&gt; 1125, 1125, 1125, 1125, 1…\n$ calendar_updated                             &lt;lgl&gt; NA, NA, NA, NA, NA, NA, N…\n$ has_availability                             &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, T…\n$ availability_30                              &lt;dbl&gt; 30, 30, 30, 28, 0, 29, 30…\n$ availability_60                              &lt;dbl&gt; 59, 53, 60, 58, 0, 58, 60…\n$ availability_90                              &lt;dbl&gt; 89, 83, 90, 62, 0, 88, 90…\n$ availability_365                             &lt;dbl&gt; 89, 148, 90, 62, 0, 88, 3…\n$ calendar_last_scraped                        &lt;date&gt; 2024-06-29, 2024-06-29, …\n$ number_of_reviews                            &lt;dbl&gt; 19, 24, 46, 20, 16, 12, 1…\n$ number_of_reviews_ltm                        &lt;dbl&gt; 0, 0, 0, 0, 2, 0, 0, 1, 1…\n$ number_of_reviews_l30d                       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ first_review                                 &lt;date&gt; 2011-12-19, 2011-07-30, …\n$ last_review                                  &lt;date&gt; 2020-01-17, 2019-10-13, …\n$ review_scores_rating                         &lt;dbl&gt; 4.44, 4.16, 4.41, 4.40, 4…\n$ review_scores_accuracy                       &lt;dbl&gt; 4.37, 4.22, 4.39, 4.16, 4…\n$ review_scores_cleanliness                    &lt;dbl&gt; 4.00, 4.09, 4.52, 4.26, 4…\n$ review_scores_checkin                        &lt;dbl&gt; 4.63, 4.43, 4.63, 4.47, 4…\n$ review_scores_communication                  &lt;dbl&gt; 4.78, 4.43, 4.64, 4.42, 4…\n$ review_scores_location                       &lt;dbl&gt; 4.26, 4.17, 4.50, 4.53, 4…\n$ review_scores_value                          &lt;dbl&gt; 4.32, 4.04, 4.36, 4.63, 4…\n$ license                                      &lt;chr&gt; NA, NA, NA, \"S0399\", \"S03…\n$ instant_bookable                             &lt;lgl&gt; FALSE, FALSE, FALSE, TRUE…\n$ calculated_host_listings_count               &lt;dbl&gt; 6, 6, 6, 49, 49, 6, 7, 49…\n$ calculated_host_listings_count_entire_homes  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0…\n$ calculated_host_listings_count_private_rooms &lt;dbl&gt; 6, 6, 6, 49, 49, 6, 6, 49…\n$ calculated_host_listings_count_shared_rooms  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ reviews_per_month                            &lt;dbl&gt; 0.12, 0.15, 0.29, 0.15, 0…\n$ geometry                                     &lt;POINT [m]&gt; POINT (41972.5 3639…\n\n\n\n\n\n\n\n\nNote\n\n\n\nOutput Explanation:: The table above shows the content of listing_sf. Notice that a new column called geometry has been added into the data frame. On the other hand, the longitude and latitude columns have been dropped from the data frame."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01a.html#geoprocessing-with-sf",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01a.html#geoprocessing-with-sf",
    "title": "1A: Geospatial Data Wrangling with R",
    "section": "10 Geoprocessing with sf",
    "text": "10 Geoprocessing with sf\nIn addition to offering tools for managing geospatial data (such as importing, exporting, assigning, and transforming projections), the sf package also includes a wide range of geoprocessing functions for GIS analysis.\nScenario:\nThe authority is planning to upgrade the existing cycling path. To do so, they need to acquire 5 meters of reserved land on both sides of the current cycling path. You are tasked with determining the extent of the land that needs to be acquired and its total area.\nSolution:\nFirstly, the st_buffer() function of the sf package is used to compute the 5-meter buffers around the cycling paths.\n\nbuffer_cycling &lt;- st_buffer(cyclingpath,\n                               dist=5, nQuadSegs = 30)\n\nThis is followed by calculating the area of the buffers as shown in the code block below.\n\nbuffer_cycling$AREA &lt;- st_area(buffer_cycling)\n\nLastly, sum() of Base R will be used to derive the total land involved\n\nsum(buffer_cycling$AREA)\n\n2218855 [m^2]\n\n\n\n10.1 Point-in-polygon count\nScenario:\nA pre-school service group want to find out the numbers of pre-schools in each Planning Subzone.\nSolution:\nThe code block below performs two operations at one go.\n\nidentify pre-schools located inside each Planning Subzone by using st_intersects().\nlength() of Base R is used to calculate numbers of pre-schools that fall inside each planning subzone.\n\n\nmpsz3414$`PreSch Count`&lt;- lengths(st_intersects(mpsz3414, preschool3414))\n\n\nWarning: You should not confuse with st_intersection().\n\nThe summary statistics of the newly derived PreSch Count field by using summary() is shown in the code block below.\n\nsummary(mpsz3414$`PreSch Count`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    4.00    7.09   10.00   72.00 \n\n\nNext top_n() from dplyr package is used with n=1 to list the planning subzone with the highest number of pre-school.\n\ntop_n(mpsz3414, 1, `PreSch Count`)\n\nSimple feature collection with 1 feature and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 39655.33 ymin: 35966 xmax: 42940.57 ymax: 38622.37\nProjected CRS: SVY21 / Singapore TM\n  OBJECTID SUBZONE_NO     SUBZONE_N SUBZONE_C CA_IND PLN_AREA_N PLN_AREA_C\n1      189          2 TAMPINES EAST    TMSZ02      N   TAMPINES         TM\n     REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR   Y_ADDR SHAPE_Leng\n1 EAST REGION       ER 21658EAAF84F4D8D 2014-12-05 41122.55 37392.39   10180.62\n  SHAPE_Area                       geometry PreSch Count\n1    4339824 MULTIPOLYGON (((42196.76 38...           72\n\n\nExercise: Calculate the density of pre-school by planning subzone.\nTo determine the density of pre-schools by planning subzone, the st_area() function from the sf package is used to calculate the area of each planning subzone, and the result is stored in a new Area column.\n\nmpsz3414$Area &lt;- mpsz3414 %&gt;%\n  st_area()\n\nNext, mutate() of dplyr package is used to compute the density by using the code block below.\n\nmpsz3414 &lt;- mpsz3414 %&gt;%\n  mutate(`PreSch Density` = `PreSch Count`/Area * 1000000)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01a.html#exploratory-data-analysis-eda",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01a.html#exploratory-data-analysis-eda",
    "title": "1A: Geospatial Data Wrangling with R",
    "section": "11 Exploratory Data Analysis (EDA)",
    "text": "11 Exploratory Data Analysis (EDA)\nIn this section, we will learn how to use ggplot2 to create functional and yet truthful statistical graphs for EDA purposes.\n\nWe will plot a histogram to reveal the distribution of PreSch Density. Conventionally, hist() of R Graphics will be used as shown in the code block below.\n\n\nhist(mpsz3414$`PreSch Density`)\n\n\n\n\n\n\n\n\nAlthough the syntax is very easy to use however the output is far from meeting publication quality. Furthermore, the function has limited room for further customisation.\nIn the code block below, appropriate ggplot2 functions will be used.\n\nggplot(data=mpsz3414,\n       aes(x= as.numeric(`PreSch Density`)))+\n  geom_histogram(bins=20,\n                 color=\"black\",\n                 fill=\"light blue\") +\n  labs(title = \"Are pre-school even distributed in Singapore?\",\n       subtitle= \"There are many planning sub-zones with a single pre-school, on the other hand, \\nthere are two planning sub-zones with at least 20 pre-schools\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Frequency\")\n\n\n\n\n\n\n\n\nExercise: Using ggplot2 method, plot a scatterplot showing the relationship between Pre-school Density and Pre-school Count.\n\nggplot(data=mpsz3414,\n       aes(y = `PreSch Count`,\n           x= as.numeric(`PreSch Density`)))+\n  geom_point(color=\"black\",\n             fill=\"light blue\") +\n  xlim(0, 40) +\n  ylim(0, 40) +\n  labs(title = \"\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Pre-school count\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nOutput Explanation: The scatterplot shows a positive relationship between pre-school density (per km²) and pre-school count, showing that areas with higher density tend to have a greater number of pre-schools."
  },
  {
    "objectID": "Exploration/index.html",
    "href": "Exploration/index.html",
    "title": "Exploration",
    "section": "",
    "text": "This page shows additional exploration using online resources as follows:"
  },
  {
    "objectID": "Exploration/index.html#introduction-to-spatial-data-science",
    "href": "Exploration/index.html#introduction-to-spatial-data-science",
    "title": "Exploration",
    "section": "Introduction to Spatial Data Science",
    "text": "Introduction to Spatial Data Science\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n01 Spatial Data Handling\n\n\nIn this exercise, we will handle spatial data to create a choropleth map of abandoned vehicles per capita in Chicago by downloading, filtering, transforming data, and using spatial join and aggregation techniques.\n\n\n33 min\n\n\n\nSep 5, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html",
    "title": "Take Home Exercise 1",
    "section": "",
    "text": "Refer to: ISSS626 Geospatial Analytics and Applications - Take-home Exercise 1: Geospatial Analytics for Public Good"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#assignment-task",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#assignment-task",
    "title": "Take Home Exercise 1",
    "section": "",
    "text": "Refer to: ISSS626 Geospatial Analytics and Applications - Take-home Exercise 1: Geospatial Analytics for Public Good"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#overview",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#overview",
    "title": "Take Home Exercise 1",
    "section": "2 Overview",
    "text": "2 Overview\nIn this exercise, we will apply spatial and spatio-temporal point pattern analysis methods to identify factors affecting road traffic accidents in the Bangkok Metropolitan Region (BMR), including:\n\nvisualizing spatio-temporal dynamics,\nconducting spatial analysis using Network Spatial Point Patterns, and\nanalyzing spatio-temporal patterns using Temporal Network Spatial Point Patterns."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#the-analytical-questions",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#the-analytical-questions",
    "title": "Take Home Exercise 1",
    "section": "3 The Analytical Questions",
    "text": "3 The Analytical Questions\nThis study seeks to uncover the factors influencing road traffic accidents in the Bangkok Metropolitan Region (BMR) by utilizing both spatial and spatio-temporal point patterns analysis methods.\n\nOur key questions are:\n\nWhat behavioral, environmental, and temporal factors contribute to these accidents?\nWhat are the spatial and temporal patterns of road traffic accidents in BMR?\nAre traffic accidents in the BMR randomly distributed throughout the region?\nIf the distribution is not random, where are the areas with higher concentrations of accidents?"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#the-data",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#the-data",
    "title": "Take Home Exercise 1",
    "section": "4 The Data",
    "text": "4 The Data\nThe following 3 datasets will be used in this exercise.\n\n\n\nDataset Name\nDescription\nFormat\nSource\n\n\n\n\nThailand Road Accident [2019-2022]\nData on road accidents in Thailand, including details on location, severity, and date of incidents.\nCSV\nKaggle\n\n\nThailand Roads (OpenStreetMap Export)\nGeospatial data showing the complete road network of Thailand, extracted from OpenStreetMap.\nESRI Shapefile\nHumanitarian Data Exchange (HDX)\n\n\nThailand - Subnational Administrative Boundaries\nGeospatial dataset detailing the administrative boundaries of Thailand’s provinces and districts.\nESRI Shapefile\nHumanitarian Data Exchange (HDX)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#installing-and-launching-the-r-packages",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#installing-and-launching-the-r-packages",
    "title": "Take Home Exercise 1",
    "section": "5 Installing and Launching the R Packages",
    "text": "5 Installing and Launching the R Packages\nThe following R packages will be used in this exercise:\n\n\n\n\n\n\n\n\nPackage\nPurpose\nUse Case in Exercise\n\n\n\n\ntidyverse\nA collection of packages for data manipulation, visualization, and data analysis.\nImporting, cleaning, and transforming data for analysis.\n\n\nsf\nImports, manages, and processes vector-based geospatial data.\nHandling and analyzing geospatial data such as road networks and administrative boundaries.\n\n\ntmap\nCreates both static and interactive thematic maps with high cartographic quality.\nVisualizing road traffic accident locations and spatial patterns in Thailand.\n\n\nspNetwork\nProvides tools for network-constrained spatial data analysis, such as point pattern analysis on road networks.\nConducting network spatial point pattern analysis to study traffic accident patterns along road networks.\n\n\nspatstat\nA toolkit for spatial point pattern analysis.\nPerforming advanced spatial analysis, such as identifying hotspots of road traffic accidents.\n\n\nplotly\nCreates interactive and web-ready visualizations.\nBuilding interactive charts and maps to explore the spatio-temporal dynamics of traffic accidents.\n\n\ngtsummary\nGenerates publication-ready summary tables of statistical results.\nSummarizing descriptive statistics and results from the analysis of road traffic accidents.\n\n\nsparr\nProvides tools for spatio-temporal analysis of point patterns, including kernel density estimation.\nPerforming spatio-temporal analysis to assess the spread and dynamics of road traffic accidents over time.\n\n\n\nTo install and load these packages, use the following code:\n\npacman::p_load(tidyverse, sf, tmap, spNetwork, spatstat, plotly, gtsummary, sparr)\n\n\n5.1 Reproducibilty\nFor reproducible results of this exercise, we will use seed value, 1234.\n\nset.seed(1234)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#import-data-and-preparation",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#import-data-and-preparation",
    "title": "Take Home Exercise 1",
    "section": "6 Import Data and Preparation",
    "text": "6 Import Data and Preparation\nIn this section, we will perform sanity checks on the raw data from from the official data sources and identify useful data for our case study area.\n\n6.1 Thai Road Accident Data\nFirstly, we will import the Thai Road Accident dataset from 2019-2022 using read_csv() of readr package.\n\nrdacc_sf &lt;- read_csv(\"data/raw_data/thai_road_accident_2019_2022.csv\")\nrdacc_sf\n\n# A tibble: 81,735 × 18\n   acc_code incident_datetime   report_datetime     province_th province_en     \n      &lt;dbl&gt; &lt;dttm&gt;              &lt;dttm&gt;              &lt;chr&gt;       &lt;chr&gt;           \n 1   571905 2019-01-01 00:00:00 2019-01-02 06:11:00 ลพบุรี        Loburi          \n 2  3790870 2019-01-01 00:03:00 2020-02-20 13:48:00 อุบลราชธานี   Ubon Ratchathani\n 3   599075 2019-01-01 00:05:00 2019-01-01 10:35:00 ประจวบคีรีขันธ์ Prachuap Khiri …\n 4   571924 2019-01-01 00:20:00 2019-01-02 05:12:00 เชียงใหม่     Chiang Mai      \n 5   599523 2019-01-01 00:25:00 2019-01-04 09:42:00 นครสวรรค์    Nakhon Sawan    \n 6   571982 2019-01-01 00:30:00 2019-01-07 12:46:00 แม่ฮ่องสอน    Mae Hong Son    \n 7   612782 2019-01-01 00:30:00 2019-10-25 14:25:00 ชุมพร        Chumphon        \n 8   599235 2019-01-01 00:35:00 2019-01-02 16:23:00 สิงห์บุรี       Sing Buri       \n 9   600643 2019-01-01 00:40:00 2019-01-11 10:01:00 สงขลา       Songkhla        \n10   599105 2019-01-01 00:45:00 2019-01-01 10:11:00 ตราด        Trat            \n# ℹ 81,725 more rows\n# ℹ 13 more variables: agency &lt;chr&gt;, route &lt;chr&gt;, vehicle_type &lt;chr&gt;,\n#   presumed_cause &lt;chr&gt;, accident_type &lt;chr&gt;,\n#   number_of_vehicles_involved &lt;dbl&gt;, number_of_fatalities &lt;dbl&gt;,\n#   number_of_injuries &lt;dbl&gt;, weather_condition &lt;chr&gt;, latitude &lt;dbl&gt;,\n#   longitude &lt;dbl&gt;, road_description &lt;chr&gt;, slope_description &lt;chr&gt;\n\n\nFrom the output above, we can observe that there are 18 columns in this dataset and there are 81,735 accidents recorded in this dataset.\n\n\n\n\n\n\nWe can check data dictionary from Kaggle to understand this dataset.\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nacc_code\nThe accident code or identifier.\n\n\nincident_datetime\nThe date and time of the accident occurrence.\n\n\nreport_datetime\nThe date and time when the accident was reported.\n\n\nprovince_th\nThe name of the province in Thailand, written in Thai.\n\n\nprovince_en\nThe name of the province in Thailand, written in English.\n\n\nagency\nThe government agency responsible for the road and traffic management.\n\n\nroute\nThe route or road segment where the accident occurred.\n\n\nvehicle_type\nThe type of vehicle involved in the accident.\n\n\npresumed_cause\nThe presumed cause or reason for the accident.\n\n\naccident_type\nThe type or nature of the accident.\n\n\nnumber_of_vehicles_involved\nThe number of vehicles involved in the accident.\n\n\nnumber_of_fatalities\nThe number of fatalities resulting from the accident.\n\n\nnumber_of_injuries\nThe number of injuries resulting from the accident.\n\n\nweather_condition\nThe weather condition at the time of the accident.\n\n\nlatitude\nThe latitude coordinate of the accident location.\n\n\nlongitude\nThe longitude coordinate of the accident location.\n\n\nroad_description\nThe description of the road type or configuration where the accident occurred.\n\n\nslope_description\nThe description of the slope condition at the accident location.\n\n\n\n\n\n\nNext, we check for null values.\n\nnull_counts &lt;- sapply(rdacc_sf, function(x) sum(is.na(x)))\nnull_counts\n\n                   acc_code           incident_datetime \n                          0                           0 \n            report_datetime                 province_th \n                          0                           0 \n                province_en                      agency \n                          0                           0 \n                      route                vehicle_type \n                          0                           0 \n             presumed_cause               accident_type \n                          0                           0 \nnumber_of_vehicles_involved        number_of_fatalities \n                          0                           0 \n         number_of_injuries           weather_condition \n                          0                           0 \n                   latitude                   longitude \n                        359                         359 \n           road_description           slope_description \n                          0                           0 \n\ndata.frame(Column = names(null_counts), Null_Count = null_counts)\n\n                                                 Column Null_Count\nacc_code                                       acc_code          0\nincident_datetime                     incident_datetime          0\nreport_datetime                         report_datetime          0\nprovince_th                                 province_th          0\nprovince_en                                 province_en          0\nagency                                           agency          0\nroute                                             route          0\nvehicle_type                               vehicle_type          0\npresumed_cause                           presumed_cause          0\naccident_type                             accident_type          0\nnumber_of_vehicles_involved number_of_vehicles_involved          0\nnumber_of_fatalities               number_of_fatalities          0\nnumber_of_injuries                   number_of_injuries          0\nweather_condition                     weather_condition          0\nlatitude                                       latitude        359\nlongitude                                     longitude        359\nroad_description                       road_description          0\nslope_description                     slope_description          0\n\n\nFrom the output above, we can notice that are 359 missing data in the latitude (lat) and longitude (lon) columns. We will remove these rows as they make up &lt;5% of the total records.\nNext, we check for duplicate values.\n\nduplicate_count &lt;- sum(duplicated(rdacc_sf))\nduplicate_count\n\n[1] 0\n\n\nThere are no exact duplicates in this dataset.\nNext, we observe the provinces available in this dataset.\n\n# Display unique values in the \"ST\" column sorted in ascending order\nsort(unique(rdacc_sf$province_en))\n\n [1] \"Amnat Charoen\"            \"Ang Thong\"               \n [3] \"Bangkok\"                  \"buogkan\"                 \n [5] \"Buri Ram\"                 \"Chachoengsao\"            \n [7] \"Chai Nat\"                 \"Chaiyaphum\"              \n [9] \"Chanthaburi\"              \"Chiang Mai\"              \n[11] \"Chiang Rai\"               \"Chon Buri\"               \n[13] \"Chumphon\"                 \"Kalasin\"                 \n[15] \"Kamphaeng Phet\"           \"Kanchanaburi\"            \n[17] \"Khon Kaen\"                \"Krabi\"                   \n[19] \"Lampang\"                  \"Lamphun\"                 \n[21] \"Loburi\"                   \"Loei\"                    \n[23] \"Mae Hong Son\"             \"Maha Sarakham\"           \n[25] \"Mukdahan\"                 \"Nakhon Nayok\"            \n[27] \"Nakhon Pathom\"            \"Nakhon Phanom\"           \n[29] \"Nakhon Ratchasima\"        \"Nakhon Sawan\"            \n[31] \"Nakhon Si Thammarat\"      \"Nan\"                     \n[33] \"Narathiwat\"               \"Nong Bua Lam Phu\"        \n[35] \"Nong Khai\"                \"Nonthaburi\"              \n[37] \"Pathum Thani\"             \"Pattani\"                 \n[39] \"Phangnga\"                 \"Phatthalung\"             \n[41] \"Phayao\"                   \"Phetchabun\"              \n[43] \"Phetchaburi\"              \"Phichit\"                 \n[45] \"Phitsanulok\"              \"Phra Nakhon Si Ayutthaya\"\n[47] \"Phrae\"                    \"Phuket\"                  \n[49] \"Prachin Buri\"             \"Prachuap Khiri Khan\"     \n[51] \"Ranong\"                   \"Ratchaburi\"              \n[53] \"Rayong\"                   \"Roi Et\"                  \n[55] \"Sa Kaeo\"                  \"Sakon Nakhon\"            \n[57] \"Samut Prakan\"             \"Samut Sakhon\"            \n[59] \"Samut Songkhram\"          \"Saraburi\"                \n[61] \"Satun\"                    \"Si Sa Ket\"               \n[63] \"Sing Buri\"                \"Songkhla\"                \n[65] \"Sukhothai\"                \"Suphan Buri\"             \n[67] \"Surat Thani\"              \"Surin\"                   \n[69] \"Tak\"                      \"Trang\"                   \n[71] \"Trat\"                     \"Ubon Ratchathani\"        \n[73] \"Udon Thani\"               \"unknown\"                 \n[75] \"Uthai Thani\"              \"Uttaradit\"               \n[77] \"Yala\"                     \"Yasothon\"                \n\n\nFrom the output above, we can notice that the dataset contains data from across 78 provinces in Thailand. Since our study area is only in Bangkok Metropolitan Region (BMR), we will filter the data for records in “Bangkok”, “Nonthaburi”, “Nakhon Pathom”, “Pathum Thani”, “Samut Prakan”, “Samut Sakhon” only.\n\n# list province names in BMR\nbmr_regions &lt;- c(\"Bangkok\", \"Nonthaburi\", \"Nakhon Pathom\", \"Pathum Thani\", \"Samut Prakan\", \"Samut Sakhon\")\n\n\n\n\n\n\n\nNote\n\n\n\nWe will perform the data preprocessing steps as follows:\n\nfilter data for 6 provinces in BMR by province_en\nremove null lat, lon rows\nconvert filtered data into a sf spatial object, using the lat, lon columns and setting the coordinate reference system (CRS) to EPSG 4326\nreproject spatial data to EPSG 32647 used in Thailand. This format is in metres.\n\n\n\n\naccidents_bmr &lt;- rdacc_sf %&gt;%\n  filter(province_en %in% bmr_regions) %&gt;%\n  filter (!is.na(longitude) & longitude != \"\",\n          !is.na(latitude ) & latitude != \"\") %&gt;%  \n  st_as_sf(coords = c(\n    \"longitude\", \"latitude\"),\n    crs=4326) %&gt;%\n  st_transform(crs=32647)\n\n\n\n6.2 Thailand Subnational Administration Boundary\nThe Thailand subnational administrative boundaries dataset includes four levels: - country (level 0), - province (level 1), - district (level 2), and - sub-district (level 3).\nFor this analysis, we will focus on the 6 provinces in the Bangkok Metropolitan Region (BMR) using level 2 boundaries as it provides a finer level of detail and allows for a more granular understanding of spatial patterns and accident hotspots within the Bangkok Metropolitan Region (BMR).\nThe data will be loaded with st_read() and transformed to EPSG:32647 (UTM Zone 47N, meters).\n\nadmin_boundary &lt;- st_read(dsn = \"data/raw_data\", \n                # try different layer\n                # 0 country\n                # layer = \"tha_admbnda_adm0_rtsd_20220121\") %&gt;%      \n                # 1 province\n                #layer = \"tha_admbnda_adm1_rtsd_20220121\") %&gt;% \n                # 2 district\n                layer = \"tha_admbnda_adm2_rtsd_20220121\") %&gt;% \n  st_transform(crs = 32647)\n\nReading layer `tha_admbnda_adm2_rtsd_20220121' from data source \n  `/Users/walter/code/isss626/isss626-gaa/Take-home_Ex/Take-home_Ex01/data/raw_data' \n  using driver `ESRI Shapefile'\nSimple feature collection with 928 features and 19 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.34336 ymin: 5.613038 xmax: 105.637 ymax: 20.46507\nGeodetic CRS:  WGS 84\n\n\n\nst_crs(admin_boundary)\n\nCoordinate Reference System:\n  User input: EPSG:32647 \n  wkt:\nPROJCRS[\"WGS 84 / UTM zone 47N\",\n    BASEGEOGCRS[\"WGS 84\",\n        ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n            MEMBER[\"World Geodetic System 1984 (Transit)\"],\n            MEMBER[\"World Geodetic System 1984 (G730)\"],\n            MEMBER[\"World Geodetic System 1984 (G873)\"],\n            MEMBER[\"World Geodetic System 1984 (G1150)\"],\n            MEMBER[\"World Geodetic System 1984 (G1674)\"],\n            MEMBER[\"World Geodetic System 1984 (G1762)\"],\n            MEMBER[\"World Geodetic System 1984 (G2139)\"],\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ENSEMBLEACCURACY[2.0]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"UTM zone 47N\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",99,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"Between 96°E and 102°E, northern hemisphere between equator and 84°N, onshore and offshore. China. Indonesia. Laos. Malaysia - West Malaysia. Mongolia. Myanmar (Burma). Russian Federation. Thailand.\"],\n        BBOX[0,96,84,102]],\n    ID[\"EPSG\",32647]]\n\n\nLet’s observe this dataset for useful attributes to filter for BMR region from the national boundary data.\n\nglimpse(admin_boundary)\n\nRows: 928\nColumns: 20\n$ Shape_Leng &lt;dbl&gt; 0.08541733, 0.13413177, 0.67634217, 0.08588647, 0.30172202,…\n$ Shape_Area &lt;dbl&gt; 0.0004504685, 0.0009501914, 0.0198588627, 0.0003369561, 0.0…\n$ ADM2_EN    &lt;chr&gt; \"Phra Nakhon\", \"Dusit\", \"Nong Chok\", \"Bang Rak\", \"Bang Khen…\n$ ADM2_TH    &lt;chr&gt; \"พระนคร\", \"ดุสิต\", \"หนองจอก\", \"บางรัก\", \"บางเขน\", \"บางกะปิ\", \"ป…\n$ ADM2_PCODE &lt;chr&gt; \"TH1001\", \"TH1002\", \"TH1003\", \"TH1004\", \"TH1005\", \"TH1006\",…\n$ ADM2_REF   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ADM2ALT1EN &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ADM2ALT2EN &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ADM2ALT1TH &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ADM2ALT2TH &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ADM1_EN    &lt;chr&gt; \"Bangkok\", \"Bangkok\", \"Bangkok\", \"Bangkok\", \"Bangkok\", \"Ban…\n$ ADM1_TH    &lt;chr&gt; \"กรุงเทพมหานคร\", \"กรุงเทพมหานคร\", \"กรุงเทพมหานคร\", \"กรุงเทพมหาน…\n$ ADM1_PCODE &lt;chr&gt; \"TH10\", \"TH10\", \"TH10\", \"TH10\", \"TH10\", \"TH10\", \"TH10\", \"TH…\n$ ADM0_EN    &lt;chr&gt; \"Thailand\", \"Thailand\", \"Thailand\", \"Thailand\", \"Thailand\",…\n$ ADM0_TH    &lt;chr&gt; \"ประเทศไทย\", \"ประเทศไทย\", \"ประเทศไทย\", \"ประเทศไทย\", \"ประเทศ…\n$ ADM0_PCODE &lt;chr&gt; \"TH\", \"TH\", \"TH\", \"TH\", \"TH\", \"TH\", \"TH\", \"TH\", \"TH\", \"TH\",…\n$ date       &lt;date&gt; 2019-02-18, 2019-02-18, 2019-02-18, 2019-02-18, 2019-02-18…\n$ validOn    &lt;date&gt; 2022-01-22, 2022-01-22, 2022-01-22, 2022-01-22, 2022-01-22…\n$ validTo    &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ geometry   &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((662263.2 15..., MULTIPOLYGON (…\n\n\nSimilar to the road accident csv file, we can filter for the BMR region using ADM1_EN.\n\nadmin_boundary_bmr &lt;- admin_boundary %&gt;% \n  select(\"ADM1_EN\") %&gt;% \n  filter(ADM1_EN %in% bmr_regions)\n\nNext, we save the filtered data in RDS format for quick loading in future analyses.\n\nwrite_rds(admin_boundary_bmr, file = \"data/rds/admin_boundary_bmr.rds\")\n\n\nadmin_boundary_bmr &lt;-read_rds(\"data/rds/admin_boundary_bmr.rds\")\n\nLet’s visualize the administrative boundaries map using tmap.\n\ntmap_mode('plot')\n\ntm_shape(admin_boundary_bmr) +\n  tm_fill(col = \"ADM1_EN\", title = \"Region\") +\n  tm_borders() +\n  tm_layout(main.title = \"BMR Administrative Boundaries\",\n            main.title.position = \"center\")\n\n\n\n\n\n\n\n\n\n\n6.3 Road Lines\nNext, we will use st_read() import the Thailand Roads dataset\n\nroads &lt;- st_read(dsn = \"data/raw_data\", \n                layer = \"hotosm_tha_roads_lines_shp\")\n\n\n\n\n\n\n\nNote\n\n\n\nWe observed that this dataset has over 2 million features.\nThe geospatial data is in the form of MULTILINESTRING, representing multiple connected line segments. Using st_cast(\"LINESTRING\") simplifies the geometry by converting these into single line segments (LINESTRING), making spatial operations, such as length calculations and spatial joins, easier for analysis.\nWe also assigned the correct CRS with st_set_crs() before transforming the data to the desired EPSG:32647 projection.\n\n\n\nroads_sf &lt;- st_set_crs(roads, 4326) %&gt;% \n  st_transform(crs = 32647) %&gt;% \n  st_cast(\"LINESTRING\")\n\n\nst_crs(roads_sf)\n\nNext, we can observe some data attributes of this dataset such as highway types.\n\n# Display unique values in the \"highway\" column sorted in ascending order\nsort(unique(roads_sf$highway))\n\n\nFor the purpose of this study, we will focus on the 6 Intercity highway classifications: see WikiProject Thailand - OpenStreetMap Wiki.\n\nhighway_types &lt;- c(\"motorway\", # controlled-access\n                  \"trunk\", # uncontrolled-access\n                  \"primary\", # 3-digit national highway\n                  \"secondary\", # 4-digit national highway\n                  \"tertiary\", # all rural roads\n                  \"unclassified\" # lowest rank of usable public roads\n                  ) \n\nroads_sf_filtered &lt;- roads_sf %&gt;%\n  select(\"highway\") %&gt;% \n  filter(highway %in% highway_types)\n\nAfter filtering for Intercity highway types, we use st_intersection() to find the roads within BMR region.\n\nroads_bmr &lt;- st_intersection(roads_sf_filtered,\n                            admin_boundary_bmr)\n\n\n\nShow the code\n# raw roads rows\nraw_size &lt;- dim(roads_sf)[1]\n# roads rows after filter for 6 intercity types\nfiltered_size &lt;- dim(roads_sf_filtered)[1]\n# roads size after filter for bmr region only\nbmr_size &lt;- dim(roads_bmr)[1]   \n\nreduction_filtered &lt;- raw_size - filtered_size\nreduction_bmr &lt;- raw_size - bmr_size \n\nreduction_filtered_percent &lt;- (reduction_filtered / raw_size) * 100\nreduction_bmr_percent &lt;- (reduction_bmr / raw_size) * 100\n\ncat(\"Raw Size:\", raw_size, \"\\n\")\ncat(\"Filtered Size:\", filtered_size, \"\\n\")\ncat(\"BMR Size:\", bmr_size, \"\\n\\n\")\n\ncat(\"Reduction from roads_sf to roads_sf_filtered:\\n\")\ncat(\"Percentage reduction:\", round(reduction_filtered_percent, 2), \"%\\n\\n\")\n\ncat(\"Reduction from roads_sf to roads_bmr:\\n\")\ncat(\"Percentage reduction:\", round(reduction_bmr_percent, 2), \"%\\n\")\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn this study, we focus on the 6 intercity highway types within the Bangkok Metropolitan Region (BMR).\nFirst, we filter the data by highway types and then narrow it down to the BMR region. This method results in a 99.01% reduction in data size, from 2,792,362 road features to 27,760.\nAlternatively, we could have first isolated the BMR region and then analyzed the distribution of highway types before filtering. However, we chose not to use this approach, as the st_intersection() step would be significantly more time-consuming.\n\n\n\nwrite_rds(roads_bmr, \"data/rds/roads_bmr.rds\")\n\n\nroads_bmr &lt;- read_rds(\"data/rds/roads_bmr.rds\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#feature-engineering",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#feature-engineering",
    "title": "Take Home Exercise 1",
    "section": "7 Feature Engineering",
    "text": "7 Feature Engineering\nAfter going through all the 3 datasets, we have a better sense of the dataset that we are working with. In this section, we will transform certain columns to more usable formats and feature engineer new columns to aid our analysis:\n\nbreak down incident_datetime to various factors\nadd season column to capture 3 seasons: Refer to Bangkok - Wikipedia\n\nMarch to May: Hot\nJun to Oct: Rainy\nNov to Feb: Cool\n\nadd peak_period to capture peak hour patterns: Refer to How Do You Beat Bangkok Traffic?\n\nMorning Peak: 7-9 am\nEvening Peak: 4-7 pm\n\nadd songkran_7_dead_days columns to capture 7 deadly days of songkran period: Refer to: 25 deaths in 234 road accidents recorded on 1st of Songkran’s ‘7 dangerous days’,\n\n7 Deadly Days of Songkran: 11-17 Apr each year\n\nRemove unused columns such as province_th and so on.\n\n\n\nShow the code\naccidents_bmr_ft &lt;- accidents_bmr %&gt;%\n  # extract month number (1 = January, 12 = December)\n  mutate(month_number = month(incident_datetime)) %&gt;%\n  \n  # extract month as factor (\"Jan\", \"Feb\")\n  mutate(month_factor = month(incident_datetime, label = TRUE, abbr = TRUE)) %&gt;%\n  \n  # extract day of month\n  mutate(day_of_month = day(incident_datetime)) %&gt;%\n  \n  # extract the day of the week (1 = Mon)\n  mutate(day_of_week = wday(incident_datetime, week_start = 1)) %&gt;%\n  \n  # extract year\n  mutate(year = year(incident_datetime)) %&gt;%\n  \n  # extract time (HH:MM:SS)\n  mutate(time = format(incident_datetime, format = \"%H:%M:%S\")) %&gt;%\n  \n  # add season col\n  mutate(season = case_when(\n    month_number %in% c(3, 4, 5) ~ \"Hot\",          # March to May\n    month_number %in% c(6, 7, 8, 9, 10) ~ \"Rainy\", # June to October\n    month_number %in% c(11, 12, 1, 2) ~ \"Cool\"     # November to February\n  )) %&gt;%\n  \n  # add peak period col (7-9 am or 4-7 pm)\n  mutate(peak_period = case_when(\n    format(incident_datetime, \"%H:%M:%S\") &gt;= \"07:00:00\" & format(incident_datetime, \"%H:%M:%S\") &lt;= \"09:00:00\" ~ \"Morning Peak\",\n    format(incident_datetime, \"%H:%M:%S\") &gt;= \"16:00:00\" & format(incident_datetime, \"%H:%M:%S\") &lt;= \"19:00:00\" ~ \"Evening Peak\",\n    TRUE ~ \"Off-Peak\"\n  )) %&gt;%\n  \n  # Add column to identify Songkran's \"7 Deadly Days\"\n  mutate(songkran_7_dead_days = month_number == 4 & day_of_month &gt;= 11 & day_of_month &lt;= 17) %&gt;%\n  # drop unused columns\n  select(-c(\"province_th\", \n            \"incident_datetime\", \n            \"report_datetime\",\n            \"route\",\n            \"agency\"))  \n\naccidents_bmr_ft\n\n\nThen, we save this processed accident data in RDS data format for future analysis.\n\nwrite_rds(accidents_bmr_ft, \"data/rds/accidents_bmr_ft.rds\")\n\n\naccidents_bmr_ft &lt;- read_rds(\"data/rds/accidents_bmr_ft.rds\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#exploratory-data-analysis-eda",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#exploratory-data-analysis-eda",
    "title": "Take Home Exercise 1",
    "section": "8 Exploratory Data Analysis (EDA)",
    "text": "8 Exploratory Data Analysis (EDA)\nIn this section, we conduct an Exploratory Data Analysis (EDA) to visualize the trends and distribution of road accidents in Thailand from 2019 to 2022. A combination of plots and choropleth maps will be used to reveal key patterns and insights.\n\n8.1 Overall Plot\nFirst, we will plot the 3 datasets onto a single choropleth map to gain a general understanding.\n\n\nShow the code\ntmap_mode('plot')\n\ntm_shape(admin_boundary_bmr) + \n  tm_polygons(col = \"ADM1_EN\", alpha = 0.6, border.col = \"black\", lwd = 0.7, title = \"Region\") +\n  tm_shape(roads_bmr) +\n  tm_lines(col = \"darkgreen\", lwd = 1.5, alpha = 0.8) +                                       \n  tm_shape(accidents_bmr_ft) + \n  tm_dots(col = \"red\", size = 0.05, alpha = 0.5) +                                                \n  tm_layout(\n    main.title = \"Road Traffic Accidents in Bangkok Metropolitan Region (2019-2022)\",\n    main.title.position = c(\"center\", \"top\"), \n    frame = FALSE,\n    legend.outside = TRUE,               \n    legend.position = c(\"right\", \"bottom\"), \n    title.size = 0.8,     \n    legend.text.size = 1, \n    legend.title.size = 1 \n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nObservations:\n\nThe majority of accidents are concentrated along the Intercity road network.\nThis suggests that the 6 existing Intercity highway types provide sufficient coverage, and we don’t require to add more highway types from the original roads dataset for this study.\n\n\n\n\n\n8.2 Visualize Geographic Distribution of Accidents by Year\nNext, we use choropleth maps and stacked bar charts to explore the distribution of road traffic accidents across the BMR from 2019 to 2022.\n\n\nShow the code\ntmap_mode('plot')\n\ntm_shape(admin_boundary_bmr) + \n  tm_polygons(border.col = \"darkgray\", alpha = 0.5) +\ntm_shape(roads_bmr) + \n  tm_lines(col = \"darkgreen\", lwd = 0.7) +\ntm_shape(accidents_bmr_ft) + \n  tm_dots(size = 0.2, col = \"red\", alpha = 0.6) +\n# facet by year\ntm_facets(by = \"year\") +\ntm_layout(\n  main.title = \"Accident Trends by Year in Bangkok Metropolitan Region (2019-2022)\",\n  main.title.size = 1.5, \n  main.title.position = c(\"center\", \"top\"), \n  frame = FALSE\n)\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# Calculate total accidents by year\naccidents_by_year &lt;- accidents_bmr_ft %&gt;%\n  st_drop_geometry() %&gt;%                  \n  group_by(year) %&gt;%\n  summarize(total_accidents = n())\n\n# Calculate total accidents by province and year\naccidents_by_province_year &lt;- accidents_bmr_ft %&gt;%\n  st_drop_geometry() %&gt;%                  \n  group_by(year, province_en) %&gt;%\n  summarize(total_accidents = n()) %&gt;%\n  ungroup()\n\n# Summarize total accidents for each year (for trendline)\ntotal_accidents_by_year &lt;- accidents_by_province_year %&gt;%\n  group_by(year) %&gt;%\n  summarize(total_accidents = sum(total_accidents))\n\n# create stacked bar chart with trendline\nfig_accidents_by_year_province &lt;- plot_ly() %&gt;%\n  # add stack bar chart\n  add_trace(\n    data = accidents_by_province_year,\n    x = ~year,\n    y = ~total_accidents,\n    color = ~province_en,\n    type = 'bar',\n    text = ~paste(province_en, \": \", total_accidents), \n    hoverinfo = 'text', \n    name = ~province_en \n  ) %&gt;%\n  # add trendline\n  add_trace(\n    data = total_accidents_by_year,\n    x = ~year,\n    y = ~total_accidents,\n    type = 'scatter',\n    mode = 'lines+markers',\n    line = list(color = 'black', dash = 'dash'),\n    marker = list(color = 'black', size = 6), \n    name = 'Total Accidents Trend'\n  ) %&gt;%\n  layout(\n    barmode = 'stack',\n    title = \"Total Accidents by Year and Province with Trendline\",\n    xaxis = list(title = \"Year\"),\n    yaxis = list(title = \"Total Accidents\"),\n    legend = list(title = list(text = \"Province\")) \n  )\n\nfig_accidents_by_year_province\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nObservations:\n\nOverall, accident trends indicate a general upward trajectory over the years.\nThe total number of accidents increased from 2019 to 2022, with a slight dip in 2021.\nBangkok consistently recorded the highest number of accidents each year.\nPathum Thani and Samut Prakan also contributed significantly to the accident counts.\nThe number of accidents in 2022 saw a noticeable rise, particularly in Bangkok and Samut Sakhon.\n\n\n\n\n\n8.3 Visualize Geographic Distribution of Accidents by Month\nNext, we visualize the distribution of accidents by months and seasons.\n\n\nShow the code\ntmap_mode('plot')\n\ntm_shape(admin_boundary_bmr) + \n  tm_polygons(border.col = \"darkgray\", alpha = 0.5) +\ntm_shape(roads_bmr) + \n  tm_lines(col = \"darkgreen\", lwd = 0.7) +\ntm_shape(accidents_bmr_ft) + \n  tm_dots(size = 0.2, col = \"red\", alpha = 0.6) +\n# facet by month_factor\ntm_facets(by = \"month_factor\") +\ntm_layout(\n  main.title = \"Accident Trends by Month in Bangkok Metropolitan Region (2019-2022)\",\n  main.title.size = 1.5, \n  main.title.position = c(\"center\", \"top\"), \n  frame = FALSE\n)\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# compute accidents by month, season\naccidents_by_month &lt;- accidents_bmr_ft %&gt;%\n  st_drop_geometry() %&gt;%                  \n  group_by(month_factor, season) %&gt;%\n  summarize(total_accidents = n()) %&gt;%\n  ungroup()\n\n\nfig_accidents_by_month_bar &lt;- plot_ly() %&gt;%\n  # add bar chart\n  add_trace(\n    data = accidents_by_month,\n    x = ~month_factor,                   \n    y = ~total_accidents,                \n    color = ~season,                     \n    # Assign colors to each season\n    colors = c('Cool' = 'lightblue', 'Hot' = 'red3', 'Rainy' = 'royalblue3'), \n    type = 'bar',                        \n    name = ~season                       \n  ) %&gt;%\n  # Add  trendline\n  add_trace(\n    data = accidents_by_month,\n    x = ~month_factor,\n    y = ~total_accidents,\n    type = 'scatter',\n    mode = 'lines+markers',              \n    line = list(color = 'black', dash = 'dash'), \n    marker = list(color = 'black', size = 6), \n    name = 'Trendline'                  \n  ) %&gt;%\n  layout(\n    title = \"Accidents by Month in Bangkok Metropolitan Region (2019-2022)\",\n    xaxis = list(title = \"Month\"),\n    yaxis = list(title = \"Total Accidents\"),\n    barmode = 'group',                 \n    legend = list(title = list(text = \"Season\"))\n  )\nfig_accidents_by_month_bar\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nObservations:\n\nThe cool season (January, November, and December) also shows relatively high accident counts, particularly in December, which concide with the holidays season.\nThe rainy season (June to October) has a relatively steady trend of accidents, with no significant peaks or drops.\nApril stands out with the highest number of accidents, potentially influenced by the Songkran festival.\n\n\n\n\n8.3.1 Special Mention: Songkran\nIn this special section, we highlight the impact of the Songkran festival on road accidents in April, a period that is known for increased travel and, consequently, heightened risk on the roads. For meaningful comparison, we compute the average daily accident rate during the 7 “deadly days” of Songkran versus the rest of April (non-Songkran days) from 2019 to 2022.\n\n\nShow the code\ncompute_average_daily_accident_rate &lt;- function(data) {\n  # filter data for April\n  april_data &lt;- data %&gt;%\n    st_drop_geometry() %&gt;%\n    filter(month_factor == \"Apr\") %&gt;%\n    group_by(year, songkran_7_dead_days) %&gt;%\n    summarize(total_accidents = n(), .groups = 'drop') %&gt;%\n    mutate(\n      days_count = ifelse(songkran_7_dead_days, 7, 30 - 7),\n      # Compute the average daily accident rate\n      average_daily_accidents = total_accidents / days_count,  \n      songkran_label = ifelse(songkran_7_dead_days, \"Songkran\", \"Non-Songkran Days\")\n    ) %&gt;%\n    select(year, songkran_label, average_daily_accidents) \n\n  return(april_data)\n}\n\naverage_daily_accident_rate &lt;- compute_average_daily_accident_rate(accidents_bmr_ft)\n\nfig_accidents_songkran &lt;- plot_ly(\n  data = average_daily_accident_rate,\n  x = ~year,                          \n  y = ~average_daily_accidents,       \n  color = ~songkran_label,         \n  # Color by Songkran label (During Songkran vs. Non-Songkran Days)\n  colors = c('Songkran' = 'tomato1', 'Non-Songkran Days' = 'royalblue3'), \n  type = 'bar',                        \n  name = ~songkran_label              \n) %&gt;%\n  layout(\n    title = \"Average Daily Accident Rate in April During Songkran vs. Non-Songkran Days (2019-2022)\",\n    xaxis = list(title = \"Year\"),\n    yaxis = list(title = \"Average Daily Accidents\"),\n    barmode = 'group',              \n    legend = list(title = list(text = \"Day Type\"))\n  )\n\nfig_accidents_songkran\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nObservations:\n\nSongkran days (in red) consistently show a higher average daily accident rate compared to non-Songkran days (in blue) across all years (2019-2022).\nNon-Songkran days have a relatively steady accident rate across all years, remaining below 10 accidents per day.\nIn 2020, due to the COVID-19 pandemic restrictions, Songkran celebrations were suspended. This likely explains the significantly lower accident rates for both day types in 2020.\n\n\n\n\n\n\n8.4 Visualize Geographic Distribution of Accidents by Day of Week\nNext, we visualize the distribution of accidents by day of week to observe the accident trends.\n\n\nShow the code\ntmap_mode('plot')\n\ntm_shape(admin_boundary_bmr) + \n  tm_polygons(border.col = \"darkgray\", alpha = 0.5) +\ntm_shape(roads_bmr) + \n  tm_lines(col = \"darkgreen\", lwd = 0.7) +\ntm_shape(accidents_bmr_ft) + \n  tm_dots(size = 0.2, col = \"red\", alpha = 0.6) +\n# facet by day of week\ntm_facets(by = \"day_of_week\") +\ntm_layout(\n  main.title = \"Accident Trends by Days of Week in BMR (2019-2022)\",\n  main.title.size = 1.5, \n  main.title.position = c(\"center\", \"top\"), \n  frame = FALSE\n)\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# Compute accidents by day of the week and group by weekday or weekend\naccidents_by_day &lt;- accidents_bmr_ft %&gt;%\n  st_drop_geometry() %&gt;%\n  mutate(\n    day_type = case_when(\n      day_of_week %in% c(6, 7) ~ \"Weekend\",\n      TRUE ~ \"Weekday\"\n    )\n  ) %&gt;%\n  group_by(day_of_week, day_type) %&gt;%\n  summarize(total_accidents = n()) %&gt;%\n  ungroup()\n\nfig_accidents_by_day_bar &lt;- plot_ly() %&gt;%\n  # Add the bar chart\n  add_trace(\n    data = accidents_by_day,\n    x = ~day_of_week,                    \n    y = ~total_accidents,               \n    color = ~day_type,                   \n    # assign color by weekday, weekend\n    colors = c('Weekday' = 'orange', 'Weekend' = 'seagreen'), \n    type = 'bar',                        \n    name = ~day_type                     \n  ) %&gt;%\n  # Add a trendline\n  add_trace(\n    data = accidents_by_day,\n    x = ~day_of_week,\n    y = ~total_accidents,\n    type = 'scatter',\n    mode = 'lines+markers',              \n    line = list(color = 'black', dash = 'dash'), \n    marker = list(color = 'black', size = 6), \n    name = 'Trendline'                  \n  ) %&gt;%\n  layout(\n    title = \"Accidents by Day of the Week in Bangkok Metropolitan Region (2019-2022)\",\n    xaxis = list(title = \"Day of the Week\", tickvals = 1:7, ticktext = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")),\n    yaxis = list(title = \"Total Accidents\"),\n    barmode = 'group',                 \n    legend = list(title = list(text = \"Day Type\"))\n  )\n\nfig_accidents_by_day_bar\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nObservations:\n\nWeekdays (in orange) show a relatively consistent number of total accidents across Monday to Thursday, with a slight increase towards Friday.\nBoth Fridays and Saturdays show high accident numbers suggesting a potential rise in traffic volume or risky driving behavior leading into the weekend.\nThe trendline reflects an upward trend from Monday, peaking on Friday, and then declining over the weekend, particularly on Sunday.\n\n\n\n\n\n8.5 Visualize Geographic Distribution of Accidents by Peak Period\nNext, we visualize the distribution of accidents by peak period.\n\n\nShow the code\ntmap_mode('plot')\n\ntm_shape(admin_boundary_bmr) + \n  tm_polygons(border.col = \"darkgray\", alpha = 0.5) +\ntm_shape(roads_bmr) + \n  tm_lines(col = \"darkgreen\", lwd = 0.7) +\ntm_shape(accidents_bmr_ft) + \n  tm_dots(size = 0.2, col = \"red\", alpha = 0.6) +\n# facet by peak_period\ntm_facets(by = \"peak_period\") +\ntm_layout(\n  main.title = \"Accident Trends by Peak Period in BMR (2019-2022)\",\n  main.title.size = 1.5, \n  main.title.position = c(\"center\", \"top\"), \n  frame = FALSE\n)\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# Group data by year and peak period type\naccidents_by_year_peak_period &lt;- accidents_bmr_ft %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(year, peak_period) %&gt;%\n  summarize(total_accidents = n(), .groups = 'drop') %&gt;%\n  mutate(\n    # Calculate the number of hours for each peak period type\n    hours_count = case_when(\n      peak_period == \"Morning Peak\" ~ 2,  # 7-9 AM (2 hours)\n      peak_period == \"Evening Peak\" ~ 3,  # 4-7 PM (3 hours)\n      peak_period == \"Off-Peak\" ~ 19      # Remaining hours in a day\n    ),\n    average_accidents_per_hour = total_accidents / hours_count  # Compute average accident rate per hour\n  )\n\n\nfig_accidents_trend &lt;- plot_ly(\n  data = accidents_by_year_peak_period,\n  x = ~year,                              \n  y = ~average_accidents_per_hour,        \n  color = ~peak_period,                    \n  colors = c('Morning Peak' = 'tomato1', 'Evening Peak' = 'orange', 'Off-Peak' = 'lightblue'),\n  type = 'bar',                       \n  mode = 'lines+markers',                 \n  name = ~peak_period                      \n) %&gt;%\n  layout(\n    title = \"Average Accident Rate per Hour by Peak Period Type Over the Years\",\n    xaxis = list(title = \"Year\"),\n    yaxis = list(title = \"Average Accidents per Hour\"),\n    legend = list(title = list(text = \"Peak Period\"))\n  )\n\n# Display the plot\nfig_accidents_trend\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nObservations:\n\nThe Morning Peak consistently shows the highest average accident rate per hour across all years, peaking sharply in 2020.\nEvening Peak follows closely, with slightly lower accident rates compared to the Morning Peak but remaining higher than Off-Peak periods.\nOff-Peak accident rates are consistently lower across all years.\nThe year 2020 stands out with the highest accident rates across all peak periods, especially in the Morning Peak.\nOverall, both Morning and Evening Peak periods exhibit higher accident rates compared to Off-Peak periods, highlighting rush hours as critical times for road accidents.\n\n\n\n\n\n8.6 Visualize Geographic Distribution of Accidents by Weather Conditions\nNext, we visualize the distribution of accidents by weak conditions.\n\n\nShow the code\ntmap_mode('plot')\n\ntm_shape(admin_boundary_bmr) + \n  tm_polygons(border.col = \"darkgray\", alpha = 0.5) +\ntm_shape(roads_bmr) + \n  tm_lines(col = \"darkgreen\", lwd = 0.7) +\ntm_shape(accidents_bmr_ft) + \n  tm_dots(size = 0.2, col = \"red\", alpha = 0.6) +\n# facet by weather\ntm_facets(by = \"weather_condition\") +\ntm_layout(\n  main.title = \"Accident Trends by Weather in BMR (2019-2022)\",\n  main.title.size = 1.5, \n  main.title.position = c(\"center\", \"top\"), \n  frame = FALSE\n)\n\n\n\n\n\n\n\n\n\nWe noticed that most of accident data is recorded during clear conditions, followed by rainy conditions. We will discuss our observations in the following section with other factors.\n\n\n8.7 Visualize Other Factors\nIn this section, we consolidated our remaining observations of the accident dataset.\n\n\nShow the code\nselected_data &lt;- accidents_bmr_ft %&gt;%\n  st_drop_geometry() %&gt;%\n  select(presumed_cause, accident_type, road_description, weather_condition, vehicle_type)\n\ntheme_gtsummary_compact()\n\nselected_data %&gt;% \n  tbl_summary(missing_text = \"NA\", sort=all_categorical(FALSE) ~ \"frequency\") %&gt;% \n  add_n() %&gt;% \n  modify_caption(\"**Table of Variable Summary**\") %&gt;%\n  bold_labels()\n\n\n\n\n\n\nTable of Variable Summary\n\n\n\n\n\n\n\nCharacteristic\nN\nN = 12,986\n1\n\n\n\n\npresumed_cause\n12,986\n\n\n\n\n    speeding\n\n\n10,143 (78%)\n\n\n    other\n\n\n957 (7.4%)\n\n\n    cutting in closely by people/vehicles/animals\n\n\n621 (4.8%)\n\n\n    vehicle equipment failure\n\n\n365 (2.8%)\n\n\n    falling asleep\n\n\n221 (1.7%)\n\n\n    driving under the influence of alcohol\n\n\n118 (0.9%)\n\n\n    running red lights/traffic signals\n\n\n96 (0.7%)\n\n\n    tailgating\n\n\n83 (0.6%)\n\n\n    abrupt lane change\n\n\n59 (0.5%)\n\n\n    unfamiliarity with the route/unskilled driving\n\n\n53 (0.4%)\n\n\n    illegal overtaking\n\n\n50 (0.4%)\n\n\n    failure to yield/signal\n\n\n33 (0.3%)\n\n\n    debris/obstruction on the road\n\n\n29 (0.2%)\n\n\n    failure to yield right of way\n\n\n21 (0.2%)\n\n\n    worn-out/tire blowout\n\n\n19 (0.1%)\n\n\n    overloaded vehicle\n\n\n18 (0.1%)\n\n\n    reversing vehicle\n\n\n15 (0.1%)\n\n\n    dangerous curve\n\n\n14 (0.1%)\n\n\n    sudden stop\n\n\n14 (0.1%)\n\n\n    slippery road\n\n\n8 (&lt;0.1%)\n\n\n    brake/anti-lock brake system failure\n\n\n7 (&lt;0.1%)\n\n\n    driving in the wrong lane\n\n\n6 (&lt;0.1%)\n\n\n    loss of control\n\n\n6 (&lt;0.1%)\n\n\n    failure to signal enter/exit parking\n\n\n5 (&lt;0.1%)\n\n\n    using mobile phone while driving\n\n\n4 (&lt;0.1%)\n\n\n    disabled vehicle without proper signals/signs\n\n\n3 (&lt;0.1%)\n\n\n    insufficient light\n\n\n3 (&lt;0.1%)\n\n\n    road in poor condition\n\n\n2 (&lt;0.1%)\n\n\n    straddling lanes\n\n\n2 (&lt;0.1%)\n\n\n    disabled vehicle without proper signals\n\n\n1 (&lt;0.1%)\n\n\n    ignoring stop sign while leaving intersection\n\n\n1 (&lt;0.1%)\n\n\n    inadequate visibility\n\n\n1 (&lt;0.1%)\n\n\n    medical condition\n\n\n1 (&lt;0.1%)\n\n\n    no presumed cause related to driver\n\n\n1 (&lt;0.1%)\n\n\n    no road divider lines\n\n\n1 (&lt;0.1%)\n\n\n    no traffic signs\n\n\n1 (&lt;0.1%)\n\n\n    obstruction in sight\n\n\n1 (&lt;0.1%)\n\n\n    repair/construction on the road\n\n\n1 (&lt;0.1%)\n\n\n    using psychoactive substances\n\n\n1 (&lt;0.1%)\n\n\n    vehicle electrical system failure\n\n\n1 (&lt;0.1%)\n\n\naccident_type\n12,986\n\n\n\n\n    rear-end collision\n\n\n6,877 (53%)\n\n\n    rollover/fallen on straight road\n\n\n3,916 (30%)\n\n\n    other\n\n\n859 (6.6%)\n\n\n    collision with obstruction (on road surface)\n\n\n459 (3.5%)\n\n\n    rollover/fallen on curved road\n\n\n415 (3.2%)\n\n\n    side collision\n\n\n139 (1.1%)\n\n\n    pedestrian collision\n\n\n121 (0.9%)\n\n\n    head-on collision (not overtaking)\n\n\n102 (0.8%)\n\n\n    collision at intersection corner\n\n\n75 (0.6%)\n\n\n    collision during overtaking\n\n\n12 (&lt;0.1%)\n\n\n    turning/retreating collision\n\n\n11 (&lt;0.1%)\n\n\nroad_description\n12,986\n\n\n\n\n    straight road\n\n\n11,084 (85%)\n\n\n    other\n\n\n1,004 (7.7%)\n\n\n    wide curve\n\n\n488 (3.8%)\n\n\n    grade-separated intersection/ramps\n\n\n150 (1.2%)\n\n\n    y-intersection\n\n\n73 (0.6%)\n\n\n    t-intersection\n\n\n66 (0.5%)\n\n\n    connecting to public/commercial area\n\n\n43 (0.3%)\n\n\n    sharp curve\n\n\n41 (0.3%)\n\n\n    merge lane\n\n\n11 (&lt;0.1%)\n\n\n    connecting to private area\n\n\n8 (&lt;0.1%)\n\n\n    four-way intersection\n\n\n6 (&lt;0.1%)\n\n\n    u-turn area\n\n\n5 (&lt;0.1%)\n\n\n    connecting to school area\n\n\n4 (&lt;0.1%)\n\n\n    roundabout\n\n\n3 (&lt;0.1%)\n\n\nweather_condition\n12,986\n\n\n\n\n    clear\n\n\n11,711 (90%)\n\n\n    rainy\n\n\n1,166 (9.0%)\n\n\n    dark\n\n\n81 (0.6%)\n\n\n    other\n\n\n22 (0.2%)\n\n\n    foggy\n\n\n4 (&lt;0.1%)\n\n\n    land slide\n\n\n1 (&lt;0.1%)\n\n\n    natural disaster\n\n\n1 (&lt;0.1%)\n\n\nvehicle_type\n12,986\n\n\n\n\n    private/passenger car\n\n\n4,486 (35%)\n\n\n    4-wheel pickup truck\n\n\n3,522 (27%)\n\n\n    motorcycle\n\n\n1,687 (13%)\n\n\n    other\n\n\n1,088 (8.4%)\n\n\n    large truck with trailer\n\n\n998 (7.7%)\n\n\n    6-wheel truck\n\n\n472 (3.6%)\n\n\n    7-10-wheel truck\n\n\n389 (3.0%)\n\n\n    van\n\n\n154 (1.2%)\n\n\n    large passenger vehicle\n\n\n76 (0.6%)\n\n\n    passenger pickup truck\n\n\n63 (0.5%)\n\n\n    bicycle\n\n\n18 (0.1%)\n\n\n    pedestrian\n\n\n18 (0.1%)\n\n\n    motorized tricycle\n\n\n14 (0.1%)\n\n\n    tractor/agricultural vehicle\n\n\n1 (&lt;0.1%)\n\n\n\n1\nn (%)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nObservations:\n\nTop Causes of Accidents: Behavioral Factors\n\nSpeeding is the leading presumed cause, responsible for 78% of accidents (10,143 cases). This is a clear indication that driver behavior is the primary factor.\nOther behavioral causes include cutting in closely (4.8%), driving under the influence of alcohol (0.9%), and tailgating (0.6%).\n\nMinor Contribution from Road Conditions\n\nRoad conditions such as slippery roads, debris, and road in poor condition account for a very small portion of the accidents (&lt;1%).\n\nWeather Conditions\n\nClear weather dominates in 90% of cases (as visualized in earlier section), while rainy conditions account for only 9% of accidents. Adverse weather such as foggy or dark conditions plays a negligible role.\n\nAccident Types\n\nMost accidents are rear-end collisions (53%) and rollovers on straight roads (30%), again pointing towards driver behavior on straightforward road networks.\n\nRoad Types\n\nStraight roads are where the vast majority (85%) of accidents occur.\n\nVehicle Types\n\nThe majority of accidents involve private/passenger cars (35%) and 4-wheel pickup trucks (27%), followed by Motorcycles (13%)\n\n\nIn summary, based on this dataset, we can observe that behavioral factors like speeding are overwhelmingly the top causes of accidents, while road conditions and weather play a much smaller role."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#spatial-measures-of-central-tendency",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#spatial-measures-of-central-tendency",
    "title": "Take Home Exercise 1",
    "section": "9 Spatial Measures of Central Tendency",
    "text": "9 Spatial Measures of Central Tendency\nIn this section, we are interested in identifying the centrality of road traffic accidents within the Bangkok Metropolitan Region (BMR) by computing two key spatial measures: the mean center and the median center.\nThe mean centre represents the average location of all accident points in the region while the median center which is less influenced by outliers, provides a more robust indication of where accidents tend to cluster, unaffected by unusually high or low values in specific areas.\n\n# Extract coordinates\naccidents_xy &lt;- st_coordinates(accidents_bmr_ft)\n\nmean_center &lt;- accidents_xy %&gt;%\n  colMeans()\n\nmean_center\n\n        X         Y \n 668399.5 1523495.8 \n\n\n\nmedian_center &lt;- accidents_xy %&gt;%\n  apply(2, median)\n\nmedian_center\n\n        X         Y \n 673446.1 1520755.0 \n\n\n\n\n\n\n\n\nNote\n\n\n\nObservations:\nThe similarity between the mean center and median center suggests that the distribution of road accidents in the Bangkok Metropolitan Region is relatively balanced, with no significant outliers skewing the spatial patterns.\n\n\n\n9.1 Spatial Measures of Central Tendency Over the Years\nIn this section, we are interested to observe in what direction has the mean centre of the Thailand road accidents moved over the years.\n\n\nShow the code\nmean_median_by_year &lt;- accidents_bmr_ft %&gt;%\n  group_by(year) %&gt;%\n  summarise(\n    mean_center_x = mean(st_coordinates(geometry)[, 1]),\n    mean_center_y = mean(st_coordinates(geometry)[, 2]),\n    median_center_x = median(st_coordinates(geometry)[, 1]),\n    median_center_y = median(st_coordinates(geometry)[, 2])\n  )\n\nplot(st_geometry(admin_boundary_bmr), main = \"Mean Centers of Accidents in BMR over the Years\")\n\npoints(accidents_xy[, 1], accidents_xy[, 2], cex = 0.7, pch = 21)\n\ncolors &lt;- c(\"goldenrod2\", \"sienna2\", \"hotpink1\", \"red1\")\nyears &lt;- unique(accidents_bmr_ft$year)\n\nfor (i in 1:nrow(mean_median_by_year)) {\n  points(mean_median_by_year$mean_center_x[i], mean_median_by_year$mean_center_y[i], \n         pch = '*', col = colors[i], cex = 4, lwd = 2)\n}\n\nlegend(\"topright\", legend = paste0(\"Year: \", years), col = colors, pch = '*', \n       pt.cex = 1.5, title = \"Centers\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nObservations:\n\nThe mean centers of accidents in BMR over the years from 2019 to 2022 are relatively close to each other within the Bangkok province, indicating that the central tendency of accidents has not shifted dramatically during this period.\nThe points show a slight progression towards the southeast over time. This may suggest a gradual shift in the concentration of accidents towards a different district of the Bangkok province."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#spatial-point-pattern-analysis-sppa",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#spatial-point-pattern-analysis-sppa",
    "title": "Take Home Exercise 1",
    "section": "10 Spatial Point Pattern Analysis (SPPA)",
    "text": "10 Spatial Point Pattern Analysis (SPPA)\nIn this section, we will conduct Spatial Point Pattern Analysis (SPPA) using the spatstat package to quantify the spatial distribution of road traffic accidents in the Bangkok Metropolitan Region (BMR). This method determines whether accidents are randomly distributed, clustered, or follow a regular pattern.\nTo explore this, we will test the Complete Spatial Randomness (CSR) hypothesis, which assumes accidents occur independently and uniformly across the region.\nWe will apply three SPPA methods:\n\nFirst-Order SPPA: This examines broader trends in accident intensity using techniques like Kernel Density Estimation (KDE) to identify “hot spots” and assess clustering or regularity.\nSecond-Order SPPA: Tools like Ripley’s K-function, G, F, and L functions will be applied to assess spatial dependence at varying distances, helping to detect clustering or dispersion.\nNetwork-Constrained SPPA: This method analyzes accident patterns along road networks, recognizing that accidents are restricted to roads. It offers a more realistic view of clustering based on the actual network structure.\n\n\nThe key questions in this section are:\n\nAre the road accidents in BMR randomly distributed throughout the region?\nIf not, where are the areas with higher accident concentrations?"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#converting-sf-format-into-ppp-format",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#converting-sf-format-into-ppp-format",
    "title": "Take Home Exercise 1",
    "section": "11 Converting sf Format into ppp format",
    "text": "11 Converting sf Format into ppp format\nBefore we can perform spatial point analysis with spatstat, we will convert the sf objects into ppp object using as.ppp().\n\n# convert to ppp and rescale from m to km)\naccidents_ppp.km &lt;- rescale(as.ppp(accidents_bmr_ft), 1000, \"km\")\n\nNext, we check for duplicated entries within the point pattern object. Using unmark, we are only comparing the coordinates, regardless of marks.\n\nany(duplicated(accidents_ppp.km, rule=\"unmark\"))\n\n[1] TRUE\n\n\nSince there may be overlapping points due to accidents occurring at the same location, we apply a random jitter to the points. This reduces overlap by introducing slight random shifts, and we summarize the jittered point pattern to verify the changes.\n\naccidents_ppp_jit.km &lt;- rjitter(accidents_ppp.km, \n                                        retry=TRUE, \n                                        nsim=1, \n                                        drop=TRUE)\n\nsummary(accidents_ppp_jit.km)\n\nMarked planar point pattern:  12986 points\nAverage intensity 1.218049 points per square km\n\nCoordinates are given to 13 decimal places\n\nmarks are numeric, of type 'double'\nSummary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 571882 3788970 3834532 4314457 6092694 7570954 \n\nWindow: rectangle = [591.2775, 710.1661] x [1486.8457, 1576.5205] km\n                    (118.9 x 89.67 km)\nWindow area = 10661.3 square km\nUnit of length: 1 km\n\n\nNext we create the owin object which is the administrative boundary of the Bangkok Metropolitan Region (BMR). We also rescale the boundary to match the units used in the point pattern (kilometers).\n\nowin_bmr.km &lt;- rescale(as.owin(admin_boundary_bmr), 1000)\nowin_bmr.km\n\nwindow: polygonal boundary\nenclosing rectangle: [587.8935, 712.4405] x [1484.4137, 1579.0763] units\n\n\nAfter defining the observation window, we subset the jittered point pattern to include only the accident points that fall within the boundary of the BMR.\nWe then summarize the resulting point pattern to ensure that the filtering was successful.\n\naccidents_owin.km &lt;- accidents_ppp_jit.km[owin_bmr.km]\nsummary(accidents_owin.km)\n\nMarked planar point pattern:  12981 points\nAverage intensity 1.69266 points per square km\n\nCoordinates are given to 13 decimal places\n\nmarks are numeric, of type 'double'\nSummary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 571882 3788970 3835196 4314334 6092693 7570954 \n\nWindow: polygonal boundary\nsingle connected closed polygon with 13779 vertices\nenclosing rectangle: [587.8935, 712.4405] x [1484.4137, 1579.0763] km\n                     (124.5 x 94.66 km)\nWindow area = 7668.99 square km\nUnit of length: 1 km\nFraction of frame area: 0.65\n\n\nNext, we generate the plot of the spatial distribution of road accidents within the BMR boundary.\n\nplot(accidents_owin.km)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#first-order-spatial-point-pattern-analysis",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#first-order-spatial-point-pattern-analysis",
    "title": "Take Home Exercise 1",
    "section": "12 First Order Spatial Point Pattern Analysis",
    "text": "12 First Order Spatial Point Pattern Analysis\nIn this section, we conduct first-order Spatial Point Pattern Analysis (SPPA) using the spatstat package to explore the intensity of traffic accidents. The analysis will include:\n\nKernel Density Estimation (KDE): Estimating accident intensity for visualizing and understanding spatial concentration of accident points.\nNearest Neighbour Analysis: To confirm spatial patterns by calculating nearest-neighbour statistics.\n\n\n12.1 Kernel Density Estimation (KDE)\nKernel Density Estimation (KDE) provides a smooth estimate of the intensity of point processes, allowing us to visualize accident density hotspots. In this step, we experiment with different automatic bandwidth selection methods to determine the most suitable one for our analysis.\n\nbw.CvL(accidents_ppp_jit.km)\n\n  sigma \n31.4123 \n\nbw.scott(accidents_ppp_jit.km)\n\n sigma.x  sigma.y \n4.530017 3.322396 \n\nbw.ppl(accidents_ppp_jit.km)\n\n    sigma \n0.4197952 \n\nbw.diggle(accidents_ppp_jit.km)\n\n     sigma \n0.04745292 \n\n\n\n\nShow the code\n# try different bandwidth methods\n# par(mfrow=c(2,2))\npar(mfrow=c(1,2))\nkde_accidents_bw_km &lt;- density(accidents_ppp_jit.km, \n                               sigma = bw.diggle,\n                               edge=TRUE,kernel='gaussian')\n\nkde_accidents_ppl_km &lt;- density(accidents_ppp_jit.km, \n                               sigma = bw.ppl,\n                               edge=TRUE,kernel='gaussian')\n\n# kde_accidents_cvl_km &lt;- density(accidents_ppp_jit.km, \n#                               sigma = bw.CvL,\n#                               edge=TRUE,kernel='gaussian')\n\n#kde_accidents_scott_km &lt;- density(accidents_ppp_jit.km, \n#                               sigma = bw.scott,\n#                               edge=TRUE,kernel='gaussian')\n\nplot(kde_accidents_bw_km)\nplot(kde_accidents_ppl_km)\n\n\n\n\n\n\n\n\n\nShow the code\n#plot(kde_accidents_cvl_km)\n#plot(kde_accidents_scott_km)\n\n\n\n\n\n\n\n\nNote\n\n\n\nObservations:\n\nThe Diggle algorithm give the narrower bandwidth, making it ideal for identifying precise accident clusters in this study, as it captures spatial details without excessive smoothing.\n\n\n\n\n\n12.2 Comparing Spatial Point Patterns using KDE\nIn this section, we will compare KDE of road accidents across provinces in BMR. This comparison allows us to visually explore the variation in accident intensity across the provinces using KDE.\nWe first create a list of owin objects for each province, rescaling the spatial windows to kilometers. Then we subset the jittered point pattern of road accidents for each province and compute the KDE for each region using bw.diggle.\n\n\nShow the code\n# make individual owins\nbmr_owins &lt;- lapply(bmr_regions, function(region) {\n  rescale(as.owin(admin_boundary_bmr %&gt;% filter(ADM1_EN == region)), 1000)\n})\n\nnames(bmr_owins) &lt;- bmr_regions\n\n\naccidents_ppps &lt;- lapply(bmr_owins, function(owin) {\n  accidents_ppp_jit.km[owin]\n})\n\ndensities &lt;- lapply(seq_along(bmr_regions), function(i) {\n  region &lt;- bmr_regions[i]\n  density(accidents_ppps[[region]], \n          sigma = bw.diggle, \n          edge = TRUE, \n          kernel = \"gaussian\")\n})\nnames(densities) &lt;- bmr_regions\n\n\n\n\nShow the code\npar(mfrow = c(3, 2), mar = c(5, 5, 2, 1))\n\n# suppress lapply output\ninvisible(lapply(seq_along(densities), function(i) {\n  region &lt;- names(densities)[i]\n  plot(densities[[i]], main = region)\n}))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nObservations:\n\nAccidents appears to cluster along the major highways of each province.\nBangkok and Samut Prakan have the highest KDE values for accidents, with densities reaching up to 500 and 600, respectively, primarily along major highways; while Nakhon Pathom and Nonthaburi have lower KDE values among the 6 provinces.\n\n\n\n\n\n12.3 Nearest Neighbour Analysis: Clark-Evans Test\nAfter performing the first-order spatial point pattern analysis, we move on to Nearest Neighbour Analysis using the Clark-Evans test to quantitatively assess whether the traffic accident distribution follows a random, clustered, or dispersed pattern. This test complements the visual insights gained from the Kernel Density Estimation (KDE) by providing statistical evidence for spatial clustering.\n\nThe Clark-Evans aggregation index (R) compares the observed mean nearest neighbour distance to the expected distance under Complete Spatial Randomness (CSR).\nIf R = 1, the points are randomly distributed.\nIf R &lt; 1, the points exhibit clustering.\nIf R &gt; 1, the points are more regularly spaced, suggesting spatial dispersion.\n\nThe test hypotheses are:\n\n\\(H_0\\): The traffic accidents are randomly distributed.\n\\(H_1\\): The traffic accidents are not randomly distributed (i.e., they are clustered or ordered).\n\nWe use the 95% confidence interval for decision-making.\n\nclarkevans.test(accidents_ppp.km,\n                correction=\"none\",\n                clipregion=\"owin_bmr.km\",\n                alternative=c(\"clustered\"),\n                nsim=99)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  accidents_ppp.km\nR = 0.16207, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe result of the Clark-Evans test shows:\n\nR = 0.16207, which is significantly less than 1, indicating that the traffic accidents exhibit strong spatial clustering.\nThe p-value &lt; 2.2e-16, which is much lower than the 0.05 significance level, thus we reject the null hypothesis (\\(H_0\\)).\n\nThe traffic accidents are not randomly distributed but are clustered, confirming that certain areas or road segments in the Bangkok Metropolitan Region (BMR) have significantly higher concentrations of accidents.\n\n\n\n\n12.4 Clark-Evans Test for Individual Provinces\nNext, we perform the Clark-Evans test for each province in the BMR to assess the spatial distribution of traffic accidents.\n\nfor (i in seq_along(accidents_ppps)) {\n  accidents_pr_ppp &lt;- accidents_ppps[[i]]\n  \n  cat(\"\\n##\", bmr_regions[i], \"\\n\")  # Print the region name for clarity\n  print(\n    clarkevans.test(accidents_pr_ppp,\n                    correction=\"none\",\n                    clipregion=NULL,\n                    alternative=c(\"two.sided\"),\n                    nsim=99)\n  )\n}\n\n\n## Bangkok \n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  accidents_pr_ppp\nR = 0.17623, p-value &lt; 2.2e-16\nalternative hypothesis: two-sided\n\n\n## Nonthaburi \n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  accidents_pr_ppp\nR = 0.41798, p-value &lt; 2.2e-16\nalternative hypothesis: two-sided\n\n\n## Nakhon Pathom \n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  accidents_pr_ppp\nR = 0.30852, p-value &lt; 2.2e-16\nalternative hypothesis: two-sided\n\n\n## Pathum Thani \n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  accidents_pr_ppp\nR = 0.28094, p-value &lt; 2.2e-16\nalternative hypothesis: two-sided\n\n\n## Samut Prakan \n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  accidents_pr_ppp\nR = 0.18842, p-value &lt; 2.2e-16\nalternative hypothesis: two-sided\n\n\n## Samut Sakhon \n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  accidents_pr_ppp\nR = 0.27484, p-value &lt; 2.2e-16\nalternative hypothesis: two-sided\n\n\n\n\n\n\n\n\nNote\n\n\n\nObservations:\n\nBangkok:\n\nR = 0.17306, p-value &lt; 2.2e-16\n\nThe R-value is significantly less than 1, indicating strong clustering of accidents in the province. The very low p-value confirms the clustering pattern is statistically significant.\n\nNonthaburi:\n\nR = 0.41915, p-value &lt; 2.2e-16\n\nThere is moderate clustering in Nonthaburi, though the clustering is less intense than in Bangkok. The p-value indicates this pattern is statistically significant.\n\nNakhon Pathom:\n\nR = 0.30873, p-value &lt; 2.2e-16\n\nThe results show moderate clustering of accidents in this province. The clustering is stronger than in Nonthaburi but still less pronounced than in Bangkok.\n\nPathum Thani:\n\nR = 0.27889, p-value &lt; 2.2e-16\n\nThere is moderate clustering of accidents in Pathum Thani, with an R-value similar to that of Nakhon Pathom, indicating a non-random pattern of accident distribution.\n\nSamut Prakan:\n\nR = 0.191, p-value &lt; 2.2e-16\n\nSamut Prakan exhibits strong clustering of accidents, with an R-value similar to Bangkok, suggesting accident hotspots in the province. This is confirmed by the extremely low p-value.\n\nSamut Sakhon:\n\nR = 0.27326, p-value &lt; 2.2e-16\n\nThe results indicate moderate clustering of accidents in this province, similar to Pathum Thani and Nakhon Pathom, suggesting multiple local accident clusters.\n\n\nIn summary, we reject the null hypothesis across all provinces in the BMR. The Clark-Evans test results show that traffic accidents are not randomly distributed but are strongly clustered. Bangkok and Samut Prakan exhibit the highest levels of clustering, while the other provinces show moderate clustering."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#second-order-spatial-point-pattern-analysis",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#second-order-spatial-point-pattern-analysis",
    "title": "Take Home Exercise 1",
    "section": "13 Second Order Spatial Point Pattern Analysis",
    "text": "13 Second Order Spatial Point Pattern Analysis\nIn this section, we will focus on the K-function to analyze the overall pattern of clustering or dispersion of road accidents across various distances. We opt for the K-function due to its ability to capture the disc-like accumulation of points, making it ideal for understanding broader clustering trends over various distances.\n\n13.1 Compute K-Function Estimate for BMR\nFirst, we will compute the K-function to estimate the spatial relationships between accidents across the BMR.\n\n\nShow the code\nK_BMR = Kest(accidents_ppp.km, correction = \"Ripley\")\nplot(K_BMR, . -r ~ r, ylab= \"K(d)-r\", xlab = \"d(km)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nObservations:\n\nThe K-function shows strong clustering of accidents across multiple distances, as the empirical curve is significantly above the Poisson process line.\nClustering exists at both local and broader spatial scales.\n\n\n\n\n\n13.2 Perform Complete Spatial Randomness Test\nTo further analyze whether the distribution of accidents deviates from Complete Spatial Randomness (CSR), we conduct a hypothesis test with the following hypotheses:\n\n\\(H_0\\): The distribution of accidents is randomly distributed across the BMR.\n\\(H_1\\): The distribution of accidents is not randomly distributed (i.e., clustered or dispersed).\n\nWe use a 95% confidence interval for this test.\n\n\nShow the code\nbmr_K.csr &lt;- envelope(accidents_ppp.km, Kest, nsim = 49, rank = 1,  glocal=TRUE)\n\n\nGenerating 49 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, \n49.\n\nDone.\n\n\nShow the code\nplot(bmr_K.csr, . - r ~ r, xlab=\"d (km)\", ylab=\"K(d)-r\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nObservations:\n\nThe empirical K-function lies above the theoretical K-function , indicating significant clustering of accidents across all distances.\nThe envelope (shaded area) confirms that the observed clustering is statistically significant, as the empirical K-function remains outside the bounds expected under complete spatial randomness.\nThus we will reject the null hypothesis."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#network-constrained-spatial-point-pattern-analysis",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#network-constrained-spatial-point-pattern-analysis",
    "title": "Take Home Exercise 1",
    "section": "14 Network Constrained Spatial Point Pattern Analysis",
    "text": "14 Network Constrained Spatial Point Pattern Analysis\nNetwork-Constrained Spatial Point Pattern Analysis (SPPA) is highly relevant for studying road accidents because these events are restricted to occurring along specific networks such as highways. Traditional spatial analysis techniques assume that events can occur anywhere in a continuous space, which is not realistic for accidents that are confined to road networks. By focusing on the structure and connectivity of roads, network-constrained methods provide more accurate insights into the spatial distribution of accidents, identifying high-risk areas such as intersections, highway segments, or urban streets.\nIn this section, we will use the spNetwork package to compute Network Kernel Density Estimation (NKDE). NKDE estimates the density of accidents while considering the network structure, offering a more realistic view of accident hotspots along the road network.\n\n14.1 Preparing the Lixels Objects\nBefore computing NKDE, the SpatialLines object need to be cut into lixels with a specified minimal distance. This task can be performed by using with lixelize_lines() of spNetwork.\n\n\n\n\n\n\nNote\n\n\n\nChoice of lixel length Initially, We chosen lixel length of 100 and mindist of 50, based on recommended settings from related research: Visualizing Traffic Accident Hotspots Based on Spatial-Temporal Network Kernel Density Estimation.\nHowever, after visualizing the NKDE results, the shorter lixel length failed to reveal meaningful trends, likely due to an overly detailed segmentation of the network. To correct this, we increased the lixel length by an order of magnitude, allowing for a broader view and better identification of accident patterns at a larger scale.\n\n\n\n# filter for linestring only else error\nroads_bmr&lt;- roads_bmr %&gt;%\n  filter(st_geometry_type(.) == \"LINESTRING\")\n\nlixels &lt;- lixelize_lines(lines = roads_bmr,\n                         # increase by 1 magnitude\n                         lx_length = 5000,\n                         mindist = 2500)\n\nNext, we will use lines_center() of spNetwork to generate a SpatialPointsDataFrame (i.e. samples) with line centre points.\n\nsamples &lt;- lines_center(lixels)\n\nTo compute the NKDE:\n\n\nShow the code\ndensities &lt;- nkde(\n  lines = roads_bmr, \n  events = accidents_bmr_ft,\n  w = rep(1, nrow(accidents_bmr_ft)),\n  samples = samples,\n  kernel_name = \"quartic\",\n  # increase by 1 magnitude\n  bw = 2250,\n  div = \"bw\",\n  method = \"simple\", \n  digits = 1, \n  tol = 1,\n  grid_shape = c(10,10), \n  max_depth = 8,\n  # agg = 5, \n  sparse = TRUE,\n  verbose = TRUE)\n\n\nWe will save the computation output as RDS data format for future analysis.\n\nwrite_rds(densities, \"data/rds/densities_2250.rds\")\n\n\ndensities &lt;- read_rds(\"data/rds/densities_2250.rds\")\n\nTo visualise the NKDE values, we have to perform a few preparation steps.\n\nInsert the computed density values (i.e. densities) into samples and lixels objects as density field. Rescale the density values.\n\n\nsamples$density &lt;- densities\nlixels$density &lt;- densities\nsamples$density &lt;- samples$density*1000\nlixels$density &lt;- lixels$density*1000\n\n\nUse tmap to visualize the NKDE output.\n\n\ncus_palette &lt;- colorRampPalette(c(\"lightyellow\", \"red\"))\n\ntm_shape(admin_boundary_bmr) +\n  tm_polygons(col = \"white\", border.col = \"black\") +\n  tm_shape(lixels) +\n  tm_lines(col = \"density\", palette = cus_palette(10), style = \"cont\", lwd = 2) + \n  tm_layout(\n    title = \"Network Kernel Density of Road Accidents in BMR\",\n    title.size = 1.5,\n    title.position = c(\"center\", \"top\"),\n    frame = FALSE, \n    legend.position = c(\"left\", \"bottom\"),\n    legend.title.size = 1.0,\n    legend.text.size = 0.8\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nObservations:\n\nThe density patterns tend to follow major highways and key intersections, particularly in the central and central-eastern areas of Bangkok Metropolitan Region (BMR), where the density of road accidents is higher."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#spatial-temporal-point-pattern-analysis",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#spatial-temporal-point-pattern-analysis",
    "title": "Take Home Exercise 1",
    "section": "15 Spatial Temporal Point Pattern Analysis",
    "text": "15 Spatial Temporal Point Pattern Analysis\nSpatio-temporal analysis combines both spatial and temporal aspects to reveal how events like road traffic accidents vary over time and location.\nIn this section, we use Spatio-temporal Kernel Density Estimation (STKDE) to visualize accident densities across space and time, capturing fluctuations in accident intensity throughout different periods.\n\nThe key questions in this section are:\n\nAre the locations of accidents in BMR spatial and spatio-temporally independent?\nIf not, where and when the observed accident locations tend to cluster?\n\n\nFor this analysis, we will focus on 2022 as it has the highest accident rate from our EDA results above.\n\n15.1 Computing STKDE by Month\nWe first filter the dataset to include only the road accidents that occurred in the year 2022.\n\naccidents_month_2022  &lt;- accidents_bmr_ft %&gt;%\n  filter(year == 2022) %&gt;%\n  select(month_number)\n\nNext, we convert the filtered accident data into a point pattern object (ppp) for spatial analysis. To ensure the plot scaling is more readable and appropriate for our analysis, we rescale the spatial units from meters to kilometers.\n\naccidents_month_2022_ppp &lt;- as.ppp(accidents_month_2022)\n# rescale from m to km\naccidents_month_2022_ppp.km &lt;- rescale(accidents_month_2022_ppp, 1000, \"km\")\n\nNext, we check for duplicated entries within the point pattern object.\n\nany(duplicated(accidents_month_2022_ppp.km))\n\n[1] TRUE\n\n\nSince there may be overlapping points due to accidents occurring at the same location, we apply a random jitter to the points. This reduces overlap by introducing slight random shifts, and we summarize the jittered point pattern to verify the changes.\n\naccidents_month_2022_ppp_jit.km &lt;- rjitter(accidents_month_2022_ppp.km, \n                                        retry=TRUE, \n                                        nsim=1, \n                                        drop=TRUE)\n\nsummary(accidents_month_2022_ppp_jit.km)\n\nMarked planar point pattern:  3593 points\nAverage intensity 0.3520227 points per square km\n\nCoordinates are given to 13 decimal places\n\nmarks are numeric, of type 'double'\nSummary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00    4.00    7.00    6.99   10.00   12.00 \n\nWindow: rectangle = [595.5406, 709.36] x [1486.8457, 1576.5205] km\n                    (113.8 x 89.67 km)\nWindow area = 10206.7 square km\nUnit of length: 1 km\n\n\nNext we create the owin object which is the administrative boundary of the Bangkok Metropolitan Region (BMR). We also rescale the boundary to match the units used in the point pattern (kilometers).\n\nowin_bmr &lt;- as.owin(admin_boundary_bmr)\nowin_bmr.km &lt;- rescale(owin_bmr, 1000)\nowin_bmr.km\n\nwindow: polygonal boundary\nenclosing rectangle: [587.8935, 712.4405] x [1484.4137, 1579.0763] units\n\n\nAfter defining the observation window, we subset the jittered point pattern to include only the accident points that fall within the boundary of the BMR.\nWe then summarize the resulting point pattern to ensure that the filtering was successful.\n\naccidents_month_2022_owin.km &lt;- accidents_month_2022_ppp_jit.km[owin_bmr.km]\nsummary(accidents_month_2022_owin.km)\n\nMarked planar point pattern:  3592 points\nAverage intensity 0.4683796 points per square km\n\nCoordinates are given to 13 decimal places\n\nmarks are numeric, of type 'double'\nSummary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   4.000   7.000   6.989  10.000  12.000 \n\nWindow: polygonal boundary\nsingle connected closed polygon with 13779 vertices\nenclosing rectangle: [587.8935, 712.4405] x [1484.4137, 1579.0763] km\n                     (124.5 x 94.66 km)\nWindow area = 7668.99 square km\nUnit of length: 1 km\nFraction of frame area: 0.65\n\n\nNext, we generate the plot of the spatial distribution of road accidents within the BMR boundary.\n\nplot(accidents_month_2022_owin.km)\n\n\n\n\n\n\n\n\nNext, we perform a Spatio-temporal Kernel Density Estimation (STKDE) on the accident data. This analysis estimates the density of road accidents in both spatial and temporal dimensions, allowing us to observe how accident densities vary over time and across locations. We then summarize the STKDE output.\n\nst_kde &lt;- spattemp.density(accidents_month_2022_owin.km)\nsummary(st_kde)\n\nSpatiotemporal Kernel Density Estimate\n\nBandwidths\n  h = 4.8827 (spatial)\n  lambda = 0.0285 (temporal)\n\nNo. of observations\n  3592 \n\nSpatial bound\n  Type: polygonal\n  2D enclosure: [587.8935, 712.4405] x [1484.414, 1579.076]\n\nTemporal bound\n  [1, 12]\n\nEvaluation\n  128 x 128 x 12 trivariate lattice\n  Density range: [5.335629e-16, 0.002847148]\n\n\nFinally, we plot the results of the spatio-temporal KDE for each month of the year 2022. These plots illustrate the monthly variation in road accident densities across the Bangkok Metropolitan Region.\n\npar(mfrow = c(4,3), mar = c(2, 2, 2, 2))\n\nfor(i in seq(1, 12)){ \n  plot(st_kde, i, \n       override.par=FALSE, \n       fix.range=TRUE, \n       main=paste(\"KDE at month\", i))\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nMonth 1: The accident hotspots are primarily concentrated in the central BMR, with moderate intensity and slight diffusion towards the outer areas.\nMonth 2-7: Generally, there is a lower intensity of hotspots during this periods\nMonth 8-11: Bangkok remains the area of highest accident density; the clusters are seen to increase in size.\nMonth 12: Accident density peaks towards the end of the year, with a notable increase in the central region, especially in December. Surrounding areas also show a rise in accident activity, likely due to higher traffic or adverse conditions during this holiday time. see Bangkok Post - Road deaths rise to 256 after 5 days of New Year holiday travel\n\n\n\n\n15.2 Observing the Impact of the “7 Deadly Days” of Songkran using STKDE\nIn this section, we apply Spatio-temporal Kernel Density Estimation (STKDE) to analyze accident patterns during the 7 Deadly Days of Songkran in April 2022. By focusing on the month of April, particularly the Songkran festival, STKDE allows us to capture and visualize how accident densities vary spatially and temporally (before, during and after the festival).\n\naccidents_april_2022  &lt;- accidents_bmr_ft %&gt;%\n  filter(year == 2022, month_number == 4) %&gt;%\n  select(day_of_month)\n\naccidents_april_2022_ppp.km &lt;- rescale(as.ppp(accidents_april_2022), 1000, \"km\")\n\nNext, we check for duplicated entries within the point pattern object.\n\nany(duplicated(accidents_april_2022_ppp.km, rule=\"unmark\"))\n\n[1] TRUE\n\n\nSince there may be overlapping points due to accidents occurring at the same location, we apply a random jitter to the points. This reduces overlap by introducing slight random shifts, and we summarize the jittered point pattern to verify the changes. Then, we subset the jittered point pattern to include only the accident points that fall within the boundary of the BMR.\n\naccidents_april_2022_ppp_jit.km &lt;- rjitter(accidents_april_2022_ppp.km, \n                                        retry=TRUE, \n                                        nsim=1, \n                                        drop=TRUE)\n\naccidents_april_2022_owin.km &lt;- accidents_april_2022_ppp_jit.km[owin_bmr.km]\nsummary(accidents_april_2022_owin.km)\n\nMarked planar point pattern:  331 points\nAverage intensity 0.04316082 points per square km\n\nCoordinates are given to 13 decimal places\n\nmarks are numeric, of type 'integer'\nSummary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    1.0    10.5    14.0    15.3    21.0    30.0 \n\nWindow: polygonal boundary\nsingle connected closed polygon with 13779 vertices\nenclosing rectangle: [587.8935, 712.4405] x [1484.4137, 1579.0763] km\n                     (124.5 x 94.66 km)\nWindow area = 7668.99 square km\nUnit of length: 1 km\nFraction of frame area: 0.65\n\n\n\nplot(accidents_april_2022_owin.km, \n     main = \"Spatial Distribution of Road Accidents in April 2022 (BMR)\")\n\n\n\n\n\n\n\n\nNext, we perform a Spatio-temporal Kernel Density Estimation (STKDE) on the accident data for April 2022. This analysis estimates the density of road accidents in both spatial and temporal dimensions, allowing us to observe how accident densities vary across locations for the month of April.\n\nst_kde_april &lt;- spattemp.density(accidents_april_2022_owin.km)\nsummary(st_kde_april)\n\nSpatiotemporal Kernel Density Estimate\n\nBandwidths\n  h = 7.7139 (spatial)\n  lambda = 1.513 (temporal)\n\nNo. of observations\n  331 \n\nSpatial bound\n  Type: polygonal\n  2D enclosure: [587.8935, 712.4405] x [1484.414, 1579.076]\n\nTemporal bound\n  [1, 30]\n\nEvaluation\n  128 x 128 x 30 trivariate lattice\n  Density range: [4.084262e-14, 3.13749e-05]\n\n\nFinally, we plot the results of the spatio-temporal KDE for each day of April 2022. These plots illustrate the monthly variation in road accident densities across the Bangkok Metropolitan Region.\n\ndays_per_plot &lt;- 6\n\ntotal_days &lt;- 30\n\nfor (i in seq(1, total_days, by = days_per_plot)) {\n  # (2 rows, 3 columns)\n  par(mfrow = c(2, 3), mar = c(2, 2, 2, 2))\n  \n  for (day in i:(i + days_per_plot - 1)) {\n    if (day &gt; total_days) break\n    \n    # Highlight Songkran period (April 11-17)\n    if (day &gt;= 11 & day &lt;= 17) {\n      plot(st_kde_april, day, \n           override.par = FALSE, \n           fix.range = TRUE, \n           main = paste(\"April 2022\", day, \"(Songkran)\"), \n           col.main = \"red\")\n    } else {\n      plot(st_kde_april, day, \n           override.par = FALSE, \n           fix.range = TRUE, \n           main = paste(\"April 2022\", day))\n    }\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nObservations for April 2022:\n\n1st to 10th April:\n\nThe highest density of accidents remains concentrated in the central region of the Bangkok Metropolitan Area (BMR), with some variation day-to-day.\nA pattern of increasing intensity is seen leading up to the Songkran festival.\n\nSongkran Festival (11th to 17th April):\n\n11th to 13th April: Significant increase in accident density during the Songkran holiday, particularly in central and southern areas.\n14th to 17th April: Accident hotspots during the Songkran period appear to expand further, possibly due to higher travel activities and road congestion.\nThe Songkran period highlights the most critical accident-prone days, showing a stark rise in traffic incidents.\n\n18th to 30th April:\n\nAccident density slightly decreases after the Songkran period but remains concentrated in similar central zones.\nPost-Songkran accident rates maintain a moderate level."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#conclusion",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#conclusion",
    "title": "Take Home Exercise 1",
    "section": "16 Conclusion",
    "text": "16 Conclusion\nThe analysis of road traffic accidents in the Bangkok Metropolitan Region (BMR) provided insights into both spatial and temporal patterns of these incidents. By the data processing steps, we added columns such as seasons, peak periods, and Songkran holidays, capturing key factors that may influence accident trends.\nSpatial analysis revealed significant clustering of accidents in the central urban areas of Bangkok and Samut Prakan. These provinces exhibited the highest Kernel Density Estimates (KDE), indicating they are major hotspots for road traffic accidents.The Clark-Evans test and second-order spatial point pattern analysis further confirmed that traffic accidents in the BMR are not randomly distributed but instead follow a clustered pattern. The consistently low R values and highly significant p-values across all six provinces underscored this spatial clustering, highlighting the concentration of accidents in certain areas. Using Network Kernel Density Estimation (NKDE), we observed that accidents are concentrated along specific road segments, particularly major highways and intersections.\nSpatio-temporal KDE provided insights into how accident patterns vary over time. The analysis captured monthly trends, revealing that accident density fluctuates throughout the year, with higher concentrations observed at the beginning and end of the year, particularly in December. This suggests seasonal influences, such as changes in traffic patterns or road conditions during the holiday season. The subsequent analysis highlighted a sharp increase in accident density during the Songkran festival (April 11-17), a time of heightened travel activity. From the STKDE plots, we can observe clear spikes in density during this period.\nIn summary, the findings of this study highlight the non-random, spatially clustered distribution of road accidents in the BMR. We hope this comprehensive spatial-temporal analysis will support policymakers in implementing data-driven strategies that effectively mitigate road accidents in the region."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#references",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#references",
    "title": "Take Home Exercise 1",
    "section": "17 References",
    "text": "17 References\n\nWikiProject Thailand - OpenStreetMap Wiki\nBangkok - Wikipedia\nHow Do You Beat Bangkok Traffic?\n25 deaths in 234 road accidents recorded on 1st of Songkran’s ‘7 dangerous days’\nSPATIAL STATISTICS\nChapter 6 Studying spatial point patterns | Crime Mapping in R\nVisualizing Traffic Accident Hotspots Based on Spatial-Temporal Network Kernel Density Estimation"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html",
    "title": "In-Class Exercise 4",
    "section": "",
    "text": "ISSS626 Geospatial Analytics and Applications - In-class Exercise 4: Geographically Weighted Summary Statistics - gwModel methods"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#exercise-reference",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#exercise-reference",
    "title": "In-Class Exercise 4",
    "section": "",
    "text": "ISSS626 Geospatial Analytics and Applications - In-class Exercise 4: Geographically Weighted Summary Statistics - gwModel methods"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#overview",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#overview",
    "title": "In-Class Exercise 4",
    "section": "2 Overview",
    "text": "2 Overview\nIn this session, we will learn about Geographically-Weighted Models.\n\nGeographically weighted regression (GWR) is a spatial analysis technique that takes non-stationary variables into consideration (e.g., climate; demographic factors; physical environment characteristics) and models the local relationships between these predictors and an outcome of interest.\nGWR is an outgrowth of ordinary least squares regression (OLS) see more: Geographically Weighted Regression | Columbia University Mailman School of Public Health\n\n\n\n\n\n\n\nNote\n\n\n\nGWModel is under active development. It supports many features such as GW discriminant analysis, GW PCA, regression models and so on.\nGWM is distance-based and does not support adjacency matrices."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#learning-outcome",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#learning-outcome",
    "title": "In-Class Exercise 4",
    "section": "3 Learning Outcome",
    "text": "3 Learning Outcome\n\nReview techniques to merge geospatial and aspatial datasets using dplyr functions like left_join(), covered in Hands-on Exercise.\nConvert spatial data from sf to sp format for compatibility with the GWmodel package.\nCompute geographically weighted summary statistics with adaptive and fixed bandwidth using GWmodel.\nVisualize geographically weighted summary statistics using tmap."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#import-the-r-packages",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#import-the-r-packages",
    "title": "In-Class Exercise 4",
    "section": "4 Import the R Packages",
    "text": "4 Import the R Packages\nThe following R packages will be used in this exercise:\n\n\n\n\n\n\n\n\nPackage\nPurpose\nUse Case in Exercise\n\n\n\n\nsf\nHandles spatial data; imports, manages, and processes vector-based geospatial data.\nImporting and managing geospatial data, such as Hunan’s county boundary shapefile.\n\n\nGWmodel\nProvides functions for geographically weighted regression and summary statistics.\nComputing geographically weighted summary statistics using adaptive and fixed bandwidth methods.\n\n\ntidyverse\nA collection of R packages for data science tasks like data manipulation, visualization, and modeling.\nWrangling aspatial data and joining with spatial datasets.\n\n\ntmap\nCreates static and interactive thematic maps using cartographic quality elements.\nVisualizing geographically weighted summary statistics and creating thematic maps.\n\n\nggstatsplot\nEnhances plots with statistical details and facilitates data visualization.\nStatistical graphics for analysis, comparison, and visualization of summary statistics.\n\n\nknitr\nEnables dynamic report generation and integration of R code with documents.\nFormatting output and generating reports for the exercise.\n\n\n\nTo install and load these packages, use the following code:\n\npacman::p_load(sf, ggstatsplot, spdep, tmap, tidyverse, knitr, GWmodel)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#the-data",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#the-data",
    "title": "In-Class Exercise 4",
    "section": "5 The Data",
    "text": "5 The Data\nThe following datasets will be used in this exercise:\n\n\n\n\n\n\n\n\nData Set\nDescription\nFormat\n\n\n\n\nHunan County Boundary Layer\nA geospatial dataset containing Hunan’s county boundaries.\nESRI Shapefile\n\n\nHunan_2012.csv\nA CSV file containing selected local development indicators for Hunan in 2012.\nCSV\n\n\n\n\nhunan_sf &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `/Users/walter/code/isss626/isss626-gaa/In-class_Ex/In-class_Ex04/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\n\n\nTip\n\n\n\nFor admin boundaries, we will typically encounter polygon or multipolygon data objects.\nA polygon represents a single contiguous area, while a multipolygon consists of multiple disjoint areas grouped together (e.g., islands that belong to the same admin region).\n\n\n\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\nglimpse(hunan2012)\n\nRows: 88\nColumns: 29\n$ County      &lt;chr&gt; \"Anhua\", \"Anren\", \"Anxiang\", \"Baojing\", \"Chaling\", \"Changn…\n$ City        &lt;chr&gt; \"Yiyang\", \"Chenzhou\", \"Changde\", \"Hunan West\", \"Zhuzhou\", …\n$ avg_wage    &lt;dbl&gt; 30544, 28058, 31935, 30843, 31251, 28518, 54540, 28597, 33…\n$ deposite    &lt;dbl&gt; 10967.0, 4598.9, 5517.2, 2250.0, 8241.4, 10860.0, 24332.0,…\n$ FAI         &lt;dbl&gt; 6831.7, 6386.1, 3541.0, 1005.4, 6508.4, 7920.0, 33624.0, 1…\n$ Gov_Rev     &lt;dbl&gt; 456.72, 220.57, 243.64, 192.59, 620.19, 769.86, 5350.00, 1…\n$ Gov_Exp     &lt;dbl&gt; 2703.0, 1454.7, 1779.5, 1379.1, 1947.0, 2631.6, 7885.5, 11…\n$ GDP         &lt;dbl&gt; 13225.0, 4941.2, 12482.0, 4087.9, 11585.0, 19886.0, 88009.…\n$ GDPPC       &lt;dbl&gt; 14567, 12761, 23667, 14563, 20078, 24418, 88656, 10132, 17…\n$ GIO         &lt;dbl&gt; 9276.90, 4189.20, 5108.90, 3623.50, 9157.70, 37392.00, 513…\n$ Loan        &lt;dbl&gt; 3954.90, 2555.30, 2806.90, 1253.70, 4287.40, 4242.80, 4053…\n$ NIPCR       &lt;dbl&gt; 3528.3, 3271.8, 7693.7, 4191.3, 3887.7, 9528.0, 17070.0, 3…\n$ Bed         &lt;dbl&gt; 2718, 970, 1931, 927, 1449, 3605, 3310, 582, 2170, 2179, 1…\n$ Emp         &lt;dbl&gt; 494.310, 290.820, 336.390, 195.170, 330.290, 548.610, 670.…\n$ EmpR        &lt;dbl&gt; 441.4, 255.4, 270.5, 145.6, 299.0, 415.1, 452.0, 127.6, 21…\n$ EmpRT       &lt;dbl&gt; 338.0, 99.4, 205.9, 116.4, 154.0, 273.7, 219.4, 94.4, 174.…\n$ Pri_Stu     &lt;dbl&gt; 54.175, 33.171, 19.584, 19.249, 33.906, 81.831, 59.151, 18…\n$ Sec_Stu     &lt;dbl&gt; 32.830, 17.505, 17.819, 11.831, 20.548, 44.485, 39.685, 7.…\n$ Household   &lt;dbl&gt; 290.4, 104.6, 148.1, 73.2, 148.7, 211.2, 300.3, 76.1, 139.…\n$ Household_R &lt;dbl&gt; 234.5, 121.9, 135.4, 69.9, 139.4, 211.7, 248.4, 59.6, 110.…\n$ NOIP        &lt;dbl&gt; 101, 34, 53, 18, 106, 115, 214, 17, 55, 70, 44, 84, 74, 17…\n$ Pop_R       &lt;dbl&gt; 670.3, 243.2, 346.0, 184.1, 301.6, 448.2, 475.1, 189.6, 31…\n$ RSCG        &lt;dbl&gt; 5760.60, 2386.40, 3957.90, 768.04, 4009.50, 5220.40, 22604…\n$ Pop_T       &lt;dbl&gt; 910.8, 388.7, 528.3, 281.3, 578.4, 816.3, 998.6, 256.7, 45…\n$ Agri        &lt;dbl&gt; 4942.253, 2357.764, 4524.410, 1118.561, 3793.550, 6430.782…\n$ Service     &lt;dbl&gt; 5414.5, 3814.1, 14100.0, 541.8, 5444.0, 13074.6, 17726.6, …\n$ Disp_Inc    &lt;dbl&gt; 12373, 16072, 16610, 13455, 20461, 20868, 183252, 12379, 1…\n$ RORP        &lt;dbl&gt; 0.7359464, 0.6256753, 0.6549309, 0.6544614, 0.5214385, 0.5…\n$ ROREmp      &lt;dbl&gt; 0.8929619, 0.8782065, 0.8041262, 0.7460163, 0.9052651, 0.7…\n\n\n\n\n\n\n\n\nNote\n\n\n\nRecall that to do left join, we need a common identifier between the 2 data objects. The content must be the same eg. same format and same case. In Hands-on Exercise 1B, we need to (PA, SZ) in the dataset to uppercase before we can join the data.\n\n\n\nhunan_sf &lt;- left_join(hunan_sf, hunan2012) %&gt;%\n  select(1:3, 7, 15, 16, 31, 32)\nhunan_sf\n\nSimple feature collection with 88 features and 8 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\nFirst 10 features:\n     NAME_2  ID_3    NAME_3    County GDPPC      GIO      Agri Service\n1   Changde 21098   Anxiang   Anxiang 23667   5108.9  4524.410 14100.0\n2   Changde 21100   Hanshou   Hanshou 20981  13491.0  6545.350 17727.0\n3   Changde 21101    Jinshi    Jinshi 34592  10935.0  2562.460  7525.0\n4   Changde 21102        Li        Li 24473  18402.0  7562.340 53160.0\n5   Changde 21103     Linli     Linli 25554   8214.0  3583.910  7031.0\n6   Changde 21104    Shimen    Shimen 27137  17795.0  5266.510  6981.0\n7  Changsha 21109   Liuyang   Liuyang 63118  99254.0 10844.470 26617.8\n8  Changsha 21110 Ningxiang Ningxiang 62202 114145.0 12804.480 18447.7\n9  Changsha 21111 Wangcheng Wangcheng 70666 148976.0  5222.356  6648.6\n10 Chenzhou 21112     Anren     Anren 12761   4189.2  2357.764  3814.1\n                         geometry\n1  POLYGON ((112.0625 29.75523...\n2  POLYGON ((112.2288 29.11684...\n3  POLYGON ((111.8927 29.6013,...\n4  POLYGON ((111.3731 29.94649...\n5  POLYGON ((111.6324 29.76288...\n6  POLYGON ((110.8825 30.11675...\n7  POLYGON ((113.9905 28.5682,...\n8  POLYGON ((112.7181 28.38299...\n9  POLYGON ((112.7914 28.52688...\n10 POLYGON ((113.1757 26.82734..."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#mapping-gdppc",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#mapping-gdppc",
    "title": "In-Class Exercise 4",
    "section": "6 Mapping GDPPC",
    "text": "6 Mapping GDPPC\nTo plot a chrolopleth map of geographic distribution of GDP per Capita (GDPPC) in Hunan:\n\nbasemap &lt;- tm_shape(hunan_sf) +\n  tm_polygons() +\n  tm_text(\"NAME_3\", size=0.5)\n\ngdppc &lt;- qtm(hunan_sf, \"GDPPC\")\ntmap_arrange(basemap, gdppc, asp=1, ncol=2)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#converting-to-spatialpolygondataframe",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#converting-to-spatialpolygondataframe",
    "title": "In-Class Exercise 4",
    "section": "7 Converting to SpatialPolygonDataFrame",
    "text": "7 Converting to SpatialPolygonDataFrame\n\n\n\n\n\n\nNote\n\n\n\nGWmodel presently is built around the older sp and not sf formats for handling spatial data in R.\n\n\n\nhunan_sp &lt;- hunan_sf %&gt;%\n  as_Spatial()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#geographically-weighted-summary-statistics-with-adaptive-bandwidths",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#geographically-weighted-summary-statistics-with-adaptive-bandwidths",
    "title": "In-Class Exercise 4",
    "section": "8 Geographically Weighted Summary Statistics with Adaptive Bandwidths",
    "text": "8 Geographically Weighted Summary Statistics with Adaptive Bandwidths\nIn this section, we aim to determine the optimal adaptive bandwidth for performing Geographically Weighted Regression (GWR). Specifically, we are interested in finding the best bandwidth to use for summarizing the spatial variation in GDP per capita (GDPPC) across the Hunan region.\n\n8.1 Determine Adaptive Bandwidth\nAn adaptive bandwidth allows the number of neighbors considered in the model to vary depending on the density of data points. This is particularly useful when data points are unevenly distributed across the study area.\nWe will use two different criteria—cross-validation (CV) and Akaike information criterion to determine the optimal bandwidth.\nThe bandwidth that minimizes these metrics will be selected.\n\n8.1.1 Cross Validation\n\nbw_CV&lt;- bw.gwr(GDPPC ~ 1,\n               data = hunan_sp,\n               approach= \"CV\",\n               adaptive = TRUE,\n               kernel = \"bisquare\",\n               longlat = T)  # great circle distance\n\nAdaptive bandwidth: 62 CV score: 15515442343 \nAdaptive bandwidth: 46 CV score: 14937956887 \nAdaptive bandwidth: 36 CV score: 14408561608 \nAdaptive bandwidth: 29 CV score: 14198527496 \nAdaptive bandwidth: 26 CV score: 13898800611 \nAdaptive bandwidth: 22 CV score: 13662299974 \nAdaptive bandwidth: 22 CV score: 13662299974 \n\n\n\nbw_CV\n\n[1] 22\n\n\n\n\n8.1.2 Akaike Information Criterion (AIC)\nNext, we use the AIC approach to determine the optimal bandwidth. AIC is a model selection criterion that balances model fit and complexity, with a lower AIC value indicating a better model.\nWe use the same GWR model setup, but the bandwidth is now optimized based on the AIC value instead of cross-validation.\n\nbw_AIC&lt;- bw.gwr(GDPPC ~ 1,\n               data = hunan_sp,\n               approach= \"AIC\",\n               adaptive = TRUE,\n               kernel = \"bisquare\",\n               longlat = T)\n\nAdaptive bandwidth (number of nearest neighbours): 62 AICc value: 1923.156 \nAdaptive bandwidth (number of nearest neighbours): 46 AICc value: 1920.469 \nAdaptive bandwidth (number of nearest neighbours): 36 AICc value: 1917.324 \nAdaptive bandwidth (number of nearest neighbours): 29 AICc value: 1916.661 \nAdaptive bandwidth (number of nearest neighbours): 26 AICc value: 1914.897 \nAdaptive bandwidth (number of nearest neighbours): 22 AICc value: 1914.045 \nAdaptive bandwidth (number of nearest neighbours): 22 AICc value: 1914.045 \n\n\n\nbw_AIC\n\n[1] 22\n\n\n\n\n\n\n\n\nNote\n\n\n\nIntepretation\nThe output from these 2 methods indicate the number of nearest neighbour we should choose. In this case, both methods produce the same result: 22 nearest neighbours.\nSometimes the result may differ, and either methods is acceptable for further analysis."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#geographically-weighted-summary-statistics-with-adaptive-bandwidth",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#geographically-weighted-summary-statistics-with-adaptive-bandwidth",
    "title": "In-Class Exercise 4",
    "section": "9 Geographically Weighted Summary Statistics with adaptive bandwidth",
    "text": "9 Geographically Weighted Summary Statistics with adaptive bandwidth\nTo compute Geographically Weighted Summary Statistics:\n\ngwstat &lt;- gwss (data = hunan_sp,\n                vars = \"GDPPC\",\n                bw = bw_AIC,\n                kernel = \"bisquare\",\n                adaptive = TRUE,\n                longlat = T)\n\n\n\n\n\n\n\nNote\n\n\n\nWe use bw_AIC as the bandwidth parameter, which was determined previously based on AIC optimization.\nAdditionally, we apply the same bisquare kernel for consistency with the CV and AIC computation above.\nThe output of the gwss() function is a gwss object, which is a list containing localized summary statistics for GDPPC across Hunan.\nNote that the abbreviation in the output refers to:\n\nLM : local mean\nLSD: local standard deviation\nLVar: local variance\nLSKe: standard estimations\nLCV: local correlation variance\n\n\n\n\n9.1 Preparing the output data\nLet’s observe the gwstat object before converting to a suitable format for analysis.\n\nclass(gwstat)\n\n[1] \"gwss\"\n\ngwstat\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n\n   ***********************Calibration information*************************\n\n   Local summary statistics calculated for variables:\n    GDPPC\n   Number of summary points: 88\n   Kernel function: bisquare \n   Summary points: the same locations as observations are used.\n   Adaptive bandwidth: 22 (number of nearest neighbours)\n   Distance metric: Great Circle distance metric is used.\n\n   ************************Local Summary Statistics:**********************\n   Summary information for Local means:\nGDPPC_LM \n    Min.  1st Qu.   Median  3rd Qu.     Max. \n13688.70 17995.43 23408.07 27865.12 49005.84 \n   Summary information for local standard deviation :\nGDPPC_LSD \n     Min.   1st Qu.    Median   3rd Qu.      Max. \n 4282.599  6297.788  8281.756 16315.028 22568.841 \n   Summary information for local variance :\nGDPPC_LVar \n     Min.   1st Qu.    Median   3rd Qu.      Max. \n 18340656  39662960  68633859 266187788 509352591 \n   Summary information for Local skewness:\nGDPPC_LSKe \n      Min.    1st Qu.     Median    3rd Qu.       Max. \n-0.2150599  0.9900027  1.3714638  1.8387524  3.7525953 \n   Summary information for localized coefficient of variation:\nGDPPC_LCV \n     Min.   1st Qu.    Median   3rd Qu.      Max. \n0.2000503 0.3107774 0.3829294 0.5129959 0.8018153 \n\n   ************************************************************************\n\n\n\ngwstat$SDF\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 88 \nextent      : 108.7831, 114.2544, 24.6342, 30.12812  (xmin, xmax, ymin, ymax)\ncrs         : +proj=longlat +datum=WGS84 +no_defs \nvariables   : 5\nnames       :         GDPPC_LM,        GDPPC_LSD,       GDPPC_LVar,         GDPPC_LSKe,         GDPPC_LCV \nmin values  : 13688.6986033259, 4282.59917616925, 18340655.7037255, -0.215059890053627, 0.200050258645349 \nmax values  : 49005.8382943034, 22568.8411539952, 509352591.034267,    3.7525953469342, 0.801815253056722 \n\n\nIn particular, we are interested to extract the SDF data table from gwstat. We can convert it into a data frame and append it onto hunan_sf.\n\ngwstat_df &lt;- as.data.frame(gwstat$SDF)\nhunan_gstat &lt;- cbind(hunan_sf, gwstat_df)\n\n\n\n9.2 Visualising geographically weighted summary statistics\n\ntm_shape(hunan_gstat) +\n  tm_fill(\"GDPPC_LM\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Distribution of geographically weighted mean\",\n            main.title.position = \"center\",\n            main.title.size = 2.0,\n            legend.text.size = 1.2,\n            legend.height = 1.50, \n            legend.width = 1.50,\n            frame = TRUE)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#geographically-weighted-summary-statistics-with-fixed-bandwidth",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#geographically-weighted-summary-statistics-with-fixed-bandwidth",
    "title": "In-Class Exercise 4",
    "section": "10 Geographically Weighted Summary Statistics with Fixed Bandwidth",
    "text": "10 Geographically Weighted Summary Statistics with Fixed Bandwidth\nSimilarly, we can use the same process to generate summary stats with fixed bandwidth.\n\n10.1 Determine Fixed Bandwidth\n\nCross-Validation\n\n\nbw_CV_fixed &lt;- bw.gwr(GDPPC ~ 1, \n             data = hunan_sp,\n             approach = \"CV\",\n             adaptive = FALSE, \n             kernel = \"bisquare\", \n             longlat = T)\n\nFixed bandwidth: 357.4897 CV score: 16265191728 \nFixed bandwidth: 220.985 CV score: 14954930931 \nFixed bandwidth: 136.6204 CV score: 14134185837 \nFixed bandwidth: 84.48025 CV score: 13693362460 \nFixed bandwidth: 52.25585 CV score: Inf \nFixed bandwidth: 104.396 CV score: 13891052305 \nFixed bandwidth: 72.17162 CV score: 13577893677 \nFixed bandwidth: 64.56447 CV score: 14681160609 \nFixed bandwidth: 76.8731 CV score: 13444716890 \nFixed bandwidth: 79.77877 CV score: 13503296834 \nFixed bandwidth: 75.07729 CV score: 13452450771 \nFixed bandwidth: 77.98296 CV score: 13457916138 \nFixed bandwidth: 76.18716 CV score: 13442911302 \nFixed bandwidth: 75.76323 CV score: 13444600639 \nFixed bandwidth: 76.44916 CV score: 13442994078 \nFixed bandwidth: 76.02523 CV score: 13443285248 \nFixed bandwidth: 76.28724 CV score: 13442844774 \nFixed bandwidth: 76.34909 CV score: 13442864995 \nFixed bandwidth: 76.24901 CV score: 13442855596 \nFixed bandwidth: 76.31086 CV score: 13442847019 \nFixed bandwidth: 76.27264 CV score: 13442846793 \nFixed bandwidth: 76.29626 CV score: 13442844829 \nFixed bandwidth: 76.28166 CV score: 13442845238 \nFixed bandwidth: 76.29068 CV score: 13442844678 \nFixed bandwidth: 76.29281 CV score: 13442844691 \nFixed bandwidth: 76.28937 CV score: 13442844698 \nFixed bandwidth: 76.2915 CV score: 13442844676 \nFixed bandwidth: 76.292 CV score: 13442844679 \nFixed bandwidth: 76.29119 CV score: 13442844676 \nFixed bandwidth: 76.29099 CV score: 13442844676 \nFixed bandwidth: 76.29131 CV score: 13442844676 \nFixed bandwidth: 76.29138 CV score: 13442844676 \nFixed bandwidth: 76.29126 CV score: 13442844676 \nFixed bandwidth: 76.29123 CV score: 13442844676 \n\n\n\nbw_CV_fixed\n\n[1] 76.29126\n\n\n\nAIC\n\n\nbw_AIC_fixed &lt;- bw.gwr(GDPPC ~ 1, \n             data = hunan_sp,\n             approach =\"AIC\",\n             adaptive = FALSE, \n             kernel = \"bisquare\", \n             longlat = T)\n\nFixed bandwidth: 357.4897 AICc value: 1927.631 \nFixed bandwidth: 220.985 AICc value: 1921.547 \nFixed bandwidth: 136.6204 AICc value: 1919.993 \nFixed bandwidth: 84.48025 AICc value: 1940.603 \nFixed bandwidth: 168.8448 AICc value: 1919.457 \nFixed bandwidth: 188.7606 AICc value: 1920.007 \nFixed bandwidth: 156.5362 AICc value: 1919.41 \nFixed bandwidth: 148.929 AICc value: 1919.527 \nFixed bandwidth: 161.2377 AICc value: 1919.392 \nFixed bandwidth: 164.1433 AICc value: 1919.403 \nFixed bandwidth: 159.4419 AICc value: 1919.393 \nFixed bandwidth: 162.3475 AICc value: 1919.394 \nFixed bandwidth: 160.5517 AICc value: 1919.391 \n\n\n\nbw_AIC_fixed\n\n[1] 160.5517\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote the results differs this time.\nWe will just use bw_AIC_fixed for this example.\n\n\n\ngwstat_fixed &lt;- gwss(data = hunan_sp,\n               vars = \"GDPPC\",\n               bw = bw_AIC_fixed,\n               kernel = \"bisquare\",\n               adaptive = FALSE,\n               longlat = T)\n\n\n\n10.2 Preparing the output data\n\ngwstat_df_fixed &lt;- as.data.frame(gwstat_fixed$SDF)\nhunan_gstat_fixed &lt;- cbind(hunan_sf, gwstat_df_fixed)\n\n\n\n10.3 Visualising geographically weighted summary statistics\n\ntm_shape(hunan_gstat_fixed) +\n  tm_fill(\"GDPPC_LM\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Distribution of geographically weighted mean\",\n            main.title.position = \"center\",\n            main.title.size = 1.8,\n            legend.text.size = 1.2,\n            legend.height = 1.50,\n            legend.width = 1.50,\n            frame = TRUE)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#visualizing-correlation",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#visualizing-correlation",
    "title": "In-Class Exercise 4",
    "section": "11 Visualizing Correlation",
    "text": "11 Visualizing Correlation\nBusiness question: Is there any relationship between GDP per capita and Gross Industry Output?\n\nggscatterstats(\n  data = hunan2012, \n  x = Agri, \n  y = GDPPC,\n  xlab = \"Gross Agriculture Output\",\n  ylab = \"GDP per capita\", \n  label.var = County, \n  label.expression = Agri &gt; 10000 & GDPPC &gt; 50000, \n  point.label.args = list(alpha = 0.7, size = 4, color = \"grey50\"),\n  xfill = \"#CC79A7\", \n  yfill = \"#009E73\", \n  title = \"Relationship between GDP PC and Gross Agriculture Output\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that above shows a conventional statistical solution to the business question. We can also approach the same question with a geospatial approach."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#geographically-weighted-correlation-with-adaptive-bandwidth",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#geographically-weighted-correlation-with-adaptive-bandwidth",
    "title": "In-Class Exercise 4",
    "section": "12 Geographically Weighted Correlation with Adaptive Bandwidth",
    "text": "12 Geographically Weighted Correlation with Adaptive Bandwidth\nTo come up with the geospatial analytics solution, we can repeat what we have learnt above.\n\n# determine bandwidth\nbw &lt;- bw.gwr(GDPPC ~ GIO, \n             data = hunan_sp, \n             approach = \"AICc\", \n             adaptive = TRUE)\n\nAdaptive bandwidth (number of nearest neighbours): 62 AICc value: 1870.235 \nAdaptive bandwidth (number of nearest neighbours): 46 AICc value: 1870.852 \nAdaptive bandwidth (number of nearest neighbours): 72 AICc value: 1869.744 \nAdaptive bandwidth (number of nearest neighbours): 78 AICc value: 1869.713 \nAdaptive bandwidth (number of nearest neighbours): 82 AICc value: 1869.604 \nAdaptive bandwidth (number of nearest neighbours): 84 AICc value: 1869.537 \nAdaptive bandwidth (number of nearest neighbours): 86 AICc value: 1869.647 \nAdaptive bandwidth (number of nearest neighbours): 83 AICc value: 1869.567 \nAdaptive bandwidth (number of nearest neighbours): 84 AICc value: 1869.537 \n\n\n\n# compute gwCorrelation\ngwstats &lt;- gwss(hunan_sp, \n                vars = c(\"GDPPC\", \"GIO\"), \n                bw = bw,\n                kernel = \"bisquare\",\n                adaptive = TRUE, \n                longlat = T)\n\ngwstats$SDF\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 88 \nextent      : 108.7831, 114.2544, 24.6342, 30.12812  (xmin, xmax, ymin, ymax)\ncrs         : +proj=longlat +datum=WGS84 +no_defs \nvariables   : 13\nnames       :         GDPPC_LM,           GIO_LM,        GDPPC_LSD,          GIO_LSD,       GDPPC_LVar,         GIO_LVar,       GDPPC_LSKe,         GIO_LSKe,         GDPPC_LCV,          GIO_LCV,    Cov_GDPPC.GIO,    Corr_GDPPC.GIO, Spearman_rho_GDPPC.GIO \nmin values  : 19131.1142970311, 10893.8161299979, 10277.2097869105, 14522.4178379531,  105621041.00417, 210900619.860099, 1.48323193793682,  2.0736607949458, 0.536791980126491, 1.00164110576375, 103845165.127288,  0.68232363208861,       0.57886543894541 \nmax values  : 30957.9353099472, 31000.8255210838, 17996.8393335404, 31051.7011545276, 323886225.997265, 964208144.590089, 2.90892414233837, 5.17918131636017, 0.648888935893182, 1.46498918439505, 417614864.583691, 0.760623282755834,       0.73344304557923 \n\n\n\n# convert result to df\ngwstat_df &lt;- as.data.frame(gwstats$SDF) %&gt;%\n  # select(c(12,13)) %&gt;%\n  select(c(\"Corr_GDPPC.GIO\",\"Spearman_rho_GDPPC.GIO\")) %&gt;%\n  rename(gwCorr = Corr_GDPPC.GIO,\n         gwSpearman = Spearman_rho_GDPPC.GIO)\n\nhunan_Corr &lt;- cbind(hunan_sf, gwstat_df)\nhunan_Corr\n\nSimple feature collection with 88 features and 10 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\nFirst 10 features:\n     NAME_2  ID_3    NAME_3    County GDPPC      GIO      Agri Service\n1   Changde 21098   Anxiang   Anxiang 23667   5108.9  4524.410 14100.0\n2   Changde 21100   Hanshou   Hanshou 20981  13491.0  6545.350 17727.0\n3   Changde 21101    Jinshi    Jinshi 34592  10935.0  2562.460  7525.0\n4   Changde 21102        Li        Li 24473  18402.0  7562.340 53160.0\n5   Changde 21103     Linli     Linli 25554   8214.0  3583.910  7031.0\n6   Changde 21104    Shimen    Shimen 27137  17795.0  5266.510  6981.0\n7  Changsha 21109   Liuyang   Liuyang 63118  99254.0 10844.470 26617.8\n8  Changsha 21110 Ningxiang Ningxiang 62202 114145.0 12804.480 18447.7\n9  Changsha 21111 Wangcheng Wangcheng 70666 148976.0  5222.356  6648.6\n10 Chenzhou 21112     Anren     Anren 12761   4189.2  2357.764  3814.1\n      gwCorr gwSpearman                       geometry\n1  0.7486038  0.7052022 POLYGON ((112.0625 29.75523...\n2  0.7444358  0.6931584 POLYGON ((112.2288 29.11684...\n3  0.7506001  0.7106856 POLYGON ((111.8927 29.6013,...\n4  0.7529990  0.7175096 POLYGON ((111.3731 29.94649...\n5  0.7521408  0.7147542 POLYGON ((111.6324 29.76288...\n6  0.7546843  0.7224617 POLYGON ((110.8825 30.11675...\n7  0.7332991  0.6800127 POLYGON ((113.9905 28.5682,...\n8  0.7305758  0.6608931 POLYGON ((112.7181 28.38299...\n9  0.7341173  0.6716514 POLYGON ((112.7914 28.52688...\n10 0.7520181  0.7236245 POLYGON ((113.1757 26.82734...\n\n\n\n12.1 Visualizing Local Correlation\n\nLocal Correlation Coefficient\n\n\ntm_shape(hunan_Corr) +\n  tm_fill(\"gwCorr\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Local Correlation Coefficient\",\n            main.title.position = \"center\",\n            main.title.size = 2.0,\n            legend.text.size = 1.2,\n            legend.height = 1.50, \n            legend.width = 1.50,\n            frame = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nInterpretation\n\nThe strongest correlations are found in the eastern and northern parts of the province, indicated by the darker shades.\nThe weaker correlations are located in the central and western areas, where the lighter colors predominate.\n\n\n\n\nLocal Spearman Coefficient\n\nNote that we will observe similar trend using Local Spearman Coefficient. See notes below.\n\ntm_shape(hunan_Corr) +\n  tm_fill(\"gwSpearman\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Local Spearman Rho\",\n            main.title.position = \"center\",\n            main.title.size = 2.0,\n            legend.text.size = 1.2,\n            legend.height = 1.50, \n            legend.width = 1.50,\n            frame = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nNotes on local correlation coefficient and the local Spearman coefficient:\n\nLocal Correlation Coefficient (Pearson)\n\n\nType: Parametric\n\nThe local correlation coefficient, often represented by Pearson’s correlation coefficient, assumes that the data follows a normal distribution.\n\nNature: Continuous\n\nIt measures the linear relationship between two continuous variables.\n\nType of Measure: Not Ranked\n\nThe Pearson correlation is sensitive to the actual values of the data points, not their ranks. It considers both the magnitude and direction of the linear relationship.\n\n\n\nLocal Spearman Coefficient\n\n\nType: Non-Parametric\n\nThe local Spearman coefficient is a rank-based measure and does not assume any specific distribution for the data. It is robust to non-normality.\n\nNature: Continuous (based on ranks)\n\nAlthough it works with ranks, the coefficient itself can take any continuous value between -1 and 1, like Pearson’s.\n\nType of Measure: Ranked\n\nThe Spearman correlation is based on the ranks of the data rather than their actual values. It measures the strength and direction of a monotonic relationship between two variables."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html",
    "title": "In-Class Exercise 2",
    "section": "",
    "text": "ISSS626 Geospatial Analytics and Applications - In-class Exercise 2: Spatial Point Patterns Analysis: spatstat methods"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#exercise-reference",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#exercise-reference",
    "title": "In-Class Exercise 2",
    "section": "",
    "text": "ISSS626 Geospatial Analytics and Applications - In-class Exercise 2: Spatial Point Patterns Analysis: spatstat methods"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#learning-outcome",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#learning-outcome",
    "title": "In-Class Exercise 2",
    "section": "2 Learning Outcome",
    "text": "2 Learning Outcome\n\nUnderstand how to handle the retired R package such as maptools\nUnderstand the difference in usage of st_combine() and st_union() in the sf package.\nRecap on usage of the spatstat package for analyzing two-dimensional spatial point patterns.\nRecap on conversion steps of sf data frames to ppp and owin objects using as.ppp() and as.owin() functions for point pattern analysis.\nRecap on Kernel Density Estimation (KDE) on spatial point events and visualize results usingspatstat.geom methods.\nUnderstand importance of setting random seed for reproducible results when applying Monte Carlo simulations for spatial analysis.\nPractice importing and visualizing data from regional data sources in preparation for Take Home Assignment 1"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#how-to-handle-retired-r-packages",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#how-to-handle-retired-r-packages",
    "title": "In-Class Exercise 2",
    "section": "3 How to Handle Retired R Packages",
    "text": "3 How to Handle Retired R Packages\nIn our work, we might need to use retired R packages. In this section, we will see how we can use a retired package such as maptools.\nAlthough maptools is retired and removed from CRAN, we can still download from Posit Public Package Manager snapshots by using the code block below.\n\n\n\n\n\n\nTip\n\n\n\nInclude #| eval: false in the installation code block to avoid repetitively downloads of maptools whenever the Quarto document is rendered.\n\n\n\ninstall.packages(\"maptools\",\n                 repos = \"https://packagemanager.posit.co/cran/2023-10-13\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#understanding-the-salient-differences-between-st_combine-and-st_union",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#understanding-the-salient-differences-between-st_combine-and-st_union",
    "title": "In-Class Exercise 2",
    "section": "4 Understanding the Salient Differences Between st_combine() and st_union()",
    "text": "4 Understanding the Salient Differences Between st_combine() and st_union()\nIn sf package, there are two functions allow us to combine multiple simple features into one simple features. They are st_combine() and st_union().\n\n\n\n\n\n\nTip\n\n\n\n\nst_combine() returns a single, combined geometry, with no resolved boundaries; returned geometries may well be invalid.\nIf y is missing, st_union(x) returns a single geometry with resolved boundaries, else the geometries for all unioned pairs of x[i] and y[j].\n\nsee Combine or union feature geometries — geos_combine • sf"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#understanding-the-spatstat-package",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#understanding-the-spatstat-package",
    "title": "In-Class Exercise 2",
    "section": "5 Understanding the spatstat Package",
    "text": "5 Understanding the spatstat Package\nspatstat R package is a comprehensive open-source toolbox for analysing Spatial Point Patterns. Focused mainly on two-dimensional point patterns, including multitype or marked points, in any spatial region.\nIt comprises of many sub-packages for specific usage.\n\n\n\nPackage\nDescription\n\n\n\n\nspatstat\nContains documentation and introductory material, including beginner’s guides, vignettes, and demos.\n\n\nspatstat.data\nContains all datasets required for the spatstat package.\n\n\nspatstat.utils\nProvides basic utility functions for use within spatstat.\n\n\nspatstat.univar\nContains functions for estimating and manipulating probability distributions of 1-dimensional random variables.\n\n\nspatstat.sparse\nFunctions for handling sparse arrays and performing linear algebra operations.\n\n\nspatstat.geom\nDefines spatial objects (e.g., point patterns, windows, pixel images) and includes geometrical operations.\n\n\nspatstat.random\nFunctions for generating random spatial patterns and simulating models.\n\n\nspatstat.explore\nCode for exploratory data analysis and nonparametric spatial data analysis.\n\n\nspatstat.model\nCode for model-fitting, diagnostics, and formal inference within spatial data analysis.\n\n\nspatstat.linnet\nDefines spatial data on linear networks and performs geometrical operations and statistical analysis."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#creating-ppp-objects-from-sf-data.frame",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#creating-ppp-objects-from-sf-data.frame",
    "title": "In-Class Exercise 2",
    "section": "6 Creating ppp Objects from sf data.frame",
    "text": "6 Creating ppp Objects from sf data.frame\nWe can derive an ppp object layer directly from a sf tibble data.frame using as.ppp() from spatstat.geom.\n\nchildcare_ppp &lt;- as.ppp(childcare_sf)\nplot(childcare_ppp)\n\n\n\n\n\n\n\n\n\nsummary(childcare_ppp)\n\nMarked planar point pattern:  1545 points\nAverage intensity 1.91145e-06 points per square unit\n\nCoordinates are given to 11 decimal places\n\nmarks are of type 'character'\nSummary:\n   Length     Class      Mode \n     1545 character character \n\nWindow: rectangle = [11203.01, 45404.24] x [25667.6, 49300.88] units\n                    (34200 x 23630 units)\nWindow area = 808287000 square units\n\n\nFrom the output above, we can observe the properties of the ppp objects."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#creating-owin-object-from-sf-data.frame",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#creating-owin-object-from-sf-data.frame",
    "title": "In-Class Exercise 2",
    "section": "7 Creating owin object from sf data.frame",
    "text": "7 Creating owin object from sf data.frame\nWe can create owin object from polygon sf tibble data.frame using as.owin() of spatstat.geom.\n\nsg_owin &lt;- as.owin(sg_sf)\nplot(sg_owin)\n\n\n\n\n\n\n\n\n\nsummary(sg_owin)\n\nWindow: polygonal boundary\n80 separate polygons (35 holes)\n                  vertices         area relative.area\npolygon 1            14650  6.97996e+08      8.93e-01\npolygon 2 (hole)         3 -2.21090e+00     -2.83e-09\npolygon 3              285  1.61128e+06      2.06e-03\npolygon 4 (hole)         3 -2.05920e-03     -2.63e-12\npolygon 5 (hole)         3 -8.83647e-03     -1.13e-11\npolygon 6              668  5.40368e+07      6.91e-02\npolygon 7               44  2.26577e+03      2.90e-06\npolygon 8               27  1.50315e+04      1.92e-05\npolygon 9              711  1.28815e+07      1.65e-02\npolygon 10 (hole)       36 -4.01660e+04     -5.14e-05\npolygon 11 (hole)      317 -5.11280e+04     -6.54e-05\npolygon 12 (hole)        3 -3.41405e-01     -4.37e-10\npolygon 13 (hole)        3 -2.89050e-05     -3.70e-14\npolygon 14              77  3.29939e+05      4.22e-04\npolygon 15              30  2.80002e+04      3.58e-05\npolygon 16 (hole)        3 -2.83151e-01     -3.62e-10\npolygon 17              71  8.18750e+03      1.05e-05\npolygon 18 (hole)        3 -1.68316e-04     -2.15e-13\npolygon 19 (hole)       36 -7.79904e+03     -9.97e-06\npolygon 20 (hole)        4 -2.05611e-02     -2.63e-11\npolygon 21 (hole)        3 -2.18000e-06     -2.79e-15\npolygon 22 (hole)        3 -3.65501e-03     -4.67e-12\npolygon 23 (hole)        3 -4.95057e-02     -6.33e-11\npolygon 24 (hole)        3 -3.99521e-02     -5.11e-11\npolygon 25 (hole)        3 -6.62377e-01     -8.47e-10\npolygon 26 (hole)        3 -2.09065e-03     -2.67e-12\npolygon 27              91  1.49663e+04      1.91e-05\npolygon 28 (hole)       26 -1.25665e+03     -1.61e-06\npolygon 29 (hole)      349 -1.21433e+03     -1.55e-06\npolygon 30 (hole)       20 -4.39069e+00     -5.62e-09\npolygon 31 (hole)       48 -1.38338e+02     -1.77e-07\npolygon 32 (hole)       28 -1.99862e+01     -2.56e-08\npolygon 33              40  1.38607e+04      1.77e-05\npolygon 34 (hole)       40 -6.00381e+03     -7.68e-06\npolygon 35 (hole)        7 -1.40545e-01     -1.80e-10\npolygon 36 (hole)       12 -8.36709e+01     -1.07e-07\npolygon 37              45  2.51218e+03      3.21e-06\npolygon 38             142  3.22293e+03      4.12e-06\npolygon 39             148  3.10395e+03      3.97e-06\npolygon 40              75  1.73526e+04      2.22e-05\npolygon 41              83  5.28920e+03      6.76e-06\npolygon 42             211  4.70521e+05      6.02e-04\npolygon 43             106  3.04104e+03      3.89e-06\npolygon 44             266  1.50631e+06      1.93e-03\npolygon 45              71  5.63061e+03      7.20e-06\npolygon 46              10  1.99717e+02      2.55e-07\npolygon 47             478  2.06120e+06      2.64e-03\npolygon 48             155  2.67502e+05      3.42e-04\npolygon 49            1027  1.27782e+06      1.63e-03\npolygon 50 (hole)        3 -1.16959e-03     -1.50e-12\npolygon 51              65  8.42861e+04      1.08e-04\npolygon 52              47  3.82087e+04      4.89e-05\npolygon 53               6  4.50259e+02      5.76e-07\npolygon 54             132  9.53357e+04      1.22e-04\npolygon 55 (hole)        3 -3.23310e-04     -4.13e-13\npolygon 56               4  2.69313e+02      3.44e-07\npolygon 57 (hole)        3 -1.46474e-03     -1.87e-12\npolygon 58            1045  4.44510e+06      5.68e-03\npolygon 59              22  6.74651e+03      8.63e-06\npolygon 60              64  3.43149e+04      4.39e-05\npolygon 61 (hole)        3 -1.98390e-03     -2.54e-12\npolygon 62 (hole)        4 -1.13774e-02     -1.46e-11\npolygon 63              14  5.86546e+03      7.50e-06\npolygon 64              95  5.96187e+04      7.62e-05\npolygon 65 (hole)        4 -1.86410e-02     -2.38e-11\npolygon 66 (hole)        3 -5.12482e-03     -6.55e-12\npolygon 67 (hole)        3 -1.96410e-03     -2.51e-12\npolygon 68 (hole)        3 -5.55856e-03     -7.11e-12\npolygon 69             234  2.08755e+06      2.67e-03\npolygon 70              10  4.90942e+02      6.28e-07\npolygon 71             234  4.72886e+05      6.05e-04\npolygon 72 (hole)       13 -3.91907e+02     -5.01e-07\npolygon 73              15  4.03300e+04      5.16e-05\npolygon 74             227  1.10308e+06      1.41e-03\npolygon 75              10  6.60195e+03      8.44e-06\npolygon 76              19  3.09221e+04      3.95e-05\npolygon 77             145  9.61782e+05      1.23e-03\npolygon 78              30  4.28933e+03      5.49e-06\npolygon 79              37  1.29481e+04      1.66e-05\npolygon 80               4  9.47108e+01      1.21e-07\nenclosing rectangle: [2667.54, 56396.44] x [15748.72, 50256.33] units\n                     (53730 x 34510 units)\nWindow area = 781945000 square units\nFraction of frame area: 0.422\n\n\nAs shown above, we can display the summary information of the owin object class."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#combining-point-events-object-and-owin-object",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#combining-point-events-object-and-owin-object",
    "title": "In-Class Exercise 2",
    "section": "8 Combining point events object and owin object",
    "text": "8 Combining point events object and owin object\nTo combine point events object and owin object:\n\nchildcareSG_ppp = childcare_ppp[sg_owin]\nplot(childcareSG_ppp)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#kernel-density-estimation-of-spatial-point-event",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#kernel-density-estimation-of-spatial-point-event",
    "title": "In-Class Exercise 2",
    "section": "9 Kernel Density Estimation of Spatial Point Event",
    "text": "9 Kernel Density Estimation of Spatial Point Event\nIn this section, we will show why we should re-scale to appropriate unit of measurement before performing KDE.\n\nkde_childcareSG_adaptive_m &lt;- adaptive.density(\n  childcareSG_ppp,\n  method=\"kernel\")\n\nchildcareSG_ppp.km &lt;- rescale.ppp(childcareSG_ppp,\n                                  1000,\n                                  \"km\")\n\nkde_childcareSG_adaptive_km &lt;- adaptive.density(\n  childcareSG_ppp.km,\n  method=\"kernel\")\n\n\npar(mfrow=c(1,2))\n\nplot(kde_childcareSG_adaptive_m)\nplot(kde_childcareSG_adaptive_km)\n\n\n\n\n\n\n\n\nFrom the output above, we can notice that the plot on the right has a more interpretable scale range from 0-40km range as compared to the left plot where rescaling was not performed."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#kernel-density-estimation",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#kernel-density-estimation",
    "title": "In-Class Exercise 2",
    "section": "10 Kernel Density Estimation",
    "text": "10 Kernel Density Estimation\nThere is 2 different ways to convert KDE output into grid object. spatstat.geom is preferred.\n\nspatstat.geom methodmaptools method\n\n\n\ngridded_kde_childcareSG_ad &lt;- as(\n  kde_childcareSG_adaptive_km,\n  \"SpatialGridDataFrame\")\nspplot(gridded_kde_childcareSG_ad)\n\n\n\n\n\n\n\n\n\n\n\ngridded_kde_childcareSG_ad &lt;- maptools::as.SpatialGridDataFrame.im(\n  kde_childcareSG_adaptive_km)\nspplot(gridded_kde_childcareSG_ad)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#kernel-density-estimation-1",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#kernel-density-estimation-1",
    "title": "In-Class Exercise 2",
    "section": "11 Kernel Density Estimation",
    "text": "11 Kernel Density Estimation\n\n11.1 Visualising KDE using tmap\nTo visualize KDE in raster output using tmap:\n\ntm_shape(kde_childcareSG_ad_raster) +\n  tm_raster(palette = \"viridis\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#extracting-study-area-using-sf-objects",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#extracting-study-area-using-sf-objects",
    "title": "In-Class Exercise 2",
    "section": "12 Extracting Study Area Using sf Objects",
    "text": "12 Extracting Study Area Using sf Objects\nTo extract and create an ppp object showing child care services and within Punggol Planning Area:\n\n\n\n\n\n\nTip\n\n\n\nfilter() of dplyr package should be used to extract the target planning areas.\n\n\n\npg_owin &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"PUNGGOL\") %&gt;%\n  as.owin()\n\nchildcare_pg = childcare_ppp[pg_owin]\n\nplot(childcare_pg)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#monte-carlo-simulation",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#monte-carlo-simulation",
    "title": "In-Class Exercise 2",
    "section": "13 Monte Carlo Simulation",
    "text": "13 Monte Carlo Simulation\n\n\n\n\n\n\nTip\n\n\n\nIn order to ensure reproducibility, it is important to include the code block below before using spatstat functions involve Monte Carlo simulation\n\n\n\nset.seed(1234)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#edge-correction-methods-of-spatstat",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#edge-correction-methods-of-spatstat",
    "title": "In-Class Exercise 2",
    "section": "14 Edge Correction Methods of spatstat",
    "text": "14 Edge Correction Methods of spatstat\nIn spatstat, edge correction methods are used to handle biases that arise when estimating spatial statistics near the boundaries of a study region. These corrections are essential for ensuring accurate estimates in spatial point pattern analysis, especially for summary statistics like the K-function, L-function, pair correlation function, etc.\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\n\nnone\nNo edge correction is applied. Assumes no bias at the edges, which may lead to underestimation of statistics near the boundaries.\n\n\nisotropic\nCorrects for edge effects by assuming the point pattern is isotropic (uniform in all directions) and compensates for missing neighbors outside the boundary.\n\n\ntranslate\n(Translation Correction) Uses translation correction by translating the observation window so every point lies entirely within it, then averaging statistics over all translations.\n\n\nRipley\n(Ripley’s Correction) Similar to isotropic correction, but specifically tailored for Ripley’s K-function and related functions. Adjusts the expected number of neighbors near edges based on the window’s shape and size.\n\n\nborder\nBorder correction reduces bias by only considering points far enough from the boundary so that their neighborhood is fully contained within the window, minimizing edge effects."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#geospatial-analytics-for-social-good-thailand-road-accident-case-study",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#geospatial-analytics-for-social-good-thailand-road-accident-case-study",
    "title": "In-Class Exercise 2",
    "section": "15 Geospatial Analytics for Social Good: Thailand Road Accident Case Study",
    "text": "15 Geospatial Analytics for Social Good: Thailand Road Accident Case Study\nThis section is in preparation of Take-home Exercise 1: Geospatial Analytics for Public Good\n\n15.1 Background\nFor an overview of the road traffic accidents in Thailand, you may refer to:\n\nRoad traffic injuries, WHO.\nRoad traffic deaths and injuries in Thailand\n\n\n\n15.2 The Study Area\nThe study area is Bangkok Metropolitan Region.\n\n\n\n\n\n\n\nNote\n\n\n\nThe projected coordinate system of Thailand is WGS 84 / UTM zone 47N and the EPSG code is 32647.\n\n\n\n\n15.3 The Data\nFor the purpose of this exercise, three basic data sets are needed, they are:\n\nThailand Road Accident [2019-2022] on Kaggle\nThailand Roads (OpenStreetMap Export) on HDX.\nThailand - Subnational Administrative Boundaries on HDX.\n\n\n15.3.1 Traffic Accident Data\n\nrdacc_sf &lt;- read_csv(\"data/geospatial/thai_road_accident_2019_2022.csv\") %&gt;%\n  filter(!is.na(longitude) & longitude != \"\",\n         !is.na(latitude) & latitude != \"\") %&gt;%\n  st_as_sf(coords = c(\n    \"longitude\", \"latitude\"),\n    crs=4326) %&gt;%\n  st_transform(crs = 32647)\n\n\nplot(rdacc_sf)\n\n\n\n\n\n\n\n\n\n\n15.3.2 Administrative Boundary\n\n# country\nadminboundary0 &lt;- st_read(dsn = \"data/geospatial\",\n                layer = \"tha_admbnda_adm0_rtsd_20220121\")\n\nReading layer `tha_admbnda_adm0_rtsd_20220121' from data source \n  `/Users/walter/code/isss626/isss626-gaa/In-class_Ex/In-class_Ex02/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1 feature and 13 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.34336 ymin: 5.613038 xmax: 105.637 ymax: 20.46507\nGeodetic CRS:  WGS 84\n\n# # province\n# adminboundary1 &lt;- st_read(dsn = \"data/geospatial\",\n#                 layer = \"tha_admbnda_adm1_rtsd_20220121\")\n\n# # district\n# adminboundary2 &lt;- st_read(dsn = \"data/geospatial\",\n#                 layer = \"tha_admbnda_adm2_rtsd_20220121\")\n\n# # sub-district\n# adminboundary3 &lt;- st_read(dsn = \"data/geospatial\",\n#                 layer = \"tha_admbnda_adm3_rtsd_20220121\")\n\n\nplot(adminboundary0, max.plot=1)\n\n\n\n\n\n\n\n# plot(adminboundary1)\n# plot(adminboundary2)\n# plot(adminboundary3)\n\n\n\n15.3.3 Thai Roads\n\nroads &lt;- st_read(dsn = \"data/geospatial\",\n                layer = \"hotosm_tha_roads_lines_shp\")\n\nReading layer `hotosm_tha_roads_lines_shp' from data source \n  `/Users/walter/code/isss626/isss626-gaa/In-class_Ex/In-class_Ex02/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 2792590 features and 14 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 97.34457 ymin: 5.643645 xmax: 105.6528 ymax: 20.47168\nCRS:           NA"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html",
    "title": "In-Class Exercise 1",
    "section": "",
    "text": "ISSS626 Geospatial Analytics and Applications - In-class Exercise 1: Geospatial Data Science with R"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#exercise-reference",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#exercise-reference",
    "title": "In-Class Exercise 1",
    "section": "",
    "text": "ISSS626 Geospatial Analytics and Applications - In-class Exercise 1: Geospatial Data Science with R"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#learning-outcome",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#learning-outcome",
    "title": "In-Class Exercise 1",
    "section": "2 Learning Outcome",
    "text": "2 Learning Outcome\n\nImport and transform geospatial data using tidyverse and sf packages.\nAnalyze spatial datasets and extract meaningful insights.\nCreate and customize choropleth maps using the tmap package.\nDevelop analytical maps to visualize spatial patterns and distributions.\nPerform statistical analysis on spatial data using the ggstatsplot package.\nManage geospatial data workflows effectively in R."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#import-the-r-packages",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#import-the-r-packages",
    "title": "In-Class Exercise 1",
    "section": "3 Import the R Packages",
    "text": "3 Import the R Packages\n\n\n\n\n\n\n\n\nPackage\nPurpose\nUse Case in Exercise\n\n\n\n\nsf\nImporting, managing, and processing geospatial data.\nHandling and processing geospatial data in R.\n\n\ntidyverse\nComprehensive set of tools for data science tasks.\nImporting, wrangling, and visualizing data.\n\n\ntmap\nCreating thematic maps for visualizing spatial data.\nDesigning and displaying thematic maps to represent spatial patterns.\n\n\nggstatsplot\nEnhancing ggplot2 plots with statistical analyses.\nAdding statistical summaries and tests to visualizations for better insights.\n\n\n\n\npacman::p_load(tidyverse, sf, tmap, ggstatsplot)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#the-data",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#the-data",
    "title": "In-Class Exercise 1",
    "section": "4 The Data",
    "text": "4 The Data\n\n\n\nDataset\nSource\nDescription\n\n\n\n\nMaster Plan 2014 Subzone Boundary (Web)\ndata.gov.sg\nGeospatial boundaries for Singapore’s planning subzones in 2014.\n\n\nMaster Plan 2019 Subzone Boundary (Web)\ndata.gov.sg\nUpdated geospatial boundaries for Singapore’s planning subzones in 2019.\n\n\nPre-Schools Location\ndata.gov.sg\nLocation data for pre-schools in Singapore.\n\n\nSingapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2023\nsingstat.gov.sg\nAspatial data on residents of Singapore by planning area, subzone, age, sex, and type of dwelling for June 2023."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#working-with-master-plan-planning-sub-zone-data",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#working-with-master-plan-planning-sub-zone-data",
    "title": "In-Class Exercise 1",
    "section": "5 Working with Master Plan Planning Sub-zone Data",
    "text": "5 Working with Master Plan Planning Sub-zone Data\nTo import the shapefile version of the Sub-zone data:\n\nmpsz14_shp &lt;- st_read(dsn = \"data\",\n                  layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/walter/code/isss626/isss626-gaa/In-class_Ex/In-class_Ex01/data' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nNotice that mpsz14_shp is of class sf and inherits from data.frame. This means it combines spatial features with tabular data structure.\n\nclass(mpsz14_shp)\n\n[1] \"sf\"         \"data.frame\"\n\n\nTo import the kml version of the Sub-zone data:\n\nmpsz14_kml &lt;- st_read(\"data/MasterPlan2014SubzoneBoundaryWebKML.kml\")\n\n\nNote that this is a corrupted file. It is likely that the agency has unknowingly uploaded a corrupted kml file.\n\nTo render a workable kml, we can convert the shp object and save it as a kml file.\n\nst_write(mpsz14_shp,\n         \"data/MasterPlan2014SubzoneBoundaryWebKML.kml\",\n         delete_dsn=TRUE)\n\nNote that the delete_dsn=TRUE argument ensures that any existing file with the same name is deleted before saving the new KML file, preventing potential conflicts or errors from overwriting.\nTo import pre-school location data in kml format:\n\npreschool_kml &lt;- st_read(\"data/PreSchoolsLocation.kml\")\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `/Users/walter/code/isss626/isss626-gaa/In-class_Ex/In-class_Ex01/data/PreSchoolsLocation.kml' \n  using driver `KML'\nSimple feature collection with 2290 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\nTo import Master Plan 2019 Subzone Boundary (No SEA) kml and MPSZ-2019 into sf simple feature data.frame:\n\nmpsz19_shp &lt;- st_read(dsn = \"data\",\n                  layer = \"MPSZ-2019\") %&gt;%\n  st_transform(crs=3414)\n\nReading layer `MPSZ-2019' from data source \n  `/Users/walter/code/isss626/isss626-gaa/In-class_Ex/In-class_Ex01/data' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\n\nmpsz19_kml &lt;- st_read(\"data/MasterPlan2019RegionBoundaryNoSeaKML.kml\")\n\nReading layer `URA_MP19_REGION_NO_SEA_PL' from data source \n  `/Users/walter/code/isss626/isss626-gaa/In-class_Ex/In-class_Ex01/data/MasterPlan2019RegionBoundaryNoSeaKML.kml' \n  using driver `KML'\nSimple feature collection with 5 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XYZ\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#handling-coordinate-systems",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#handling-coordinate-systems",
    "title": "In-Class Exercise 1",
    "section": "6 Handling Coordinate Systems",
    "text": "6 Handling Coordinate Systems\nTo check the project of the imported sf objects:\n\nst_crs(mpsz19_shp)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nNext, re-write the code chunk to import the Master Plan Sub-zone 2019 and Pre-schools Location with proper transformation.\n\nmpsz19_shp &lt;- st_read(dsn = \"data/\",\n                layer = \"MPSZ-2019\") %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `MPSZ-2019' from data source \n  `/Users/walter/code/isss626/isss626-gaa/In-class_Ex/In-class_Ex01/data' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\npreschool &lt;- st_read(\"data/PreSchoolsLocation.kml\") %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `/Users/walter/code/isss626/isss626-gaa/In-class_Ex/In-class_Ex01/data/PreSchoolsLocation.kml' \n  using driver `KML'\nSimple feature collection with 2290 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#geospatial-data-wrangling",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#geospatial-data-wrangling",
    "title": "In-Class Exercise 1",
    "section": "7 Geospatial Data Wrangling",
    "text": "7 Geospatial Data Wrangling\n\n7.1 Point-in-Polygon count\nThe code below counts the number of preschools within each planning subzone. It uses st_intersects to find intersections between the geometries of the subzones (mpsz19_shp) and the preschool locations, identifying where the two sets of geometries overlap or share common points.\nThe lengths() function is then applied to count the number of intersecting preschools for each subzone, effectively returning the total number of preschools within each subzone in the mpsz19_shp shapefile.\n\nmpsz19_shp &lt;- mpsz19_shp %&gt;%\n  mutate(`PreSch Count` = lengths(\n    st_intersects(mpsz19_shp, preschool)))\n\n\n\n7.2 Computing Density\nThe code below performs the following tasks:\nDerive the area of each planning sub-zone.\nDrop the unit of measurement of the area (i.e. m^2)\nCalculate the density of pre-school at the planning sub-zone level.\n\nmpsz19_shp &lt;- mpsz19_shp %&gt;%\n  mutate(Area = units::drop_units(\n    st_area(.)),\n    `PreSch Density` = `PreSch Count` / Area * 1000000\n  )"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#statistical-analysis",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#statistical-analysis",
    "title": "In-Class Exercise 1",
    "section": "8 Statistical Analysis",
    "text": "8 Statistical Analysis\nIn this section, we will use appropriate Exploratory Data Analysis (EDA) and Confirmatory Data Analysis (CDA) methods to explore and confirm the statistical relationship between Pre-school Density and Pre-school count.\n\n# Convert the 'PreSch Density' column in 'mpsz19_shp' to numeric format\nmpsz19_shp$`PreSch Density` &lt;- as.numeric(as.character(mpsz19_shp$`PreSch Density`))\n\n# Convert the 'PreSch Count' column in 'mpsz19_shp' to numeric format\nmpsz19_shp$`PreSch Count` &lt;- as.numeric(as.character(mpsz19_shp$`PreSch Count`))\n\n# Convert the spatial data frame 'mpsz19_shp' to a regular data frame for plotting\nmpsz19_df &lt;- as.data.frame(mpsz19_shp)\n\n# Create a scatter plot with statistical details using the 'ggscatterstats' function from the 'ggstatsplot' package\nggstatsplot::ggscatterstats(data = mpsz19_df,\n               x = `PreSch Density`, # Set 'PreSch Density' as the x-axis variable\n               y = `PreSch Count`, # Set 'PreSch Count' as the y-axis variable\n               type = \"parametric\", # Specify the type of statistical test to use (parametric)\n               label.var = SUBZONE_N, # Label points with the subzone names\n               label.expression = `PreSch Count` &gt; 40) # Label only those points where 'PreSch Count' is greater than 40\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nOutput Intepretation:\nThe plot shows a moderately strong positive correlation between preschool density and the number of preschools in each subzone. As the preschool density increases, the number of preschools tends to increase as well."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#working-with-population-data",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#working-with-population-data",
    "title": "In-Class Exercise 1",
    "section": "9 Working with Population data",
    "text": "9 Working with Population data\n\npopdata &lt;- read_csv(\"data/respopagesextod2023.csv\")\n\nTo prepare a data.frame showing population by Planning Area and Planning subzone:\n\npopdata2023 &lt;- popdata %&gt;%\n  # Group the data by Planning Area (PA), Subzone (SZ), and Age Group (AG)\n  group_by(PA, SZ, AG) %&gt;%\n  # Summarize the population (POP) by calculating the sum of 'Pop' within each group\n  summarise(`POP` = sum(`Pop`)) %&gt;%\n  # Remove the grouping structure to avoid issues in further operations\n  ungroup() %&gt;%\n  # Reshape the data to a wider format: create separate columns for each Age Group (AG)\n  # with their corresponding population values (POP)\n  pivot_wider(names_from = AG, values_from = POP)\n\ncolnames(popdata2023)\n\n [1] \"PA\"          \"SZ\"          \"0_to_4\"      \"10_to_14\"    \"15_to_19\"   \n [6] \"20_to_24\"    \"25_to_29\"    \"30_to_34\"    \"35_to_39\"    \"40_to_44\"   \n[11] \"45_to_49\"    \"50_to_54\"    \"55_to_59\"    \"5_to_9\"      \"60_to_64\"   \n[16] \"65_to_69\"    \"70_to_74\"    \"75_to_79\"    \"80_to_84\"    \"85_to_89\"   \n[21] \"90_and_Over\"\n\n\nNow, we will perform data processing to derive a tibble data.framewith the following fields PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY where by:\n\nYOUNG: age group 0 to 4 until age groyup 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group.\n\n\npopdata2023 &lt;- popdata2023 %&gt;%\n  # Calculate the 'YOUNG' population: sum of age groups 0-24, 10-24, and 5-9\n  mutate(YOUNG = rowSums(.[3:6]) + rowSums(.[14])) %&gt;%\n  # Calculate the 'ECONOMY ACTIVE' population: sum of age groups 25-59 and 60-64\n  mutate(`ECONOMY ACTIVE` = rowSums(.[7:13]) + rowSums(.[15])) %&gt;%\n  # Calculate the 'AGED' population: sum of age groups 65 and above\n  mutate(`AGED` = rowSums(.[16:21])) %&gt;%\n  # Calculate the 'TOTAL' population: sum of all age groups\n  mutate(`TOTAL` = rowSums(.[3:21])) %&gt;%\n  # Calculate the 'DEPENDENCY' ratio: (YOUNG + AGED) / ECONOMY ACTIVE\n  mutate(`DEPENDENCY` = (`YOUNG` + `AGED`) / `ECONOMY ACTIVE`) %&gt;%\n  # Select only the relevant columns to keep in the final data frame\n  select(`PA`, `SZ`, `YOUNG`, `ECONOMY ACTIVE`, `AGED`, `TOTAL`, `DEPENDENCY`)\n\nNext, we will joining aspatial and geospatial data. First, we use toupper() to convert elements of PA and SZ to upper case.\n\npopdata2023 &lt;- popdata2023 %&gt;%\n  mutate_at(.vars = vars(PA, SZ),\n          .funs = list(toupper))\n\nThe code below demonstrates how to use left_join() to merge the geospatial data mpsz19_shp with the population data popdata2023. By keeping mpsz19_shp as the left table in the first join, we ensure that the geometry details of the spatial data are preserved:\n\nmpsz_pop2023 &lt;- left_join(mpsz19_shp, popdata2023,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\nIn contrast, the second join keeps popdata2023 as the left table, meaning the population data is retained, and geometry details from mpsz19_shp are added where they match:\n\npop2023_mpsz &lt;- left_join(popdata2023, mpsz19_shp,\n                          by = c(\"SZ\" = \"SUBZONE_N\"))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#choropleth-map-of-dependency-ratio-by-planning-subzone",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#choropleth-map-of-dependency-ratio-by-planning-subzone",
    "title": "In-Class Exercise 1",
    "section": "10 Choropleth Map of Dependency Ratio by Planning Subzone",
    "text": "10 Choropleth Map of Dependency Ratio by Planning Subzone\n\n# Set the base shape for the map using the merged geospatial and population data\ntm_shape(mpsz_pop2023) +\n\n  # Fill the map areas based on the \"DEPENDENCY\" variable, using a quantile classification and a blue color palette\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n\n  # Customize the layout of the map with a title and legend adjustments\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.title.size = 1,\n            legend.height = 0.45,\n            legend.width = 0.35) + # Corrected: add \"+\" to continue chaining elements\n\n  # Add semi-transparent borders around the subzones\n  tm_borders(alpha = 0.5) +\n\n  # Add compass with a star-shaped style\n  tm_compass(type = \"8star\", size = 1.5) +\n\n  # Add scale bar\n  tm_scale_bar() +\n\n  # Add grid to map\n  tm_grid(alpha = 0.2) +\n\n  # Cite the data source\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authority (URA)\\n and Population data from Department of Statistics (DOS)\",\n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe output is a choropleth map showing the distribution of the dependency ratio by planning subzone in Singapore.\n\nThe map clearly shows which areas have higher or lower dependency ratios. For instance, darker areas like certain central regions have higher dependency ratios, suggesting a higher number of dependents (both young and aged) compared to the working-age population.\nLighter areas have lower ratios, indicating a balance or a larger working-age population compared to dependents."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#analytical-map-percentile-map",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#analytical-map-percentile-map",
    "title": "In-Class Exercise 1",
    "section": "11 Analytical Map: Percentile Map",
    "text": "11 Analytical Map: Percentile Map\nThe percentile map is a special type of quantile map with six specific categories: 0-1%,1-10%, 10-50%,50-90%,90-99%, and 99-100%. The corresponding breakpoints can be derived by means of the base R quantile command, passing an explicit vector of cumulative probabilities as c(0,.01,.1,.5,.9,.99,1). Note that the begin and endpoint need to be included.\nFirst, we have to process the data by dropping NA records.\n\nmpsz_pop2023 &lt;- mpsz_pop2023 %&gt;%\n  drop_na()\n\nNext, we defines a function to get the input data and field to be used for creating the percentile map.\n\nget.var &lt;- function(vname,df) {\n  v &lt;- df[vname] %&gt;%\n    st_set_geometry(NULL)\n  v &lt;- unname(v[,1])\n  return(v)\n}\n\nThen, we creates a percentile mapping function for computing and plotting the percentile map.\n\n# Create a percentile map based on a variable\npercentmap &lt;- function(vnam, df, legtitle = NA, mtitle = \"Percentile Map\") {\n  # Define percentile breaks to be used for map legend\n  percent &lt;- c(0, .01, .1, .5, .9, .99, 1)\n\n  # Retrieve the variable from the data frame based on the variable name\n  var &lt;- get.var(vnam, df)\n\n  # Calculate the percentile values (break points) for the variable\n  bperc &lt;- quantile(var, percent)\n\n  # Create the base shape layer using the mpsz_pop2023 dataset\n  tm_shape(mpsz_pop2023) +\n  tm_polygons() + # Draw polygons for subzones\n\n  # Overlay the specified data frame 'df' on the map\n  tm_shape(df) +\n     tm_fill(vnam, # Fill polygons based on the variable 'vnam'\n             title = legtitle, # Set legend title\n             breaks = bperc, # Use calculated percentiles for breaks\n             palette = \"Blues\", # Apply a blue color palette\n             labels = c(\"&lt; 1%\", \"1% - 10%\", \"10% - 50%\", \"50% - 90%\", \"90% - 99%\", \"&gt; 99%\"))  +\n  tm_borders() + # Add borders to the map polygons\n\n  # Customize the layout and appearance of the map\n  tm_layout(main.title = mtitle, # Set the main title of the map\n            title.position = c(\"right\", \"bottom\"))\n}\n\nFinally, we can plot the percentile map.\n\npercentmap(\"DEPENDENCY\", mpsz_pop2023)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#analytical-map-box-map",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#analytical-map-box-map",
    "title": "In-Class Exercise 1",
    "section": "12 Analytical Map: Box Map",
    "text": "12 Analytical Map: Box Map\nIn essence, a box map is an augmented quartile map, with an additional lower and upper category. When there are lower outliers, then the starting point for the breaks is the minimum value, and the second break is the lower fence. In contrast, when there are no lower outliers, then the starting point for the breaks will be the lower fence, and the second break is the minimum value (there will be no observations that fall in the interval between the lower fence and the minimum value).\n\nCreate the boxbreaks function\n\nThe code block below is an R function that creating break points for a box map.\n\narguments:\n\nv: vector with observations\nmult: multiplier for IQR (default 1.5)\n\nreturns:\n\nbb: vector with 7 break points compute quartile and fences\n\n\n\n# int a function 'boxbreaks' to calculate break points based on boxplot statistics\nboxbreaks &lt;- function(v, mult = 1.5) {\n  # Calculate the quartiles of the input vector 'v' and remove names\n  qv &lt;- unname(quantile(v))\n\n  # Calculate the interquartile range (IQR)\n  iqr &lt;- qv[4] - qv[2]\n\n  # Calculate the upper and lower fences for outlier detection\n  upfence &lt;- qv[4] + mult * iqr\n  lofence &lt;- qv[2] - mult * iqr\n\n  # Initialize a numeric vector 'bb' with 7 elements to store the break points\n  bb &lt;- vector(mode = \"numeric\", length = 7)\n\n  # Determine lower break points based on lower fence\n  if (lofence &lt; qv[1]) {  # No lower outliers\n    bb[1] &lt;- lofence      # Set lower fence as the first break point\n    bb[2] &lt;- floor(qv[1]) # Round down to the nearest integer for the next break point\n  } else {                # There are lower outliers\n    bb[2] &lt;- lofence      # Set lower fence as the second break point\n    bb[1] &lt;- qv[1]        # Set the minimum value as the first break point\n  }\n\n  # Determine upper break points based on upper fence\n  if (upfence &gt; qv[5]) {  # No upper outliers\n    bb[7] &lt;- upfence      # Set upper fence as the last break point\n    bb[6] &lt;- ceiling(qv[5]) # Round up to the nearest integer for the previous break point\n  } else {                # There are upper outliers\n    bb[6] &lt;- upfence      # Set upper fence as the sixth break point\n    bb[7] &lt;- qv[5]        # Set the maximum value as the last break point\n  }\n\n  # Set the inner quartile values (Q1, Median, Q3) as middle break points\n  bb[3:5] &lt;- qv[2:4]\n\n  return(bb)\n}\n\n\nCreate the get.var function\n\nThe R function below extracts a variable as a vector out of an sf data frame.\n\narguments:\n\nvname: variable name (as character, in quotes)\ndf: name of sf data frame\n\nreturns:\n\nv: vector with values (without a column name)\n\n\n\n# init  a function 'get.var' to extract a variable from a data frame without its spatial geometry\nget.var &lt;- function(vname, df) {\n  # Select the specified variable 'vname' from the data frame 'df' and remove the spatial geometry information\n  v &lt;- df[vname] %&gt;%\n    st_set_geometry(NULL) # Remove the geometry component from the sf object to get a plain data frame\n\n  # Remove the column name from the variable and convert it to a plain vector\n  v &lt;- unname(v[,1])\n\n  return(v)\n}\n\n\nCreate the Boxmap function\n\nThe code chunk below is an R function to create a box map.\n\narguments:\n\nvnam: variable name (as character, in quotes)\ndf: simple features polygon layer\nlegtitle: legend title\nmtitle: map title\nmult: multiplier for IQR\n\nreturns:\n\na tmap-element (plots a map)\n\n\n\n# Define a function 'boxmap' to create a box map based on a variable\nboxmap &lt;- function(vnam, df,\n                   legtitle = NA, # Set default value for legend title as NA\n                   mtitle = \"Box Map\", # Set default value for main title\n                   mult = 1.5) { # Set default multiplier for calculating fences in boxplot\n\n  # Extract the variable data from the data frame without geometry\n  var &lt;- get.var(vnam, df)\n\n  # Calculate the break points for the box map using the 'boxbreaks' function\n  bb &lt;- boxbreaks(var)\n\n  # Create the base shape layer using the specified data frame 'df'\n  tm_shape(df) +\n    tm_polygons() + # Draw polygons for spatial units\n\n  # Overlay the data frame again to apply fill color based on the variable\n  tm_shape(df) +\n     tm_fill(vnam, # Fill polygons based on the variable 'vnam'\n             title = legtitle, # Set legend title\n             breaks = bb, # Use calculated boxplot breaks for coloring\n             palette = \"Blues\", # Apply a blue color palette\n             labels = c(\"Lower outlier\",  # Label for lower outliers\n                        \"&lt; 25%\",          # Label for first quartile\n                        \"25% - 50%\",      # Label for second quartile (median)\n                        \"50% - 75%\",      # Label for third quartile\n                        \"&gt; 75%\",          # Label for upper quartile\n                        \"Upper outlier\")) + # Label for upper outliers\n  tm_borders() + # Add borders to the polygons on the map\n\n  # Customize the layout and appearance of the map\n  tm_layout(main.title = mtitle, # Set the main title of the map\n            title.position = c(\"left\", \"top\"), # Position the title at the top left\n            frame = F) # Remove the map frame\n}\n\n\nFinally, plot the Box Map\n\nStatic Box Map\n\nboxmap(\"DEPENDENCY\", mpsz_pop2023)\n\n\n\n\n\n\n\n\nInteractive Box Map\n\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"view\")\ntm_basemap(\"Esri.WorldGrayCanvas\") +\nboxmap(\"DEPENDENCY\", mpsz_pop2023)"
  }
]